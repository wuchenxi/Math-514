\documentclass{article} % USenglish for autoref
%\usepackage[paper=screen, centering, papersize=20cm]{geometry}
\usepackage{cmap}		% to search and copy ligatures
\usepackage[utf8]{inputenc}	% for Linux computer and Mac
%\usepackage[latin1]{inputenc}	% fÃ¼r Windows computer
\usepackage[T1]{fontenc}	% to search for ligatures in the pdf
\usepackage[USenglish]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[]{algorithm2e}
%\usepackage{stmaryrd}		% for \mapsfrom
\usepackage{aliascnt}		% for Aliascounter so that autoref gives the thms the right names
\usepackage[pdfborder={0 0 0}]{hyperref}   % if you want to have math environment in captions you have to use \texorpdfstring{$x^2$}{x2}; without frame around the links
\usepackage[figure]{hypcap}		% to make autoref link to figures and not the captions of figures
%\usepackage{paralist}		% for compactitem
\usepackage{mathtools}		% for \coloneqq
\usepackage[]{todonotes}	% use option disable to disable all the todos
\usepackage{float}
\usepackage{lipsum}
\theoremstyle{break}
\newtheorem{definition}{Definition}[section]  
\newtheorem{exa}[definition]{Example}
\newtheorem{cor}[definition]{Corollary}
\newtheorem{lem}[definition]{Lemma}
\newtheorem{conj}[definition]{Conjecture}
\newtheorem{quest}[definition]{Research question}
\newtheorem{thm}[definition]{Theorem}  
\newtheorem{prop}[definition]{Proposition}
\newtheorem{rem}[definition]{Remark}


\renewcommand{\labelenumi}{(\roman{enumi})} % roman numbers in enumerations

\usepackage{graphicx,tikz,pgfplots} % Allows including images

\usetikzlibrary{calc,decorations.markings}
\usetikzlibrary{shapes,snakes}

\DeclareMathOperator{\Aff}{Aff}
\DeclareMathOperator{\der}{der}
\DeclareMathOperator{\Trans}{Trans}
\DeclareMathOperator{\dir}{dir}
\DeclareMathOperator{\Sing}{Sing}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\NN}{\mathbb{N}}


%For L Cal and L Twidle:
\newcommand{\LC}{\mathcal{L}} 
\newcommand{\LT}{\widetilde{\mathcal{L}}}

%For X bar
\newcommand{\XB}{\overline{X}}

%For the real and complex numbers
\newcommand{\RR}{\mathbb{R}} 
\newcommand{\CC}{\mathbb{C}} 


\newcommand{\remark}[2][]{\todo[color=green!50, #1]{#2}}

\title{Math 514}


\begin{document}
\maketitle
  \begin{itemize}
  \item Instructor: Chenxi Wu cwu367@wisc.edu
 \item Section 1: 9:55-10:45 am Section 2: 12:05-12:55 pm  
  \item Office hours: 10:45am-noon Monday, Wednesday or by appointment
  \end{itemize}
  
  Recall that in the first half of the semester, we covered the following topics:
  \begin{enumerate}
  \item Methods for root finding: Newton's method etc.
  \item Numerical Linear Algebra: LU decomposition, QR algorithm etc.
  \end{enumerate}
  
  The following are the new topics we will cover in the second half of the semester:
  \begin{enumerate}
  \item Interpolation and approximation: how to get the formula of a function using discrete date.
  \item Numerical integration: how to integrate a function knowing only its value on a discrete set. 
  \item Numerical solution for differential equations: numerical solution for ODE and PDE.
  \end{enumerate}
  The first topic will be the foundation of the second and third topic.

  \newpage
  
\tableofcontents
\newpage
  
  \section{Polynomial interpolation (Chapter 6)}
  
  \begin{definition}
   The {\bf polynomial interpolation} of a function $f$ at points $x_0, \dots x_n$, is a polynomial $p$ that shares some properties of $f$ at those points, e.g. has the same value or same derivatives.
  \end{definition}

  We will focus on single variable functions, and discuss two kinds of polynomial interpolation problems:
  \begin{enumerate}
  \item Lagrange interpolation: find a polynomial $p$ of degree at most $n$, such that $p(x_i)=f(x_i)$.
  \item Hermite interpolation: find a polynomial $p$ of degree at most $2n+1$, such that $p(x_i)=f(x_i)$, $p'(x_i)=f'(x_i)$.
  \end{enumerate}

  An application for Hermite interpolation is for approximating smooth curves where the direction of the curve at certain points are also specified.

  
  \subsection{Lagrange interpolation}
  \subsubsection{Existence}
  \begin{thm}\label{ext_lag} (Lemma 6.1 in textbook) The Lagrange interpolation polynomial exists. In other words, for any $n+1$ distinct real numbers $x_0,\dots, x_n$, and $n+1$ real numbers $y_0, \dots, y_n$, there is a polynomial $p$ of degree at most $n$ such that $p(x_i)=y_i$.
  \end{thm}
  How do we find such a $p$?\\
  
  Firstly, we observe that the map
  \[T: p\mapsto [p(x_0), \dots p(x_n)]^T\in \mathbb{R}^{n+1}\]
  is linear. In other words, $(cp+dq)(x_i)=cp(x_i)+dq(x_i)$ for all $i$.
  Hence, finding $p$ is like solving a system of non homogenous linear equations. Recall from linear algebra, let $e_i$ be the standard basis vector of $\mathbb{R}^{n+1}$ corresponding to $y_i=1$ and $y_j=0$ for all $j\not=i$, then, if we can find some $p_i$ of degree at most $n$ such that $T(p_i)=e_i$, (i.e. $p_i(x_j)=\begin{cases} 1 & i=j\\ 0 &i\not=j\end{cases}$)  then
  \[T(\sum_iy_ip_i)=\sum_iy_ie_i=[y_0, \dots y_n]^T\ .\]

  
  Now we try and find the $p_i$: $p_i(x_j)=0$ for all $j\not=i$, so $(x-x_j)$ must be a factor of $p_i$. So $p_i$ must be something times
  \[\prod_{j\not=i}(x-x_j)\ .\]

  On the other hand, $p_i(x_i)=1$, so the ``something'' should be
  \[{1\over\prod_{j\not=i}(x_i-x_j)}\ .\]

  Now we have a proof of Theorem 1:

  \begin{proof}
    Let
    \[p(x)=\sum_i\left( y_i\cdot {\prod_{j\not=i}(x-x_j)\over \prod_{j\not=i}(x_i-x_j)}\right)\]
    Then $p(x_k)=\sum_i\left( y_i\cdot {\prod_{j\not=i}(x_k-x_j)\over \prod_{j\not=i}(x_i-x_j)}\right)y_i$. The $k$-th term is $y_k\times 1$ while all other terms are zero, hence the answer is $y_k$.
  \end{proof}


  \subsubsection{Uniqueness}
  
  The problem of finding Lagrange interpolation polynomial is one with $n+1$ conditions and $n+1$ unknowns, so intuitively there should be a discrete set of solutions. Actually the solution can be shown to be unique:

  \begin{thm}\label{uniq_lag} (Theorem 6.1 in textbook)
  The Lagrange interpolation polynomial is unique. In other words, given $x_0, \dots, x_n$ and $y_0, \dots, y_n$ with $x_i$ distinct, there is a single polynomial $p$ of degree at most $n$ such that $p(x_i)=y_i$
  \end{thm}

  \begin{proof}
    The first step of the proof is to reduce the problem to the case where $y_i=0$. Suppose $p$ and $q$ are two such polynomials, then $(p-q)(x_i)=0$ for all $i$. So, to prove the theorem, we only need to show that if a polynomial $r=p-q$ of degree at most $n$ vanishes at $n+1$ distinct points, then $r=0$. This fact follows from the ``fundamental theorem of algebra'' and can be proved using long division. Here we provide two other alternative proofs:

    \begin{enumerate}
    \item Approach I: use the mean value theorem in calculus. Firstly we show the following fact:
      \begin{lem}\label{key_lem} If $f \in C^{m-1}$ ($f$ is $m-1$-th order differentiable with $m-1$-th derivative continuous), and $f=0$ at $m$ distinct points, then for any $0\leq k\leq m-1$, $f^{(k)}=0$ at at least $m-k$ points.
      \end{lem}
      \begin{proof}
       By mean value theorem, between two consecutive zeros of $f$ there must be a zero of $f'$. Hence $f'$ vanishes at at least $m-1$ points. Now let $f'$ take the role of $f$ and continue the process, we get $f''$ vanishes at at least $m-2$ points, etc. 
      \end{proof}
      Suppose $r$ vanishes at $x_0, \dots, x_n$, and $r$ is of degree $d>0$. Then $r^{(d)}$ is a non zero constant. Apply Lemma~\ref{key_lem} with $m=n+1$ and $k=d$, we see a contradiction. Hence $r=const$, which implies $r=0$.
  \item Approach II: Let $r=\sum_ja_jx^j$, then $a_j$ are solutions of a system of linear equation $\sum_ja_jx_i^j=0$. However from linear algebra, 
  \[\left|\begin{array}{cccc}1 & 1& \dots & 1\\ x_1 & x_2 &\dots &x_{n+1}\\ \dots & \dots & \dots & \dots \\ x_1^n & x_2^n & \dots & x_{n+1}^n\end{array}\right|=\prod_{i<j}(x_j-x_i)\not=0\]
Hence $a_j=0$ for all $j$, which implies that $r=0$.  
    \end{enumerate}
  \end{proof}


  \subsubsection{Error Estimate}
  
  We know that $f^{(n+1)}=0$ iff $f$ is a polynomial of degree at most $n$, so one may guess that if $f^{(n+1)}$ is small, $f$ should be close to a polynomial of degree at most $n$, hence probably close to its Lagrange interpolation polynomial at $n+1$ points. To make this more precise, we have the following theorem on error estimate of Lagrange interpolation:
\begin{thm}\label{err_lag} (Theorem 6.2 in textbook) If $f\in C^{n+1}$, $p$ is the Lagrange interpolation of $f$ at $n+1$ distinct points $x_0,\dots x_n$. Then for any $x$, there is some $s\in[\min\{x_i, x\}, \max\{x_i, x\}]$, such that
  \[f(x)-p(x)={f^{(n+1)}(s)\prod_i(x-x_i)\over (n+1)!}\]
\end{thm}


 \begin{proof} When $x=x_i$ it's obvious. Now suppose $x$ is distinct from all $x_i$. Consider the auxiliary function:
  \[G(t)=f(t)-p(t)-(f(x)-p(x))\cdot{\prod_i(t-x_i)\over \prod_i(x-x_i)}\]
  Then $G=0$ at $x_i$ and $x$, hence by Lemma~\ref{key_lem} (let $m=n+2$, $k=n+1$), there must be some point $s\in[\min\{x_i, x\}, \max\{x_i, x\}]$ where
  \[G^{(n+1)}(s)=f^{(n+1)}(s)-{(f(x)-p(x))(n+1)!\over \prod_i(x-x_i)}=0\]
  Hence
  \[f(x)-p(x)={f^{(n+1)}(s)\prod_i(x-x_i)\over (n+1)!}\]
\end{proof}


When the set $\{x_i\}$ becomes denser, $\prod_i(x-x_i)$ decreases, and $(n+1)!$ increases. However, when $n\rightarrow \infty$, the Lagrange interpolation polynomial may not converge to $f$ even if $f$ is smooth, if $f^{(n)}$ increases too fast.

\begin{exa}\label{exa1}$f(x)=\cos(x)$, $x_i=5i/n$, $i=0, 1, 2, \dots n$\\
\begin{figure}[H]
\begin{tikzpicture}[yscale=2, xscale=2]
  \draw[->] (0, 0) -- (5, 0) node[right] {$x$};
  \draw[->] (0, -1) -- (0, 1) node[above] {$y$};
  \draw[scale=1, domain=0:5, smooth, variable=\x, black, very thick, dashed] plot ({\x}, {cos(deg(\x))});
  \draw[scale=1, domain=0:5, smooth, variable=\x, red] plot ({\x}, {cos(deg(0))*(\x-5)/(0-5)+cos(deg(5))*(\x-0)/(5-0)});
  \draw[scale=1, domain=0:5, smooth, variable=\x, green] plot ({\x}, {cos(deg(0))*(\x-5)*(\x-2.5)/(0-5)/(0-2.5)-cos(deg(2.5))*(\x-0)*(\x-5)/2.5/2.5+cos(deg(5))*(\x-0)*(\x-2.5)/(5-0)/2.5});
  \draw[scale=1, domain=0:5, smooth, variable=\x, blue] plot({\x}, {cos(deg(0.0))*(\x-0.5)/(0.0-0.5)*(\x-1.0)/(0.0-1.0)*(\x-1.5)/(0.0-1.5)*(\x-2.0)/(0.0-2.0)*(\x-2.5)/(0.0-2.5)*(\x-3.0)/(0.0-3.0)*(\x-3.5)/(0.0-3.5)*(\x-4.0)/(0.0-4.0)*(\x-4.5)/(0.0-4.5)*(\x-5.0)/(0.0-5.0)+cos(deg(0.5))*(\x-0.0)/(0.5-0.0)*(\x-1.0)/(0.5-1.0)*(\x-1.5)/(0.5-1.5)*(\x-2.0)/(0.5-2.0)*(\x-2.5)/(0.5-2.5)*(\x-3.0)/(0.5-3.0)*(\x-3.5)/(0.5-3.5)*(\x-4.0)/(0.5-4.0)*(\x-4.5)/(0.5-4.5)*(\x-5.0)/(0.5-5.0)+cos(deg(1.0))*(\x-0.0)/(1.0-0.0)*(\x-0.5)/(1.0-0.5)*(\x-1.5)/(1.0-1.5)*(\x-2.0)/(1.0-2.0)*(\x-2.5)/(1.0-2.5)*(\x-3.0)/(1.0-3.0)*(\x-3.5)/(1.0-3.5)*(\x-4.0)/(1.0-4.0)*(\x-4.5)/(1.0-4.5)*(\x-5.0)/(1.0-5.0)+cos(deg(1.5))*(\x-0.0)/(1.5-0.0)*(\x-0.5)/(1.5-0.5)*(\x-1.0)/(1.5-1.0)*(\x-2.0)/(1.5-2.0)*(\x-2.5)/(1.5-2.5)*(\x-3.0)/(1.5-3.0)*(\x-3.5)/(1.5-3.5)*(\x-4.0)/(1.5-4.0)*(\x-4.5)/(1.5-4.5)*(\x-5.0)/(1.5-5.0)+cos(deg(2.0))*(\x-0.0)/(2.0-0.0)*(\x-0.5)/(2.0-0.5)*(\x-1.0)/(2.0-1.0)*(\x-1.5)/(2.0-1.5)*(\x-2.5)/(2.0-2.5)*(\x-3.0)/(2.0-3.0)*(\x-3.5)/(2.0-3.5)*(\x-4.0)/(2.0-4.0)*(\x-4.5)/(2.0-4.5)*(\x-5.0)/(2.0-5.0)+cos(deg(2.5))*(\x-0.0)/(2.5-0.0)*(\x-0.5)/(2.5-0.5)*(\x-1.0)/(2.5-1.0)*(\x-1.5)/(2.5-1.5)*(\x-2.0)/(2.5-2.0)*(\x-3.0)/(2.5-3.0)*(\x-3.5)/(2.5-3.5)*(\x-4.0)/(2.5-4.0)*(\x-4.5)/(2.5-4.5)*(\x-5.0)/(2.5-5.0)+cos(deg(3.0))*(\x-0.0)/(3.0-0.0)*(\x-0.5)/(3.0-0.5)*(\x-1.0)/(3.0-1.0)*(\x-1.5)/(3.0-1.5)*(\x-2.0)/(3.0-2.0)*(\x-2.5)/(3.0-2.5)*(\x-3.5)/(3.0-3.5)*(\x-4.0)/(3.0-4.0)*(\x-4.5)/(3.0-4.5)*(\x-5.0)/(3.0-5.0)+cos(deg(3.5))*(\x-0.0)/(3.5-0.0)*(\x-0.5)/(3.5-0.5)*(\x-1.0)/(3.5-1.0)*(\x-1.5)/(3.5-1.5)*(\x-2.0)/(3.5-2.0)*(\x-2.5)/(3.5-2.5)*(\x-3.0)/(3.5-3.0)*(\x-4.0)/(3.5-4.0)*(\x-4.5)/(3.5-4.5)*(\x-5.0)/(3.5-5.0)+cos(deg(4.0))*(\x-0.0)/(4.0-0.0)*(\x-0.5)/(4.0-0.5)*(\x-1.0)/(4.0-1.0)*(\x-1.5)/(4.0-1.5)*(\x-2.0)/(4.0-2.0)*(\x-2.5)/(4.0-2.5)*(\x-3.0)/(4.0-3.0)*(\x-3.5)/(4.0-3.5)*(\x-4.5)/(4.0-4.5)*(\x-5.0)/(4.0-5.0)+cos(deg(4.5))*(\x-0.0)/(4.5-0.0)*(\x-0.5)/(4.5-0.5)*(\x-1.0)/(4.5-1.0)*(\x-1.5)/(4.5-1.5)*(\x-2.0)/(4.5-2.0)*(\x-2.5)/(4.5-2.5)*(\x-3.0)/(4.5-3.0)*(\x-3.5)/(4.5-3.5)*(\x-4.0)/(4.5-4.0)*(\x-5.0)/(4.5-5.0)+cos(deg(5.0))*(\x-0.0)/(5.0-0.0)*(\x-0.5)/(5.0-0.5)*(\x-1.0)/(5.0-1.0)*(\x-1.5)/(5.0-1.5)*(\x-2.0)/(5.0-2.0)*(\x-2.5)/(5.0-2.5)*(\x-3.0)/(5.0-3.0)*(\x-3.5)/(5.0-3.5)*(\x-4.0)/(5.0-4.0)*(\x-4.5)/(5.0-4.5)});
\end{tikzpicture}
\caption{Black dashed line: $y=\cos(x)$. Red line: Lagrange interpolation with 2 points. Green line: Lagrange interpolation with 3 points.
Blue line: Lagrange interpolation with 11 points.}
\end{figure}
\end{exa}


\begin{exa}\label{exa2} $f(x)=1/(1+2(x-2)^2)$, $x_i=5i/n$,  $i=0, 1, 2, \dots n$.
  \begin{figure}[H]
\begin{tikzpicture}[yscale=2, xscale=2]
  \draw[->] (0, 0) -- (5, 0) node[right] {$x$};
  \draw[->] (0, -1) -- (0, 1.3) node[above] {$y$};
  \draw[scale=1, domain=0:5, smooth, variable=\x, black, very thick, dashed] plot ({\x}, {1/(1+2*(\x-2)*(\x-2))});
  \draw[scale=1, domain=0:5, smooth, variable=\x, red] plot ({\x}, {1/9*(\x-5)/(0-5)+1/19*(\x-0)/(5-0)});
  \draw[scale=1, domain=0:5, smooth, variable=\x, green] plot ({\x}, {1/9*(\x-2.5)*(\x-5)/(0-2.5)/(0-5)+1/1.5*(\x-0)*(\x-5)/(2.5-0)/(2.5-5)+1/19*(\x-0)*(\x-2.5)/5/2.5});
  
  \draw[scale=1, domain=0:5, smooth, variable=\x, blue] plot ({\x}, {1/(1+2*(0.0-2)*(0.0-2))*(\x-0.5)/(0.0-0.5)*(\x-1.0)/(0.0-1.0)*(\x-1.5)/(0.0-1.5)*(\x-2.0)/(0.0-2.0)*(\x-2.5)/(0.0-2.5)*(\x-3.0)/(0.0-3.0)*(\x-3.5)/(0.0-3.5)*(\x-4.0)/(0.0-4.0)*(\x-4.5)/(0.0-4.5)*(\x-5.0)/(0.0-5.0)+1/(1+2*(0.5-2)*(0.5-2))*(\x-0.0)/(0.5-0.0)*(\x-1.0)/(0.5-1.0)*(\x-1.5)/(0.5-1.5)*(\x-2.0)/(0.5-2.0)*(\x-2.5)/(0.5-2.5)*(\x-3.0)/(0.5-3.0)*(\x-3.5)/(0.5-3.5)*(\x-4.0)/(0.5-4.0)*(\x-4.5)/(0.5-4.5)*(\x-5.0)/(0.5-5.0)+1/(1+2*(1.0-2)*(1.0-2))*(\x-0.0)/(1.0-0.0)*(\x-0.5)/(1.0-0.5)*(\x-1.5)/(1.0-1.5)*(\x-2.0)/(1.0-2.0)*(\x-2.5)/(1.0-2.5)*(\x-3.0)/(1.0-3.0)*(\x-3.5)/(1.0-3.5)*(\x-4.0)/(1.0-4.0)*(\x-4.5)/(1.0-4.5)*(\x-5.0)/(1.0-5.0)+1/(1+2*(1.5-2)*(1.5-2))*(\x-0.0)/(1.5-0.0)*(\x-0.5)/(1.5-0.5)*(\x-1.0)/(1.5-1.0)*(\x-2.0)/(1.5-2.0)*(\x-2.5)/(1.5-2.5)*(\x-3.0)/(1.5-3.0)*(\x-3.5)/(1.5-3.5)*(\x-4.0)/(1.5-4.0)*(\x-4.5)/(1.5-4.5)*(\x-5.0)/(1.5-5.0)+1/(1+2*(2.0-2)*(2.0-2))*(\x-0.0)/(2.0-0.0)*(\x-0.5)/(2.0-0.5)*(\x-1.0)/(2.0-1.0)*(\x-1.5)/(2.0-1.5)*(\x-2.5)/(2.0-2.5)*(\x-3.0)/(2.0-3.0)*(\x-3.5)/(2.0-3.5)*(\x-4.0)/(2.0-4.0)*(\x-4.5)/(2.0-4.5)*(\x-5.0)/(2.0-5.0)+1/(1+2*(2.5-2)*(2.5-2))*(\x-0.0)/(2.5-0.0)*(\x-0.5)/(2.5-0.5)*(\x-1.0)/(2.5-1.0)*(\x-1.5)/(2.5-1.5)*(\x-2.0)/(2.5-2.0)*(\x-3.0)/(2.5-3.0)*(\x-3.5)/(2.5-3.5)*(\x-4.0)/(2.5-4.0)*(\x-4.5)/(2.5-4.5)*(\x-5.0)/(2.5-5.0)+1/(1+2*(3.0-2)*(3.0-2))*(\x-0.0)/(3.0-0.0)*(\x-0.5)/(3.0-0.5)*(\x-1.0)/(3.0-1.0)*(\x-1.5)/(3.0-1.5)*(\x-2.0)/(3.0-2.0)*(\x-2.5)/(3.0-2.5)*(\x-3.5)/(3.0-3.5)*(\x-4.0)/(3.0-4.0)*(\x-4.5)/(3.0-4.5)*(\x-5.0)/(3.0-5.0)+1/(1+2*(3.5-2)*(3.5-2))*(\x-0.0)/(3.5-0.0)*(\x-0.5)/(3.5-0.5)*(\x-1.0)/(3.5-1.0)*(\x-1.5)/(3.5-1.5)*(\x-2.0)/(3.5-2.0)*(\x-2.5)/(3.5-2.5)*(\x-3.0)/(3.5-3.0)*(\x-4.0)/(3.5-4.0)*(\x-4.5)/(3.5-4.5)*(\x-5.0)/(3.5-5.0)+1/(1+2*(4.0-2)*(4.0-2))*(\x-0.0)/(4.0-0.0)*(\x-0.5)/(4.0-0.5)*(\x-1.0)/(4.0-1.0)*(\x-1.5)/(4.0-1.5)*(\x-2.0)/(4.0-2.0)*(\x-2.5)/(4.0-2.5)*(\x-3.0)/(4.0-3.0)*(\x-3.5)/(4.0-3.5)*(\x-4.5)/(4.0-4.5)*(\x-5.0)/(4.0-5.0)+1/(1+2*(4.5-2)*(4.5-2))*(\x-0.0)/(4.5-0.0)*(\x-0.5)/(4.5-0.5)*(\x-1.0)/(4.5-1.0)*(\x-1.5)/(4.5-1.5)*(\x-2.0)/(4.5-2.0)*(\x-2.5)/(4.5-2.5)*(\x-3.0)/(4.5-3.0)*(\x-3.5)/(4.5-3.5)*(\x-4.0)/(4.5-4.0)*(\x-5.0)/(4.5-5.0)+1/(1+2*(5.0-2)*(5.0-2))*(\x-0.0)/(5.0-0.0)*(\x-0.5)/(5.0-0.5)*(\x-1.0)/(5.0-1.0)*(\x-1.5)/(5.0-1.5)*(\x-2.0)/(5.0-2.0)*(\x-2.5)/(5.0-2.5)*(\x-3.0)/(5.0-3.0)*(\x-3.5)/(5.0-3.5)*(\x-4.0)/(5.0-4.0)*(\x-4.5)/(5.0-4.5)});
\end{tikzpicture}
\caption{Black dashed line: $y=1/(1+2(x-2)^2)$. Red line: Lagrange interpolation with 2 points. Green line: Lagrange interpolation with 3 points. Blue line: Lagrange interpolation with 11 points.}
\end{figure}
\end{exa}

The reason that the Lagrange interpolation polynomials in Example \ref{exa1} converges but those in Example \ref{exa2} don't, is that the higher order derivatives of $\cos$ is $\pm \sin$, $\pm \cos$ hence all bounded, while it is not true for the function in Example \ref{exa2}. As a practice, calculate the $k$-th derivative of $1/(1+2(x-2)^2)$ at $x=2$.


\subsection{Hermite interpolation polynomial}

\subsubsection{Existence}

Similar to the Lagrange case, we can construct the Hermite interpolation polynomial as follows:
\begin{thm}\label{ext_her} (Existence part of Theorem 6.3 in textbook) There is a polynomial $p$ of degree at most $2n+1$, such that $p(x_i)=y_i$, $p'(x_i)=z_i$, $i=0, \dots n$, where $x_i$ are distinct.
\end{thm}
Use the same strategy as the Lagrange case, possibly via a few trials and errors, one can find the formula of $p$ as below:
\begin{proof}
  Let
  \[p(x)=\sum_i\left( z_i\cdot {(x-x_i)\prod_{j\not=i}(x-x_j)^2\over \prod_{j\not=i}(x_i-x_j)^2}\right. \]
  \[\left. +y_i\cdot \left(1-(x-x_i)\sum_{j\not=i}{2\over x_i-x_j}\right)\cdot {\prod_{j\not=i}(x-x_j)^2\over \prod_{j\not=i}(x_i-x_j)^2} \right)\]
  Then by calculation,
  \[p(x_k)=\sum_i\left( z_i\cdot {(x_k-x_i)\prod_{j\not=i}(x_k-x_j)^2\over \prod_{j\not=i}(x_i-x_j)^2}\right. \]
  \[\left. +y_i\cdot \left(1-(x_k-x_i)\sum_{j\not=i}{2\over x_i-x_j}\right)\cdot {\prod_{j\not=i}(x_k-x_j)^2\over \prod_{j\not=i}(x_i-x_j)^2} \right)\]
  In the first sum, all terms have a factor $(x_k-x_k)$, so it must be zero. In the second sum, all but the $k$-th term is zero, and the $k$-th term is $y_k$.
  Similarly, by taking derivative and let $x=x_k$, we can show that $p'(x_k)=z_k$.
  \end{proof}

  
\subsubsection{Uniqueness and Error Estimate}
  
  The mean value theorem argument (i.e. Lemma~\ref{key_lem}) can also be used to show the uniqueness and error estimate for Hermite interpolation polynomials:

  \begin{thm}\label{uniq_her} (Uniqueness part of Theorem 6.3 in textbook)
 The Hermite interpolation polynomial is unique. In other words, there is a unique $p$ of degree at most $2n+1$ such that $p(x_i)=y_i$, $p'(x_i)=z_i$, $i=0, \dots n$, where $x_i$ are distinct.
\end{thm}
\begin{proof} Similar to the proof of Theorem~\ref{uniq_lag}, if we have two Hermite interpolation polynomials $p$ and $q$, then $r=p-q$ satisfies $r(x_i)=r'(x_i)=0$ and $r$ has degree at most $2n+1$. However, if $r$ is non zero, it can not have $n+1$ distincts roots $x_i$ with multiplicity at least $2$ each, hence $r=0$.\\

  We can also prove $r=0$ using analysis like in Theorem~\ref{uniq_lag}. If $r$ has degree at most $2n+1$, $r'=r=0$ at $n+1$ points, then there must be $n$ other points where $r'=0$. Now suppose $r$ has degree $d>0$. Apply Lemma~\ref{key_lem}, let $m=2n+2$, $k=d$, then we get $r^{(d)}$ vanishes at $2n+2-d$ points, which contradicts with the fact that $r^{(d)}$ is a non zero constant. Hence $r=const$ which implies that $r=0$.
\end{proof}

\begin{thm}\label{err_her} (Theorem 6.4 in textbook) If $f\in C^{(2n+2)}$, there is $s\in [\min\{x_i, x\}, \max\{x_i, x\}]$, such that
  \[f(x)-p(x)={f^{(2n+2)}(s)\prod_i(x-x_i)^2\over (2n+2)!}\]
\end{thm}

\begin{proof}
  If $x=x_i$ then it is trivially true. Now assume $x$ is not in $\{x_i\}$. Let
  \[G(t)=f(t)-p(t)-{(f(x)-p(x))\prod_i(t-x_i)^2\over \prod_i(x-x_i)^2}\]
  Then $G$ vanishes at the $n+2$ points $x, x_0, \dots x_n$, and $G'$ vanishes at $n+1$ of them $x_0, \dots, x_n$. By the same argument as above, $G'$ vanishes at $n+1$ more points, hence it is zero at at least $2n+2$ points. Now use Lemma~\ref{key_lem} on $G'$ for $m=2n+2$, $k=2n+1$.
\end{proof}


\subsection{Applications}
\subsubsection{Numerical Differentiation}\label{nd}
Suppose $p$ is the Lagrange interpolation of $f$ at $n+1$ points. By mean value theorem, $f'-p'$ is zero at $n$ points $d_1, \dots d_n$, so $p'$ can be seen as the Lagrange interpolation polynomial with condition $p'(d_i)=f'(d_i)$ (see Theorem 6.5 in textbook). Now one can get an estimate for $f'(x)-p'(x)$ using Theorem~\ref{err_lag}.\\

\begin{exa}
  For example, if we know the value of $f$ at $x+ih$ for $i=-1, 0, 1$ as $y_{-1}$, $y_0$ and $y_1$, then the Lagrange interpolation polynomial is:
  \[p(x+t)=y_{-1}t(t-h)/(2h^2)-y_0(t+h)(t-h)/h^2+y_1t(t+h)/(2h^2)\]
  So
  \[p'(0)={y_1-y_{-1}\over 2h}={f(x+h)-f(x-h)\over 2h}\]
  As $h\rightarrow 0$ this indeed converges to $f'(x)$.
 \end{exa}

However, this approach is generally unstable. If $f$ is complex analytic one can use complex analysis to do it which is stable, which we will not cover in this class.\\

Numerical differentiation is useful in optimization or root finding via Newton's method.


\subsubsection{Cubic B{\'e}zier curves}
The {\bf cubic B{\'e}zier curve} is a curve parametrized by cubic functions: $\gamma: [0, 1]\rightarrow \mathbb{R}^2$, $\gamma(t)=(\gamma_1(t), \gamma_2(t))$, where $\gamma_1$ and $\gamma_2$ are both of degree at most $3$, and $\gamma(0)=P_0$, $\gamma'(0)=3(P_1-P_0)$, $\gamma(1)=P_3$, $\gamma'(1)=3(P_3-P_2)$, where $P_0$, $P_1$, $P_2$ and $P_3$ are the four ``control points''.

To find the formula for cubic B{\'e}zier curve, we can apply the formula for Hermite interpolation polynomial for $n=1$. B{\'e}zier curves has many applications in computer graphics and font design, and you might have already used it in applications that generate or edit vector graphics. Below is an example (drawn using LaTeX/TikZ):
\begin{figure}[H]
  \begin{tikzpicture}
    \node at (0, 2) {$P_0$};
    \node at (-1, 0) {$P_1$};
    \node at (1, 3) {$P_2$};
    \node at (3, 1) {$P_3$};
    \draw (0, 2) .. controls (-1, 0) and (1, 3) .. (3, 1);
  \end{tikzpicture}
\end{figure}

\subsubsection{Linear and Hermite splines (Chapter 11)}

From the Example 2 above we see that polynomial interpolation with high degree is not guaranteed to work well. Hence, in practice, we often try to keep the degree of the polynomial low, which means that we will need to use piecewise functions for interpolation. We will discuss two kinds of piecewise polynomial interpolation: {\bf linear spline} and {\bf Hermite cubic spline}. The textbook also covered the {\bf natural cubic spline}.

\paragraph{Linear Spline}

\begin{definition}
  Let $f$ be a single variable function on $[a, b]$, $a=x_0<x_1\dots <x_n=b$ $n+1$ distinct points. The {\bf Linear Spline} $s_L$ with {\bf knots} at $x_i$ is defined as
  \[s_L(x)={x_i-x\over x_i-x_{i-1}}f(x_{i-1})+{x-x_{i-1}\over x_i-x_{i-1}}f(x_i), \text{ where }x_{i-1}\leq x\leq x_i\]
\end{definition}
In other words, use the 2-point Lagrange interpolation for each interval $[x_{i-1}, x_i]$.\\


\begin{thm}\label{err_spline_lin} (Theorem 11.1 in textbook)
Let $f\in C^2$, $h=\max\{x_i-x_{i-1}\}$, $M=\max|f''|$, then for any $x\in [a, b]$, $|f(x)-s_L(x)|\leq {1\over 8}h^2M$.
\end{thm}

\begin{proof}
  Suppose $x$ is between $x_{i-1}$ and $x_i$. Theorem~\ref{err_lag} implies that
  \[f(x)-s_L(x)={f''(c)(x-x_{i-1})(x-x_i)\over 2!}\]
  for some $c\in [x_{i-1}, x_i]$. From assumption, $|f''(c)|<M$ and
  \[|(x-x_{i-1})(x-x_i)|\leq|(x_i-x_{i-1})/2|^2\leq h^2/4\ .\]
\end{proof}

The linear spline formula can be alternatively written as $s_L=\sum_if(x_i)\phi_i$, where $\phi_i$ are the ``hat functions'', where, if $i=1, \dots, n-1$,
\[\phi_i(x)=\begin{cases} (x-x_{i-1})/(x_i-x_{i-1}) & x\in [x_{i-1}, x_i]\\
    (x-x_{i+1})/(x_i-x_{i+1}) & x\in [x_i, x_{i+1}] \\ 0 & otherwise \end{cases}\]
$\phi_0$ and $\phi_n$ can be written down similarly. As a consequence, $s_L$ lies in the span of $\phi_n$. \\


\paragraph{Hermite cubic spline}

\begin{definition} Let $f\in C^1[a, b]$,  $a=x_0<x_1\dots <x_n=b$ $n+1$ distinct points. The {\bf Hermite Cubic Spline} $s_H$ with {\bf knots} at $x_i$ is defined as $s_H(x)=p_i(x)$ for $x\in [x_{i-1}, x_i]$, where $p_i$ is the Hermite interpolation polynomial defined using $\{x_{i-1}, x_i\}$.
\end{definition}


\begin{thm}\label{err_spline_her} (Theorem 11.4 in textbook)
Let $f\in C^2$, $h=\max\{x_i-x_{i-1}\}$, $M=\max|f^{(4)}|$, then for any $x\in [a, b]$, $|f(x)-s_H(x)|\leq {1\over 384}h^4M$.
\end{thm}
The proof is similar to Theorem~\ref{err_spline_lin}. Note that $384=4!2^4$.\\

One can also find a set of basis functions for $s_H$.

% \newpage

% \subsubsection{Natural cubic spline}

% \begin{definition} Let $f\in C[a, b]$, $a=x_0<x_1\dots <x_n=b$ $n+1$ distinct points. The {\bf Natural Cubic Spline} $s_2$ is a piecewise polynomial function which is cubic on each $[x_{i-1}, x_i]$, in $C^2$, and $s_2''(x_0)=s_2''(x_n)=0$.
% \end{definition}

% \begin{thm} The natural cubic spline exists and is unique.\end{thm}
% \begin{proof}
% To calculate $s_2$, firstly note that $s_2''$ is piecewise linear with linear pieces on each $[x_{i-1}, x_i]$, hence one only need to find $\sigma_i=s_2''(x_i)$ for all $i$, then integrate the resulting linear spline twice. By assumption $\sigma_0=\sigma_n=0$, and for $i=1, \dots, n-1$, by integrating twice, we have
% \[f(x_{i\pm 1})=f(x_i)+(x_{i\pm 1}-x_i)f'(x_i)+{1\over 2}(x_{i\pm 1}-x_i)^2\sigma_i\]
% \[+{1\over 6}(x_{i\pm 1}-x_i)^3\left({\sigma_{i\pm 1}-\sigma_i\over x_{i\pm 1}-x_i}\right)\ .\]

% \newpage

% Now eliminate $f'(x_i)$, we get
% \[(x_i-x_{i-1})\sigma_{i-1}+2(x_{i+1}-x_{i-1})\sigma_i+(x_{i+1}-x_i)\sigma_{i+1}\]
% \[=6\left({f(x_{i+1}-f(x_i)\over x_{i+1}-x_i}-{f(x_i)-f(x_{i-1})\over x_i-x_{i-1}}\right)\ .\]
% So we get a system of linear equations for $\sigma_i$. From linear algebra, we know that the solution exists and is unique if the coefficient matrix is non-singular, i.e. if the system of linear equations:
% \[y_0=y_n=0\]
% \[(x_i-x_{i-1})y_{i-1}+2(x_{i+1}-x_{i-1})y_i+(x_{i+1}-x_i)y_{i+1}=0\]
% has only non zero solution.\\

% Suppose it has a non-zero solution $(y'_0, \dots, y'_n)$, let $j$ be the first index such that $|y'_j|$ reaches its maximum, then
% \[|2(x_{j+1}-x_{j-1})y_j|>|(x_j-x_{j-1})y_{j-1}|+|(x_{j+1}-x_j)y_{j+1}|\]
% A contradiction.
% \end{proof}

% There is a similar error estimate for natural cubic spline as well, with a larger constant.

% \newpage

% \subsection{Basis function for spline}


% There are analogous basis function for the other two splines as well.

% The basis for $s_H$ is similar.\\

% The natural cubic spline $s_2$ also lies in a finite dimensional vector space.  When $x_i$ are equally spaced, $x_{i}-x_{i-1}=h$, $j=2, \dots, n-2$, a basis function can be chosen as
% \[\psi_j(x)=\begin{cases}{1\over 4h^4}\sum_{k=0}^4(-1)^k{4 \choose k}(x-(x_j-(2+k)h))^3 & |x-x_j|<2h \\ 0 & otherwise \end{cases}\]


\subsection{Review}

\begin{itemize}
\item Definition and formula of Lagrange/Hermite interpolation polynomials.
\item Uniqueness.
\item Error estimate.
\end{itemize}

\section{Approximation Theory (Chapter 8, 9)}

We can see that the Lagrange interpolation polynomial, Hermite interpolation polynomial, and the splines all lie in a vector space spanned by finitely many functions. In other words, all these algorithms can be seen as a way to {\bf approximate a function using the linear combination of simpler functions}.\\

Recall that a set of functions form a vector space if it is closed under addition and scalar multiplication.


\subsection{Approximation in normed vector space}

\begin{definition} Let $V$ be a vector space. A {\bf norm} on $V$ is a function: $\|\cdot \|: V\rightarrow \mathbb{R}_{\geq 0}$ such that:
  \begin{itemize}
  \item $\|x\|=0$ iff $x=0$
  \item $\|x+y\|\leq \|x\|+\|y\|$
  \item $\|cx\|=|c|\|x\|$.
  \end{itemize}
\end{definition}

\begin{exa}
  \begin{enumerate}
   \item $V=C([a, b])$ (continuous functions on $[a, b]$), $\|f\|_\infty=\max|f|$. This is called the $L^\infty$ norm.
   \item $V=L^2([a, b])$, $\|f\|_2=(\int_a^b|f(x)|^2dx)^{1/2}$. This is called the $L^2$ norm.
   \item Replace $2$ with $p\geq 1$ we get $L^p$ norm. If $p<1$, the triangle inequality is no longer satisfied.  
  \end{enumerate}
\end{exa}


\begin{definition}
Let $L=span\{x_1, \dots x_m\}$ be a $m$-dimensional subspace of $V$, $x\in V$. The {\bf best approximation} of $x$ is the element $x'\in L$ that minimizes $\|x-x'\|$.  
\end{definition}

\begin{thm} (Theorem 8.2 in textbook) The best approximation always exists.\end{thm}

The proof has two steps:

\begin{enumerate}
\item $\|\cdot-x\|$ is continuous on $L$.
\item $\|\cdot-x\|$ goes to infinity at infinity.
\end{enumerate}

Key idea: if a function is defined on a finite dimensional vector space, continuous, and goes to infinity at infinity, then it has a minimum.


Please ignore the proof below if you are not interested.

\begin{proof}
  Let $x_1, \dots x_m$ be a basis of $L$. Consider a function $F_x: \mathbb{R}^m\rightarrow \mathbb{R}$ defined as
  \[F_x((t_1, \dots, t_m))=\|x-\sum_i t_ix_i\|\]
  The first step of the proof is to show that $F$ is continuous:
  \begin{lem}\label{cont} $F_x$ is continuous.\end{lem}
  \begin{proof}
  Suppose $t'\in\mathbb{R}^m$ satisfies that $|t_i'|<\epsilon$ for all $i$, then by triangle inequality,
  \[|F_x(t+t')-F_x(t)|\leq |\sum_i t_i'x_i|\leq \epsilon\sum_i |x_i|\]
  This implies that if $t'$ is sufficiently small, $F_x(t+t')$ can be arbitrarily close to $F_x(t)$, hence $F_x$ is continuous.
\end{proof}

  
  Now, let $D_R\subset\mathbb{R}^m=\{t:|t_i|\leq R\text{ for all }i\}$. It is a closed set, hence compact (recall the definition of compactness in your analysis class), hence a continuous function $F_x$ takes minimum at some point $x^*_R$ on $D_R$. We just need to show that if $R$ is large enough, $x^*_R$ is also the minimum of $F_x$.\\

  Let $g>0$ be the minimum of $F_0$ on the set $D_1$. Now we set $R_0=(2|x|+1)/g$. Then for any $y$ outside $D_{R_0}$, then $F_x(y)=\|y-x\|\geq \|x\|+1>F_x(0)\geq F_x(x^*_{R_0})$
\end{proof}


\subsection{Stone-Weiersterass theorem}

\begin{thm} (Theorem 8.1 in textbook) For continuous function $f\in C([a, b])$, any $\epsilon>0$, there is some polynomial $p$ such that $\|f-p\|_\infty<\epsilon$.
\end{thm}

There are many proofs, some work for more general settings. An easy proof is first use linear spline to approximate $f$, then use polynomials to approximate the basis function (which is a linear combination of absolute values, which can be approximated by $(x^2+\epsilon')^{1/2}$, which can be approximated using Taylor expansion).



\subsection{Approximation in inner product space}

Sometimes the norm on a vector space arises from a inner product (a symmetric, positive definite, bilinear form) $(\cdot, \cdot): V\times V\rightarrow \mathbb{R}$, by $||x||=\sqrt{(x, x)}$. If so, we call it an {\bf inner product space}.\\

\begin{exa}The $L^2$ norm on $L^2([a, b])$ arises from inner product $(f, g)=\int_a^b fg dx$. Let $w$ be a non negative, continuous and integrable ``weight function'' on $[a, b]$, we can also defined the ``weighted $L^2$ norm'' which is from $(f, g)_w=\int_a^bwfg dx$.\end{exa}

It's easy to see that the $L^2_w$ norm satisfies:
\[\|f\|_{w}\leq(\int_a^bwdx)^{1/2}\|f\|_\infty\]

\begin{exa}On $C^1([a, b])$ we can define the $(1, 2)$ Sobolev norm $\|f\|_{1, 2}=(\int_a^b|f(x)|^2+|f'(x)|^2dx)^{1/2}$. This norm also come from an inner product
  \[(f, g)_{1, 2}=\int_a^b f(x)g(x)+f'(x)g'(x)dx\]
\end{exa}


Let $L=span\{x_1,\dots,  x_m\}$, then we can use Gram-Schmidt process to get an orthonormal basis of $L$ under $(\cdot, \cdot)$, called $\{e_1,\dots, e_m\}$. Then we have:

\begin{thm}The best approximation of $x\in V$ by an element of $L$ is unique, and it is
  \[x^*=\sum_i(x, e_i)e_i\]
\end{thm}

\begin{proof}
  For any other $x'=\sum_i t_ie_i\in L$,
  \[\|x'-x\|^2=((x'-x^*)+(x^*-x), (x'-x^*)+(x^*-x))\]
  \[=\|x'-x^*\|^2+\|x^*-x\|^2+2(\sum_i(t_i-(x, e_i))e_i, \sum_i(x, e_i)e_i-x)\]
  \[=\|x'-x^*\|^2+\|x^*-x\|^2+2\sum_j(t_i-(x, e_j))(e_j, \sum_i(x, e_i)e_i-x)\]
  \[=\|x'-x^*\|^2+\|x^*-x\|^2+2\sum_j(t_i-(x, e_j))(\sum_i(x, e_j)(e_j, e_j)-(x, e_j))\]
  \[=\|x'-x^*\|^2+\|x^*-x\|^2\geq \|x^*-x\|^2\]
  And equality is reached only when $x'=x^*$.
\end{proof}


When the inner product is the $L^2_w$ inner product, the integrals in the formula for best approximation will often be calculated numerically (cf. next Section).\\

The proof is the same as the finite dimensional case you have seen in linear algebra.\\

If $x_i$ are only orthogonal and not orthonormal, the formula becomes
\[x^*=\sum_i{(x, x_i)\over (x_i, x_i)}x_i\]

If $x_i$ are not known to be orthogonal either, the formula becomes
\[x^*=\sum_i(\sum_j(x, x_j)(A^{-1})_{i, j})x_i\]
Where
\[A_{i,j}=(x_i, x_j)\]


\subsection{Orthogonal polynomials}

\begin{definition}We call $\phi_j$, $j=0, 1, 2, \dots$ a system of {\bf orthogonal polynomials} with weight $w$, if 
  \begin{enumerate}
   \item $\phi_j$ is of degree $j$.
   \item $\phi_j$ are orthogonal to each other in $L^2_w$ norm.
  \end{enumerate}
\end{definition}

\begin{thm}\label{orth_exist} If $w$ is positive, continuous and integrable on $(a, b)$ then a system of {\bf orthogonal polynomials} with weight $w$ exists. \end{thm}

\begin{proof}
  This is Gram-Schmidt applied to $\{1, x, x^2, x^3, \dots\}$.
  \[\phi_0=1\]
  \[\phi_j=x^j-\sum_{i=0}^{j-1}{\int_a^b wt^j\phi_i(t)dt\over \int_a^b w\phi_i^2(t)dt}\phi_i(x)\] 
\end{proof}


From linear algebra, we know that the system of orthogonal polynomials is unique up to scaling, since $\phi_j$ is the basis vector of the orthogonal complement of $span\{1, x, \dots, x^{j-1})$ in $span\{1, x, \dots, x^j\}$.\\


 \begin{rem}Stone-Weiersterass theorem implies that as degree increases, optimal approximation in $L^2_w([a, b])$ can become arbitrarily accurate. In other words, the orthogonal polynomials form an orthonormal basis of $L^2_w([a, b])$. (Which is NOT a basis in the sense of linear algebra. In algebra there is only finite sum.)
 \end{rem}


\begin{exa}
  Let $(a, b)=(-1, 1)$.
  \begin{itemize}
  \item If $w=1$, the resulting orthogonal polynomials are called the {\bf Legendre polynomials} $L_j$.
  \item If $w(x)=(1-x^2)^{-1/2}$, the resulting orthogonal polynomials are called the {\bf Chebyshev polynomials} $T_j$. 
  \end{itemize}
 \end{exa}

 \begin{rem}
The Chebyshev polynomials have a particularly nice formula:
\[T_j=\cos(j\cos^{-1}x)\ .\]
They are polynomials because
\[T_0=1, T_1=x, T_2=2x^2-1\]
\[T_j=\cos(j\cos^{-1}x)=x\cos((j-1)\cos^{-1}x)-\sin(\cos^{-1}x)\sin((j-1)\cos^{-1}x)\]
\[=x\cos((j-1)\cos^{-1}x)-\sin^2(\cos^{-1}(x))\cos((j-2)\cos^{-1}x)\]
\[-\sin(\cos^{-1}x)\sin((j-2)\cos^{-1}x)\cos(\cos^{-1}x)\]
\[=xT_{j-1}-(1-x^2)T_{j-2}-x(T_{j-3}-T_{j-1})/2\ .\]


 They are othogonal, because $j\not=j'$,
\[\int_{-1}^1 \cos(j\cos^{-1}x)\cos(j'\cos^{-1}x)(1-x^2)^{-1/2}dx\]
\[=-\int_{-1}^{1}\cos(j\cos^{-1}x)\cos(j'\cos^{-1}x)d\cos^{-1}(x)\]
\[=\int_0^\pi\cos(jt)\cos(j't)dt=0\]
\end{rem}

\begin{rem}Furthermore, if $T_j=\cos(j\cos^{-1}x)$, $2^{-j}T_{j+1}$ is the degree $j+1$ monic (leading coefficient being 1) polynomial with the smallest $L^\infty$ norm. This tells us that the term $\prod_i(x-x_i)$ in Theorem~\ref{err_lag} can be minimized (in $L^\infty$) if $x_i$ are chosen as the roots of Chebyshev polynomials, or, in other words, if $\prod_i(x-x_i)=2^{-n}T_{n+1}$. This is proved in Chapter 8 of the textbook.
\end{rem}


\begin{figure}[H]
\begin{tikzpicture}[yscale=1, xscale=1]
  \draw[->] (-1, 0) -- (1, 0) node[right] {$x$};
  \draw[->] (0, -1.5) -- (0, 1.5) node[above] {$y$};
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot ({\x}, {1});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot ({\x}, {\x});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot ({\x}, {2*\x*\x-1});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot ({\x}, {4*\x*\x*\x-3*\x});

    \draw[->] (2, 0) -- (4, 0) node[right] {$x$};
  \draw[->] (3, -1.5) -- (3, 1.5) node[above] {$y$};
  \draw[scale=1, domain=2:4, smooth, variable=\x, blue] plot ({\x}, {1});
  \draw[scale=1, domain=2:4, smooth, variable=\x, blue] plot ({\x}, {\x-3});
  \draw[scale=1, domain=2:4, smooth, variable=\x, blue] plot ({\x}, {(\x-3)*(\x-3)-1/3});
  \draw[scale=1, domain=2:4, smooth, variable=\x, blue] plot ({\x}, {(\x-3)*(\x-3)*(\x-3)-0.6*(\x-3)});
  
\end{tikzpicture}
\caption{Chebyshev polynomials and Lagrange polynomials}
\end{figure}


\begin{itemize}
\item First Legendre polynomials (which I calculated using Gram-Schmidt, another alternative calculation can be found in the exercises, and also HW 4)
  \[L_0=1, L_1=x, L_2=x^2-{1\over 3}\]
  \[L_3=x^3-{3\over 5}x\]

\item First Chebyshev polynomials:

  \[T_0=1, T_1=x, T_2=2x^2-1, T_3=4x^3-3x\]

\end{itemize}





\begin{thm}\label{orth_roots}
  If the weight function $w$ is positive, continuous and integrable on $(a, b)$, then $\phi_j$ has $j$ distinct real roots in $(a, b)$. 
\end{thm}
\begin{proof}
Suppose not, then $\phi_j$ switches sign fewer than $j$ times in $(a, b)$. Suppose $x_1, \dots, x_k$ are the points in $(a, b)$ where $\phi_j$ changes sign, then $(\phi_j, \prod_{i=1}^k(x-x_i))$ is non zero. However $\prod_{i=1}^k(x-x_i)\in span\{\phi_0, \dots \phi_{j-1}\}$, hence a contradiction.
\end{proof}

This Theorem will be used in the next section when we discuss Gauss's method for numerical integration.

\subsection{Review}

\begin{itemize}
\item Normed vector space, inner product space, $L^\infty$, $L^2$ and $L^2_w$ norms.
\item Existence of optimal approximation. Calculation of optimal approximation for inner product space.
\item Concept of orthogonal polynomials.
\end{itemize}


\begin{exa} Consider the function $y=e^x$ on $[-1, 1]$.
  \begin{itemize}
  \item Find the Lagrange interpolation polynomial, interpolating at $0$, $\pm 1$.
  \item Find the Hermite interpolation polynomial, interpolating at $\pm 1$.
  \item Find the best approximation via a polynomial of degree at most $2$, under the $L^2$ norm.
  \end{itemize}
\end{exa}

  Answer:
  \begin{itemize}
  \item Use formula $p_L=\sum_iy_i\prod_{j\not=i}(x-x_j)/\prod_{j\not=i}(x_i-x_j)$:
    \[p_L(x)=e^{-1}\cdot {x(x-1)\over -1\cdot -2}+1\cdot {(x+1)(x-1)\over 1\cdot -1}+e\cdot {x(x+1)\over 1\cdot 2}\]
    \[=(e^{-1}/2+e/2-1)x^2+(e/2-e^{-1}/2)x+1\]
  \item Use formula $p_H=\sum_iz_i(x-x_i)\prod_{j\not=i}(x-x_j)^2/\prod_{j\not=i}(x_i-x_j)^2+\sum_iy_i(1-(x-x_i)\sum_{j\not=i}(2/(x_i-x_j)))\prod_{j\not=i}(x-x_j)^2/\prod_{j\not=i}(x_i-x_j)^2$:
    \[p_H(x)=e^{-1}\cdot{(x+1)(x-1)^2\over (-1-1)^2}+e\cdot {(x-1)(x+1)^2\over (1+1)^2}\]
    \[+e^{-1}\cdot (1+(x+1))\cdot {(x-1)^2\over (-1-1)^2}+e\cdot (1-(x-1))\cdot {(x+1)^2\over (1+1)^2}\]
    \[=(e^{-1}/2)x^3+(e/4-e^{-1}/4)x^2+(e/2-e^{-1})x+e/4+3e^{-1}/4\]

    
    \item Use formula $x^*=\sum_i((x, x_i)/(x_i, x_i))x_i$:
  \[p_2(x)={\int_{-1}^1e^tdt\over \int_{-1}^1 1^2 dt}\cdot 1+{\int_{-1}^1 te^tdt\over \int_{-1}^1 t^2}\cdot x+{\int_{-1}^1(t^2-1/3)e^tdt\over \int_{-1}^1 (t^2-1/3)^2dt}\cdot (x^2-1/3)\]

  \[={(e-e^{-1})\over 2}\cdot 1+{2e^{-1}\over 2/3}\cdot x+{2e/3-14e^{-1}/3\over 8/45}(x^2-1/3)\]
  \[={15e-105e^{-1}\over 4}x^2+3e^{-1}x+{-3e+33e^{-1}\over 4}\]
  \end{itemize}



\begin{figure}[H]
\begin{tikzpicture}[yscale=2, xscale=2]
  \draw[->] (-1, 0) -- (1, 0) node[right] {$x$};
  \draw[->] (0, -0.5) -- (0, 3) node[above] {$y$};
  \draw[scale=1, domain=-1:1, smooth, variable=\x, thick, black] plot ({\x}, {exp(\x)});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot({\x}, {(exp(-1)/2+exp(1)/2-1)*\x*\x+(exp(1)/2-exp(-1)/2)*\x+1});
\draw[scale=1, domain=-1:1, smooth, variable=\x, green] plot({\x}, {(exp(-1)/2)*\x*\x*\x+(exp(1)/4-exp(-1)/4)*\x*\x+(exp(1)/2-exp(-1))*\x+exp(1)/4+3*exp(-1)/4});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, red] plot({\x}, {(30*exp(1)-210*exp(-1))/8*\x*\x+3*exp(-1)*\x+(-3*exp(1)+33*exp(-1))/4});
\end{tikzpicture}
\caption{Black line: $y=e^x$. Blue line: Lagrange interpolation. Green line: Hermite interpolation. Red line: $L^2$ best approximation}
\end{figure}



\section{Numerical Integration (Chapter 7, 10)}

\subsection{Quadrature rule}

Question: Estimate $\int_a^bf(x)dx$.\\

Let $x_0=a<x_1<\dots<x_n=b$ be $n+1$ distinct points in $[a, b]$, then we can use the Lagrange interpolation polynomial to estimate $f$, and hence 
\[\int_a^b f(x)dx\approx \sum_kw_kf(x_k)\]
  Where
  \[w_k=\int_a^b {\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)} dx\]
To estimate the integration.\\

The points $x_i$ are called {\bf quadrature points}, and $w_i$ called {\bf quadrature weights}. The formula still works if some $x_i$ is outside $[a, b]$.


\subsection{Newton-Cotes method}

\begin{definition} When $a=x_0<x_1<\dots<x_n=b$ are $n+1$ evenly spaced points, the formula above is called the {\bf Newton-Cotes formula}, the evenly spaced $x_i$ the {\bf Newton-Cotes quadrature}.
 \end{definition}

\begin{exa} When $n=1$, $w_0=w_1={b-a\over 2}$, this is called the {\bf Trapezium rule} (as it's like calculating the area of a collection of trapeziums). When $n=2$, $w_0=w_2={b-a\over 6}$, $w_1={2(b-a)\over 3}$. This is called {\bf Simpson's rule}.
\end{exa}

\begin{exa} $\int_0^1\sin(x)dx$. Using Trapezium rule, the estimate is
  \[{\sin(0)+\sin(1)\over 2}=0.4207\]
  Using Simpson's rule, the estimate is
  \[\sin(0)/6+\sin(0.5)*2/3+\sin(1)/6=0.4599\]
  The true value is $1-\cos(1)=0.4597$.
\end{exa}


\begin{thm} \label{bound1}(Theorem 7.1 in the textbook)
The error for the quadrature is bounded by
\[{\max|f^{(n+1)}|\over (n+1)!}\int_a^b\prod_i|x-x_i| dx\]
\end{thm}

The proof is follows immediately from the error estimate of Lagrange interpolation. When $x_i$ are Newton-Cotes quadrature, the error bound is $O(\max|f^{(n+1)}|(b-a)^{n+2})$, because when we scale the interval $[a, b]$ by $C$, the function being integrated is scaled by $C^{n+1}$ while the range of the integration is also scaled by $C$.


For Newton-Cotes, when $n$ is even (in order words when we have odd number of quadrature points), the error bound can be improved to $\max|f^{(n+2)}|\cdot O((b-a)^{n+3})$ provided $f\in C^{n+2}$:

\begin{thm}\label{qbound} Let $n$ be an even number, $f\in C^{n+2}([a, b])$, $I_n(f)$ the Newton-Cotes formula using $n+1$ evenly spaced points on $[a, b]$, then there is some $C_n$ (depending on $n$) such that
  \[|\int_a^bf(x)dx-I_n(f)|\leq C_n\max|f^{(n+2)}|(b-a)^{n+3}\]
\end{thm}
\begin{proof}
  The uniqueness of Lagrange interpolation implies that if $f$ is a polynomial of degree at most $n$, $\int_a^bf(x)dx=I_n(f)$. Now consider the polynomial $g=\prod_i(x-x_i)$. Because $x_i$ are evenly spaced, the graph of $\prod_i(x-x_i)$ is symmetric with respect to the point $(x_{n/2}, 0)$ where $x_{n/2}=(a+b)/2$. So $\int_a^bgdx=0=I_n(g)$. However, any polynomial of degree at most $n+1$ can be written in the form $cg+h$, where $h$ is a polynomial of degree at most $n$. Hence, if $f$ is any polynomial of degree at most $n+1$, $\int_a^bf(x)dx=I_n(f)$.\\

  
  Now suppose $f\in C^{n+2}$. Let $x_{n+1}$ be the midpoint of $[x_0, x_1]$, let $p'$ be the Lagrange interpolation polynomial of $f$, then $f-p'$ vanishes at $x_0, \dots, x_n, x_{n+1}$, hence the quadrature formula using $x_0, \dots x_{n+1}$ is $0$. Apply Theorem~\ref{bound1} we get
  \[|\int_a^b(f-p')dx|\leq C_n\max|f^{n+2}|(b-a)^{n+3}\]
  However, because $p'(x_i)=f(x_i)$ for $i=0, 1, 2, \dots, n$,
  \[\int_a^bp'dx=I_n(p')=I_n(f)\]
  Which proves the theorem.
\end{proof}


We can also use mean value theorem to get finer bounds. As an example, when $n=2$, we have

\begin{thm}(Theorem 7.2 in the textbook) If $f\in C^4$, there is some $c\in [a, b]$ such that
  \[\int_a^b f(x)dx-(b-a)\cdot(f(a)/6+2f((a+b)/2)/3+f(b)/6)=-{f^{(4)}(c)(b-a)^5\over 2880}\]
\end{thm}

\begin{proof}
  Let
  \[G_1(t)=\int_{(a+b)/2-t}^{(a+b)/2+t}f(s)ds-2t\cdot(f((a+b)/2-t)/6\]
  \[+2f((a+b)/2)/3+f((a+b)/2+t)/6)\]
  \[G(t)=G_1(t)-({t\over (b-a)/2})^5G_1((b-a)/2)\]
  Then $G(0)=G((b-a)/2)=G'(0)=G''(0)=0$. So there is $0<c_1<(b-a)/2$ such that $G'(c_1)=0$, $0<c_2<c_1$ such that $G''(c_2)=0$, $0<c_3<c_2$ such that $G'''(c_3)=0$.\\

By calculation, $G_1'''(c_3)={c_3\over 3}\cdot(f'''((a+b)/2-c_3)-f'''((a+b)/2+c_3))$, so
\[{c_3\over 3}\cdot(f'''((a+b)/2-c_3)-f'''((a+b)/2+c_3))-{1920\over (b-a)^5}c_3^2G_1((b-a)/2)=0\]
Hence
\[{\cdot(f'''((a+b)/2+c_3)-f'''((a+b)/2-c_3))\over 2c_3}=-{2880\over (b-a)^5}G_1((b-a)/2)\]
Now apply mean value theorem for $f'''$ on $[(b+a)/2-c_3, (b+a)/2+c_3]$, we get the $c$. 
\end{proof}

\begin{proof}[Alternative proof]
  Following the same argument as Theorem~\ref{qbound}, we know that if $f$ is a polynomial of degree $3$, $\int_a^bfdx$ equals the result of Simpson's rule.\\

  Now let $p$ be the Lagrange interpolation of $f$ at four distinct points $a$, $b$, $a+b\over 2$ and $d$. Then
  \[\int_a^bpdx=(b-a)({p(a)\over 6}+{2p((a+b)/2)\over 3}+{p(b)\over 6})\]
  \[=(b-a)({f(a)\over 6}+{2f((a+b)/2)\over 3}+{f(b)\over 6})\]
  However, by error bound of Lagrange polynomials, 
  \[f(x)-p(x)={f^{(4)}(c)(x-a)(x-b)(x-(a+b)/2)(x-d)\over 4!}\]
  Now push $d$ towards $c$, which makes $(x-a)(x-b)(x-(a+b)/2)(x-d)$ non positive on $[a, b]$. Now integrate for $x\in [a, b]$ and use integration mean value theorem, we get the theorem.
\end{proof}


\subsection{Composite Method}

For the same reason as in Example~\ref{exa2}, when $n\rightarrow\infty$ the error can not be guaranteed to decay to $0$. So we often evenly decompose the interval $[a, b]$ into subintervals then carry out low order Newton-Cotes.\\

Let $a=x_0<\dots<x_n=b$ be $n+1$ evenly spaced points on $[a, b]$. If $n=dm$, we can cut $[a, b]$ into $m$ subintervals each with $d+1$ quadrature points, and apply Newton-Cotes on each.\\

For example, if $d=1$, we cut $[a, b]$ into $n$ subintervals and apply trapezium rule on each we get
\[{b-a\over n}(f(x_0)/2+\sum_{i=1}^{n-1}f(x_i)+f(x_n)/2)\]
If $d=2$, we cut $[a, b]$ into $n/2$ subintervals, and apply simpson's rule on each, we get
\[{b-a\over 3n}(f(x_0)+4\sum_{i=1}^{n/2}f(x_{2i-1})+2\sum_{i=1}^{n/2-1}f(x_{2i})+f(x_n))\]



When $f$ is smooth with bounded higher order derivatives, the error estimate for each subinterval is $O(n^{-3})$ and $O(n^{-5})$ using the trapezium and Simpson's rule. Hence, the error composite trapezium and composite Simpson's rules decay like $O(n^{-2})$ and $O(n^{-4})$ respectively.


\begin{exa} $\int_0^1\sin(x)dx$. For this case, the error for composite trapezium \& Simpson's rule can be calculated explicitly. \end{exa}

\begin{itemize}
\item Composite trapezium rule with $n+1$ points:
  \[I_n={\sum_{i=1}^{n-1}\sin({i\over n})+sin(1)/2\over n}\]
  \[={\sum_{i=1}^{n-1}(\cos({i-1/2\over n})-\cos({i+1/2\over n}))+\sin({1\over 2n})\sin(1)\over 2n\sin({1\over 2n})}\]
  \[={\cos({1\over 2n})-\cos(1)\cos({1\over 2n})\over 2n\sin({1\over 2n})}=(1-\cos(1))\cdot{\cos({1\over 2n})\over 2n\sin({1\over 2n})}\]
  And it is easy to see that
  \[{\cos({1\over 2n})\over 2n\sin({1\over 2n})}=1-{1\over 3}(2n)^{-2}+\dots\]
  So the error decay at $O(n^{-2})$.


\item Composite Simpson's rule with $2m+1$ points:
  \[I_m={4\sum_{i=1}^{m}\sin({2i-1\over 2m})+2\sum_{i=1}^{m-1}\sin({i\over m})+sin(1)\over 6m}\]
  \[={2(1-\cos(1))+\cos({1\over 2m})-\cos({2m-1\over 2m})+\sin(1)\sin({1\over 2m})\over 6m\sin({1\over 2m})}\]
  \[=(1-\cos(1))\cdot{2+\cos({1\over 2m})\over 6m\sin({1\over 2m})}\]
  \[=(1-\cos(1))\cdot{3-(2m)^{-2}/2+(2m)^{-4}/24+O((2m)^{-6})\over 3-(2m)^{-2}/2+(2m)^{-4}/40+O((2m)^{-6})}\]
  \[=(1-\cos(1))+O(m^{-4})\]
\end{itemize}


\begin{exa}$\int_{-0.5}^{0.5}\sqrt{1-x^2}dx$. The right answer should be
  \[\sqrt{3}/4+\pi/6=0.9566114774905181\]
\end{exa}

\begin{enumerate}
  \item Trapezium rule:
    \[(\sqrt{3/4}+\sqrt{3/4})/2=0.8660254037844386\]
  \item Simpson's rule:
    \[\sqrt{3/4}/6+1\times 2/3+\sqrt{3/4}/6=0.9553418012614795\]
  \end{enumerate}

\RestyleAlgo{boxruled}
\LinesNumbered
 \begin{algorithm}[H]
 $r\leftarrow f(a)+f(b)$\;  
 \For{$i=1, \dots n-1$}{
   $r\leftarrow r+2\times f({(n-i)a+ib\over n})$\;
 }
 The answer is ${(b-a)r\over 2n}$\;
 \caption{Composite Trapezium rule}
\end{algorithm}


\RestyleAlgo{boxruled}
\LinesNumbered
 \begin{algorithm}[H]
 $r\leftarrow f(a)+f(b)$\;  
 \For{$i=1, \dots n-1$}{
   \uIf{$i$ is odd}{
     $r\leftarrow r+4\times f({(n-i)a+ib\over n})$\;
   }
   \Else{
     $r\leftarrow r+2\times f({(n-i)a+ib\over n})$\;
   }
 }
 The answer is ${(b-a)r\over 3n}$\;
 \caption{Composite Simpson's rule}
\end{algorithm}


 
\begin{lstlisting}[language=Python]
from math import *
f=lambda x : (1-x*x)**0.5
def composite_trapezium(n, a, b, f):
    r=0
    r+=0.5*(f(a)+f(b))
    for i in range(1, n):
        r+=f(((n-i)*a+i*b)/n)
    return r*(b-a)/n
def composite_simpsons(n, a, b, f):
    r=0
    r+=f(a)+f(b)
    for i in range(1, n, 2):
        r+=4*f(((n-i)*a+i*b)/n)
    for i in range(2, n, 2):
        r+=2*f(((n-i)*a+i*b)/n)
    return r*(b-a)/3/n
\end{lstlisting}


\begin{tikzpicture}
	\begin{axis}[
		xlabel=$log(n)$,
		ylabel=$-log(Error)$]
	\addplot[color=red,mark=x, only marks] coordinates {
		(0.6931471805599453,3.746560449724477)
(1.0986122886681098,4.547453810338138)
(1.3862943611198906,5.118965503519308)
(1.6094379124341003,5.5633925579666705)
(1.791759469228055,5.927002052110047)
(1.9459101490553132,6.2346716549051004)
(2.0794415416798357,6.501320810552323)
(2.1972245773362196,6.73660161087667)
(2.302585092994046,6.947117728247236)
(2.3978952727983707,7.137586004803848)
(2.4849066497880004,7.311492816297971)
(2.5649493574615367,7.4714878376833544)
(2.6390573296152584,7.6196319546081686)
(2.70805020110221,7.757559684401888)
(2.772588722239781,7.886589202729626)
(2.833213344056216,8.00779902926391)
(2.8903717578961645,8.12208280373383)
(2.9444389791664403,8.23018925813059)
(2.995732273553991,8.332751940018328)
(3.044522437723423,8.43031168655338)
(3.091042453358316,8.52333387219665)
(3.1354942159291497,8.612221823753858)
(3.1780538303479458,8.697327381267385)
(3.2188758248682006,8.778959303716851)
(3.258096538021482,8.857390026592949)
(3.295836866004329,8.932861144496894)
(3.332204510175204,9.00558789696487)
(3.367295829986474,9.075762867305217)
(3.4011973816621555,9.143559054538375)
};

	\addplot[color=blue,mark=+, only marks] coordinates {
	(1.3862943611198906,9.114856038771517)
(1.791759469228055,10.641478993908981)
(2.0794415416798357,11.752422282602218)
(2.302585092994046,12.624884254357372)
(2.4849066497880004,13.342684719274926)
(2.6390573296152584,13.95214413140681)
(2.772588722239781,14.481537873892268)
(2.8903717578961645,14.949379792923526)
(2.995732273553991,15.368444387441553)
(3.091042453358316,15.7479127618939)
(3.1780538303479458,16.09460251736614)
(3.258096538021482,16.413713532022914)
(3.332204510175204,16.70930151946475)
(3.4011973816621555,16.98459025133957)
	};
      \end{axis}
      
\end{tikzpicture}


If we do $[a, b]=[-1, 1]$, with the same function as above, we get:

\begin{tikzpicture}
	\begin{axis}[
		xlabel=$log(n)$,
		ylabel=$-log(Error)$]
	\addplot[color=red,mark=x, only marks] coordinates {
(0.6931471805599453,0.560722828587799)
(1.0986122886681098,1.1592620458654719)
(1.3862943611198906,1.5858633733845402)
(1.6094379124341003,1.9175974979571417)
(1.791759469228055,2.1890810890660632)
(1.9459101490553132,2.418874071480326)
(2.0794415416798357,2.6180935155697216)
(2.1972245773362196,2.793928232434429)
(2.302585092994046,2.9512961008710596)
(2.3978952727983707,3.0937101353866443)
(2.4849066497880004,3.2237673778435036)
(2.5649493574615367,3.3434420342796574)
(2.6390573296152584,3.4542699820784497)
(2.70805020110221,3.557469639536541)
(2.772588722239781,3.6540238458643706)
(2.833213344056216,3.744736933949296)
(2.8903717578961645,3.8302755020496635)
(2.9444389791664403,3.9111981709248282)
(2.995732273553991,3.9879777148660915)
(3.044522437723423,4.061017798022754)
(3.091042453358316,4.130665820977959)
(3.1354942159291497,4.197222914402594)
(3.1780538303479458,4.260951807849626)
(3.2188758248682006,4.322083093796923)
(3.258096538021482,4.380820264323835)
(3.295836866004329,4.437343798158711)
(3.332204510175204,4.491814505168301)
(3.367295829986474,4.544376284523874)
(3.4011973816621555,4.595158415724219)
};

	\addplot[color=blue,mark=+, only marks] coordinates {
(1.3862943611198906,2.491780761461278)
(1.791759469228055,3.105837770018371)
(2.0794415416798357,3.5405025767733957)
(2.302585092994046,3.8771701458682797)
(2.4849066497880004,4.151981616128501)
(2.6390573296152584,4.384170395855168)
(2.772588722239781,4.585196798057869)
(2.8903717578961645,4.762442852040901)
(2.995732273553991,4.920943489214822)
(3.091042453358316,5.064286761613613)
(3.1780538303479458,5.195120026882598)
(3.258096538021482,5.315452488738959)
(3.332204510175204,5.426845119840145)
(3.4011973816621555,5.530534801579673)
	};
      \end{axis}
    \end{tikzpicture}

    
   Because the derivatives go to infinity at $x$ close to $\pm 1$, both methods only have $O(n^{-1})$ error bound.


\subsection{Friday Review And Examples}
   
   \begin{itemize}
   \item Interpolation
     \begin{itemize}
      \item Lagrange interpolation
      \item Hermite interpolation
      \item Uniqueness and error estimate
      \end{itemize}
    \item Approximation theory
      \begin{itemize}
        \item Normed vector space and inner product space
        \item Gram-Schmidt
        \item Orthogonal projection
        \end{itemize}
      \item Numerical integration
        \begin{itemize}
        \item Quadrature rule: $I=\sum_{i=0}^nw_if(x_i)$
        \item Newton-Cotes quadrature, $n=1, 2$
        \item Error estimate via error estimate of Lagrange interpolation
        \item Composite methods
        \end{itemize}
      \end{itemize}
      Example: $f(x)=\sin(x)$, $x\in [0, \pi]$, $x_0=0, x_1=\pi/2, x_2=\pi$.
      \begin{itemize}
      \item The Lagrange interpolation polynomial is
        \[p(x)=0\cdot {(x-\pi/2)(x-\pi)\over \pi^2/2}+1\cdot {x(x-\pi)\over -\pi^2/4}+0\cdot {(x-\pi/2)x\over \pi^2/2}\]
      \item The numerical integration using Simpson's rule is
        \[I(f)=\int_0^\pi p(x)dx=0\cdot {\pi\over 6}+1\cdot {2\pi\over 3}+0\cdot {\pi\over 6}={2\pi\over 3}\]
      \item The error estimate for Lagrange interpolation:
        \[|f(x)-p(x)|=\left|{f'''(s)x(x-{\pi/2})(x-\pi)\over 3!}\right|\leq {1\over 6}|x(x-\pi/2)(x-\pi)|\]
      \item Integrate the error estimate above, we get
        \[|\int_0^\pi f(x)dx-I(f)|\leq {1\over 6}\int_0^\pi|x(x-\pi/2)(x-\pi)|={1\over 6}\cdot \pi^4 \cdot{1\over 32}={\pi^4\over 192}\approx 0.507\]
      \item We can find an alternative error estimate via the following procedure
        \begin{enumerate}
         \item We see that if $g$ is a polynomial of degree $2$, $g$ equals its Lagrange interpolation at $x_0, x_1, x_2$, hence $I(g)=\int_0^\pi gdx$.
         \item Furthermore, if $f_3=x(x-\pi/2)(x-\pi)$, $I(f_3)=0=\int_0^\pi f_3dx$.
         \item Hence, if $g_3$ is a polynomial of degree 3, then $g_3=af_3+g$ for some $a\in\mathbb{R}$, and $g$ is a polynomial of degree $2$. Hence
           \[I(g_3)=aI(f_3)+I(g)=\int_0^\pi g(x)dx=\int_0^\pi (af_3+g)dx=\int_0^\pi g_3dx\]
         \item Now Let $p$ be the Lagrange interpolation polynomial of $f$ at $x_0, x_1, x_2, x_3=c$, then $\int_0^\pi pdx=I(p)=I(f)$.
         \item The error estimate for Lagrange interpolation gives us
           \[|f(x)-p(x)|=\left|{f''''(s)x(x-c)(x-{\pi/2})(x-\pi)\over 4!}\right|\]
           \[\leq {1\over 24}|x(x-c)(x-\pi/2)(x-\pi)|\]
         \item Integrate the error estimate, let $c\rightarrow {\pi/2}$, we get
           \[|\int_0^\pi f(x)dx-I(f)|\leq {1\over 24}\int_0^\pi |x(x-\pi/2)^2(x-\pi)| dx={\pi^5\over 2880}\approx 0.106\]
           This is the bound in Theorem 7.2 in the textbook.
         \end{enumerate}
       \item The actual error is $2\pi/3-2=0.094$.
      \end{itemize}
      Exercises 1: Let $f(x)=x^3(1-x)$, $x\in [0, 1]$. $x_0=0$, $x_1=c$, $x_2=1$, $c\in (0, 1)$.
      \begin{itemize}
      \item Find the Lagrange interpolation of $f$ at $x_0$, $x_1$, $x_2$.
      \item Integrate the Lagrange interpolation on $[0, 1]$ to get an estimate of $\int_0^1f(x)dx$. 
      \item Find $c$ such that the estimate you found above is optimal.
      \end{itemize}
      
      Exercise 2: Let $[a, b]=[0, 1]$, $x_0=0$, $x_1=0.5$, $x_2=1$.
      \begin{itemize}
      \item Write down the formula for Simpson's rule and the composite trapezium rule using $x_i$.
      \item Find a continuous function on $[a, b]$ where the Simpson's rule works better than the composite trapezium rule, and vice versa.
      \end{itemize}
      
       Answer:\\

       Exercises 1:\\
       \begin{itemize}
       \item $p=c^3(1-c)\cdot {x(x-1))\over c(c-1)}=-c^2x(x-1)$.
       \item $I=c^2/6$. 
       \item $\int_0^1f(x)dx=1/20$, so $c$ should be $\sqrt{0.3}$.
       \end{itemize}
      
       Exercise 2:\\
       \begin{itemize}
       \item Simpson's rule: $f(0)/6+2f(0.5)/3+f(1)/6$. Composite trapezium rule: $f(0)/4+f(0.5)/2+f(1)/4$.
       \item $f(x)=x^2$. $f(x)=|x-0.5|$. 
       \end{itemize}

       
       Correction for the lecture on 11/6: We can also estimate the integral by decomposing the interval into $n$ subintervals with the same length, and calculate the sum of the areas of rectangles with those subintervals as base. If the height is taken as the value of the function at end points (i.e. $I={b-a\over n}\sum_{i=0}^{n-1}f(x_i)$), or use the maximum or minimum on the interval as in the definition of Riemann integration, then the error grows as $O(1/n)$ if $f$ is smooth, where $n$ is the number of subintervals, which is worse than composite trapezium rule. However, if the height is taken as the value of the function at midpoints (i.e. $I={b-a\over n}\sum_{i=0}^{n-1}f({x_i+x_{i+1}\over 2})$) the error grows as $O(1/n^2)$ if $f$ is smooth (so it's about as accurate as the trapezium rule).
       
  \subsection{Gauss quadrature (Also see Sections 10.2-10.4 in the textbook)}
  Motivation:
  \begin{enumerate}
  \item From the error bound of composite rules, we know that if the quadrature has error bound $O(|b-a|^k)$, the composite rule will have an error bound of $O(n^{-k+1})$.
  \item So suppose $f$ is sufficiently smooth, it might be good to try and make the number $k$ as large as possible.
  \item From the proof of Theorem~\ref{qbound}, we see that if a quadrature rule gives accurate answer to polynomials of degree $d$, then the error bound is $O(|b-a|^{d+2})$. For example, for Simpson's rule $d=3$. 
  \item So, it may be good to {\bf strategically choose the quadrature points} such that the quadrature rule works for polynomials of high degrees. 
  \end{enumerate}

  Recall from the definition of Legendre polynomial, if $L_j$ are the Legendre polynomials, then
  \[L_j\perp span\{L_0, L_1, \dots L_{j-1}\}\]
  Under $L^2([-1, 1])$ norm.\\


  \begin{thm}\label{high_poly} Let $I(f)$ be the result of quadrature rule on $[-1, 1]$, using quadrature points  $x_i$, $i=0, \dots, n$, which the roots of $L_{n+1}$. For any polynomial $g$ of degree no more than $2n+1$, $I(g)=\int_{-1}^1gdx$.\end{thm}

  \begin{proof}
  It's easy to see that
  \[span\{1, x, \dots x^{2n+1}\}=span\{L_0, \dots L_{n+1}, L_{n+1}L_1, \dots, L_{n+1}L_n\}\]
  (A way to see it is by first recognizing that $span\{1, x, \dots x^{2n+1}\}$ has dimension $2n+2$, and show, from definition, that
  \[\{L_0, \dots L_{n+1}, L_{n+1}L_1, \dots, L_{n+1}L_n\}\]
  is a linearly independent set of $2n+2$ elements (because they all have different degrees) hence must be a basis.)\\


  Because both $I$ and $\int_{-1}^1$ are linear on the vector space consisting of polynomials of degree no more than $2n+1$, and two linear transformations are the same if and only if they are identical on the basis vectors, we only need to prove it when $g$ is $L_j$, $0\leq j\leq n$, as well as when $g$ is $L_jL_{n+1}$, $0\leq j\leq n$.
  \begin{itemize}
  \item Case 1: $g=L_j$ for some $j\leq n$. In this case, $p$ is of degree no more than $n$, hence $p$ is identical to its Lagrange interpolation at $n+1$ points. Hence $I(g)=\int_{-1}^1gdx$.
   \item Case 2: $g=L_jL_{n+1}$, $j\leq n$. Note that $L_{n+1}$ is proportional to $L_{n+1}L_0$ as $L_0$ is of degree $0$ hence a constant. Because $g$ has a factor $L_{n+1}$, $g(x_i)=0$ for all $i$, hence $I(g)=0$. On the other hand, because $L_ {n+1}$ and $L_j$ are orthogonal on $L^2([-1, 1])$, $\int_{-1}^1gdx=0$
  \end{itemize}
  \end{proof}

  \begin{definition}
  The Gauss-Legendre quadrature points $x_0,\dots, x_n$ on $[a, b]$ is defined as $x_j={a+b\over 2}+c_j{b-a\over 2}$, where $c_j$ is the $j+1$-th root of the $n+1$-th Legendre polynomial. In other words, $x_j$ is the $j+1$-th root of the weight-$1$ orthogonal polynomial on $[a, b]$ with index $n+1$, $\psi_{n+1}$.
\end{definition}

  Now we can use Theorem~\ref{high_poly} to prove an error bound for Gauss-Legendre quadrature:
  \begin{thm}(Theorem 10.1 in textbook)
    Let $f\in C^{2n+2}$, then there is some $c\in [a,  b]$ such that
    \[\int_a^bfdx-I(f)={f^{(2n+2)}(c)\over (2n+2)!}\int_a^b\prod_i(x-x_i)^2 dx\]
  \end{thm}

  \begin{proof}
    Let $H$ be the Hermite interpolation polynomial of $f$ at $x_i$, then Theorem~\ref{high_poly} implies that $\int_a^bHdx=I(H)=I(f)$. Hence the result follows from the error bound of Hermite interpolation (Theorem~\ref{err_her}).\\


  More precisely, if $M$ and $m$ are the upper and lower bound of $f^{(2n+2)}$ on $[a, b]$ respectively, we have
    \[{m\prod_i(x-x_i)^2\over(2n+1)!}\leq f(x)-H(x)\leq {M\prod_i(x-x_i)^2\over(2n+1)!}\]

  Hence
    \[{m\over (2n+2)!}\int_a^b\prod_i(x-x_i)^2 dx\leq \int_a^bfdx-I(f)\leq {M\over (2n+2)!}\int_a^b\prod_i(x-x_i)^2\]
    And the existence of $c$ follows from intermediate value theorem in analysis.
  \end{proof}

  
  \begin{rem}
As $|b-a|\rightarrow 0$ the error decay at $|b-a|^{2n+3}$, which is better than the $|b-a|^{n+2}$ or $|b-a|^{n+3}$ in Newton-Cotes.
    \end{rem}
  

    \begin{exa} Use $n=1$ Gauss-Legendre quadrature to estimate $\int_0^\pi \sin(x)dx$.
    \end{exa}

    Firstly find the quadrature points. The first method is by using the formula of Legendre polynomials in HW4 problem 4, which is ${(1-x^2)^2}''=12x^2-4$, so roots are $\pm{1\over\sqrt{3}}$, so $x_0={\pi/2}(1-1/\sqrt{3})=0.6639$, $x_1={\pi/2}(1+1/\sqrt{3})=2.4777$.

    The second method is by finding a monic quadratic polynomial orthogonal to both $1$ and $x$ under $L^2([0, \pi])$. In other words, we need $a$ and $b$ such that
\[\int_0^\pi (x^2+ax+b)dx={\pi^3\over 3}+{a\pi^2\over 2}+b\pi=0\]
\[\int_0^\pi (x^2+ax+b)xdx={\pi^4\over 4}+{a\pi^3\over 3}+{b\pi^2\over 2}=0\]
Solve for $a$ and $b$ then find the roots.\\


    The quadrature weights are 
\[w_0=\int_0^\pi{x-x_1\over x_0-x_1}dx={x_1\pi-\pi^2/2\over x_1-x_0}=\pi/2=1.5708\]
\[w_1=\int_0^\pi{x-x_0\over x_1-x_0}dx={\pi^2/2-x_0\pi\over x_1-x_0}=\pi/2=1.5708\]
    So 
    \[I_1(\sin)=w_0\sin(x_0)+w_1\sin(x_1)=1.9358\]
    Error is $0.064$.\\

Another key property of the Gauss-Legendre quadrature is that all its weights are positive, because we have
\begin{thm}\label{pos_wt}
  The weight for Gauss-Legendre quadrature is
  \[w_k=\int_a^b{\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}dx=\int_a^b\left({\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}\right)^2dx\]
\end{thm}

\begin{proof}
Suppose $\psi_{n+1}=c\prod_i(x-x_i)$ is the weight 1 orthogonal polynomial on $[a, b]$ with index $n+1$, then
\[\int_a^b{\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}-\left({\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}\right)^2dx\]
\[={1\over(\prod_{j\not=k}(x_k-x_j)^2}\int_a^b\prod_{j\not=k}(x-x_j)(\prod_{j\not=k}(x_k-x_j)-\prod_{j\not=k}(x-x_j))dx\]
Let $h(x)=\prod_{j\not=k}(x_k-x_j)-\prod_{j\not=k}(x-x_j)$, then $\deg(h)=n$, and $h(x_k)=0$, hence $h=(x-x_k)h_1$ where $\deg(h_1)=n-1$. Now we have
\[\int_a^b\prod_{j\not=k}(x-x_j)(\prod_{j\not=k}(x_k-x_j)-\prod_{j\not=k}(x-x_j))dx=\int_a^b(\psi_{n+1}\cdot{h_1\over c})dx=0\]
\end{proof}


\begin{rem} For comparison, when $n=8$, the Newton-Cotes quadrature weights are not all positive.\end{rem}

As a consequence, we have:
\begin{thm} (Theorem 10.2 in textbook) Given any continuous function $f$, the Gauss-Legendre quadrature result $I_n(f)$ converges to $\int_a^bfdx$ as $n\rightarrow\infty. $\end{thm}
\begin{proof}
  By Weierstrass approximation theorem there is some $p$ such that $|f-p|<\epsilon$ on $[a, b]$, hence for any $n>\deg(p)$,
  \[|\int_a^bfdx-I_n(f)|\leq |\int_a^bfdx-\int_a^bpdx|+|\int_a^bpdx-I_n(p)|+|I_n(p)-I_n(f)|\]
  \[\leq \epsilon(b-a)+0+\epsilon(b-a)\]
  The last bit is due to
  \[|I_n(p)-I_n(f)|=|\sum_iw_i(p(x_i)-f(x_i))|\leq \epsilon\sum_i|w_i|=\epsilon\sum_iw_i\]
  And $I_n(1)=\int_a^b1dx=b-a$ so $\sum_iw_i=b-a$.  Now let $\epsilon\rightarrow 0$ we get the convergence.
\end{proof}
If $w_i$ are not all positive, one can not remove the absolute value, hence this may not be true for other quadrature rules.


    \begin{rem} The Gauss-Legendre quadrature can be generalized to deal with the problem of estimating $\int_a^bfwdx$ where $w$ is a given weight function not depend on $f$, by replacing Legendre polynomials with other orthogonal polynomials, which are called ``Gauss quadrature''.
    \end{rem}

 \begin{definition}
  The Gauss quadrature points with weight $w$, $x_0,\dots, x_n$ on $[a, b]$ is defined as $x_j$ is the $j+1$-th root of the weight-$w$ orthogonal polynomial on $[a, b]$ with index $n+1$, $\psi_{n+1}$. The quadrature weights are chosen as $w_k=\int_a^bw{\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}dx$.
\end{definition}

 For example, if $[a, b]=[-1, 1]$, and $w$ is $(1-x^2)^{-1/2}$ we call it Chebyshev-Gauss quadrature.

\begin{thm}
  \begin{enumerate}
  \item $w_k=\int_a^bw\left({\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}\right)^2dx$ hence is always positive.
\item If $f$ is a polynomial of degree no more than $2n+1$, then $\int_a^bwfdx=I_n(f)$, where $I$ is the Gauss quadrature rule with $n+1$ quadrature points.
\item If $f$ is continuous on $[a, b]$, $\lim_{n\rightarrow\infty}I_n(f)=\int_a^bwfdx$ (Theorem 10.2 in textbook).
\item If $f\in C^{2n+2}$, then there is some $c\in [a, b]$, where $\int_a^bwfdx-I(f)={f^{(2n+2)}(c)\over (2n+2)!}\int_a^bw\prod_i(x-x_i)^2 dx$. (Theorem 10.1 in textbook)
  \end{enumerate}
\end{thm}

The proof is very similar to the Gauss-Legendre case.


\subsection{Friday Review and Examples}

\begin{itemize}
\item Definition of Gauss Legendre quadrature.
\item Error bound.
\item Convergence.
\item General Gauss quadrature.
\end{itemize}

Key ideas:

\begin{itemize}
\item Error bound for Lagrange and Hermite interpolation.
\item Quadrature and integration as linear transformation.
\item Inner product and orthogonal polynomials.
\end{itemize}


\begin{exa}$I=\int_0^\pi{1\over \sqrt{\sin x}}dx$. The correct answer is 5.2441151\end{exa}

\begin{itemize}
\item By change of variable,
  \[I={\pi\over 2}\int_{-1}^1{1\over\sqrt{\cos (\pi x/2)}}dx\]
\item Let $w=(1-x^2)^{-1/2}$, $f={(1-x^2)^{1/2}\over\sqrt{\cos (\pi x/2)}}$. Then $\int_{-1}^1{1\over\sqrt{\cos (\pi x/2)}}dx=\int fwdx$.
\item Now do Chebyshev-Gauss formula for $f$. Suppose $n=2$, then the quadrature points, which are the roots of $T_3=\cos(3\cos^{-1}x)$, are $0, \pm{\sqrt{3}\over 2}$.
\item The quadrature weights are
  \[w_1=\int_{-1}^1{(x^2-3/4)/(-3/4)\over\sqrt{1-x^2}}dx=\pi/3\]
  \[w_0=w_2=\pi/3\]
  (One can check that for Chebyshev-Gauss, the weights are always $\pi\over n+1$.)
\item The Chebyshev-Gauss for $n=2$ is
  \[I_2=\sum_{i=0}^2w_if(x_i)=3.3384\]
\item The final answer is $5.2439$, difference is $0.00018$.
\end{itemize}


Exercises:

\begin{enumerate}
\item \begin{enumerate}
  \item Write down the Lagrange interpolation polynomial $p$ of a function $f$, with interpolation points $-1$, $0$ and $1$.
  \item Calculate $\int_0^1p(x)dx$, write it into the form of $w_0f(-1)+w_1f(0)+w_2f(1)$.
\item Show that the formula $I(f)=w_0f(-1)+w_1f(0)+w_2f(1)$ will give accurate answer if $f$ is a polynomial of degree $2$.
  \item Use the error estimate for Lagrange interpolation polynomial, find an upper bound for $\int_0^1f(x)dx-I(f)$.
  \end{enumerate}
  \item \begin{enumerate}
    \item Let $w=e^{-x}$, $L^2_w([0, \infty))$ be the space of functions $f$ such that $\int_0^\infty wf^2dx$ exists. Show that $1, x, x^2\in L^2_w([0, \infty))$.
     \item Use the inner product $(f, g)=\int_0^\infty wfgdx$, carry out Gram-Schmidt process for $\{1, x, x^2\}$ (the results are called {\bf Laguerre polynomials}).
\item Find the roots of the third polynomial you get from the above step, call them $x_0$ and $x_1$.
    \item Find $w_0$ and $w_1$ such that $\int_0^\infty wgdx=w_0g(x_0)+w_1g(x_1)$ for any polynomial $g$ of degree 1. This is called the Gauss-Laguerre quadrature.
\end{enumerate}

\end{enumerate}
Answer:

\begin{enumerate}
\item
 \begin{enumerate}
  \item $p(x)=f(-1)(x(x-1)/2)+f(0)(1-x^2)+f(1)(x(x+1)/2)$
  \item $-f(-1)/12+2f(0)/3+5w_2f(1)/12$.
\item Because in this case, due to uniqueness of Lagrange interpolation, $f=p$.
  \item The error bound for Lagrange interpolation polynomial is $|f(x)-p(x)|\leq\max|f'''||x^3-x|/3!$, so the error bound for $I$ is $\max|f'''|/24$ Here maximum is taken on $[-1, 1]$.
  \end{enumerate}
 
\item Note that $\int_0^\infty x^ne^{-x}dx=n!$
\begin{enumerate}
    \item Because the weighted $L^2$ norms of them are $1, 2, 24$ respectively.
     \item $\{1, x-1, x^2-4x+2\}$
\item $2\pm\sqrt{2}$.
\item \[{1+\sqrt{2}\over 2\sqrt{2}}g(2-\sqrt{2})+{\sqrt{2}-1\over 2\sqrt{2}}g(2+\sqrt{2})\]
\end{enumerate}
\end{enumerate}


\subsection{Composite Gauss quadrature (Section 10.5)}

One can combine composite method and Gauss quadrature: divide the interval into $m$ subintervals of equal length, then apply Gauss quadrature on each. For example, if we apply Gauss-Legendre quadrature with $k+1$ quadrature points to each subinterval, when the function is in $C^{2k+2}$, the error decay at a speed of $O(m^{-2k-2})$.


\begin{exa} $\int_{-0.5}^{0.5}\sqrt{1-x^2}dx$, using composite Gauss quadrature with $k=1$\end{exa}

Roots of degree $2$ Legendre polynomials are $\pm1/\sqrt{3}$.

\begin{lstlisting}[language=Python]
def composite_gauss(n, a, b, f):
    r=0
    m=int((n+1)/2)
    for i in range(m):
        l0=(i*b+(m-i)*a)/m
        l1=((i+1)*b+(m-i-1)*a)/m
        x0=(l0+l1)/2-(l1-l0)/2/(3**0.5)
        x1=(l0+l1)/2+(l1-l0)/2/(3**0.5)
        r+=f(x0)+f(x1)
    return r*(b-a)/(n+1)
\end{lstlisting}


\begin{tikzpicture}
	\begin{axis}[
		xlabel=$log(n)$,
		ylabel=$-log(Error)$]
	\addplot[color=red,mark=x, only marks] coordinates {
		(0.6931471805599453,3.746560449724477)
(1.0986122886681098,4.547453810338138)
(1.3862943611198906,5.118965503519308)
(1.6094379124341003,5.5633925579666705)
(1.791759469228055,5.927002052110047)
(1.9459101490553132,6.2346716549051004)
(2.0794415416798357,6.501320810552323)
(2.1972245773362196,6.73660161087667)
(2.302585092994046,6.947117728247236)
(2.3978952727983707,7.137586004803848)
(2.4849066497880004,7.311492816297971)
(2.5649493574615367,7.4714878376833544)
(2.6390573296152584,7.6196319546081686)
(2.70805020110221,7.757559684401888)
(2.772588722239781,7.886589202729626)
(2.833213344056216,8.00779902926391)
(2.8903717578961645,8.12208280373383)
(2.9444389791664403,8.23018925813059)
(2.995732273553991,8.332751940018328)
(3.044522437723423,8.43031168655338)
(3.091042453358316,8.52333387219665)
(3.1354942159291497,8.612221823753858)
(3.1780538303479458,8.697327381267385)
(3.2188758248682006,8.778959303716851)
(3.258096538021482,8.857390026592949)
(3.295836866004329,8.932861144496894)
(3.332204510175204,9.00558789696487)
(3.367295829986474,9.075762867305217)
(3.4011973816621555,9.143559054538375)
};

	\addplot[color=blue,mark=+, only marks] coordinates {
	(1.3862943611198906,9.114856038771517)
(1.791759469228055,10.641478993908981)
(2.0794415416798357,11.752422282602218)
(2.302585092994046,12.624884254357372)
(2.4849066497880004,13.342684719274926)
(2.6390573296152584,13.95214413140681)
(2.772588722239781,14.481537873892268)
(2.8903717578961645,14.949379792923526)
(2.995732273553991,15.368444387441553)
(3.091042453358316,15.7479127618939)
(3.1780538303479458,16.09460251736614)
(3.258096538021482,16.413713532022914)
(3.332204510175204,16.70930151946475)
(3.4011973816621555,16.98459025133957)
};
	\addplot[color=brown,mark=*, only marks] coordinates {
(1.0986122886681098,9.533573517620844)
(1.6094379124341003,11.053629765917707)
(1.9459101490553132,12.161876761968388)
(2.1972245773362196,13.032985223792194)
(2.3978952727983707,13.750015347951813)
(2.5649493574615367,14.358996558690196)
(2.70805020110221,14.888073888245788)
(2.833213344056216,15.3556959536854)
(2.9444389791664403,15.774601761766425)
(3.044522437723423,16.15395181051019)
(3.1354942159291497,16.50055106888232)
(3.2188758248682006,16.819591361689714)
(3.295836866004329,17.115123042359492)
(3.367295829986474,17.390366204550023)
	};
      \end{axis}
      
\end{tikzpicture}



\subsection{Other topics in numerical integration}

\subsubsection{Modified Gauss Quadratures (Section 10.6), won't be in final exam}

Sometimes some quadrature points are pre-determined while others can be chosen strategically, in which case we can choose them as roots of orthogonal polynomials. If the pre-determined point is one of the end point we call this strategy {\bf Radau quadrature}, if it is both end points we call it {\bf Lobatto quadrature}.\\

If there are $n+1$ quadrature points, $k$ of them are predetermined and the rest roots of Legendre polynomial of degree $n+1-k$, the error is bounded by $O((b-a)^{2n+3-k})$. When $n=k=2$ we get Simpson's method.\\


\subsubsection{Richardson Extrapolation (Section 7.6, 7.7), won't be in final exam}

A common trick in numerical analysis is {\bf extrapolation}: if a sequence $a_n$ can be calculated whose limit as $n\rightarrow \infty$ is some $a$, and the speed of convergence is known, we can use linear combination of successive terms to speed up the convergence. For example, for composite trapezium rule, Euler-Maclaurin formula says that, when $f$ is ``good enough'', 
\[I_n-I=\sum_{k=1}^\infty c_kn^{-2k}\]
So ${4I_{2n}-I_n\over 3}$ (which is identical to composite Simpson's rule) has $O(n^{-4})$ convergence, and one can do it repeatedly to get higher convergence speed.

\subsection{Review}

\begin{itemize}
\item Quadrature rule
\item Newton-Cotes quadrature
\item Gauss quadrature
\item Composite method
\end{itemize}


\section{Numerical ODE: IVP (Chapter 12)}

Initial value problem of ODE:
\[y'=f(t, y), y(0)=y_0\]
Here $y$ can be a real valued function or $\mathbb{R}^n$-valued function. For now we focus on the case where $y$ is real valued.\\

We always assume $f$ is continuous and Lipschitz on the second parameter ($|f(t, y)-f(t, z)|\leq L|y-z|$ for all pair $y, z$, $L$ is called the Lipschitz constant). So by Picard's Theorem, IVP has a unique solution.\\

Numerical integration can be seen as a special case of IVP of numerical ODE, with $f$ independent from $y$.\\

Strategy: Find some small positive number $h$ as the ``step size'', estimate the value of $y$ at $nh$ for all $n$.\\


\subsection{Euler's Method (12.2, 12.3)}

$h$ is a small number, $z$ an approximation of $y$ evaluated using:
\[z(0)=y_0\]
\[z((n+1)h)=z(nh)+hf(nh, z(nh))\]

\begin{itemize}
\item The motivation is using PL function to approximate $y$.
\item {\bf Truncated error} $T_n$ is error introduced at step $n$, divided by step size. More precisely, suppose the method is $z((n+1)h)=G(z(nh))$, then
  \[T_n={G(y(nh))-y((n+1)h)\over h}\]. 
\item A method has {\bf order of accuracy} $p$ if when $f$ is sufficiently smooth, $T=O(h^p)$.
\item {\bf Global error at time} $t$ is the difference between the estimated $z(t)$ and the true value of $y(t)$.
\item A method is called {\bf consistent} if truncated error goes to $0$ as $h\rightarrow 0$. {\bf Convergent} if global error goes to $0$ as $h\rightarrow 0$.
\item Under certain assumptions, global error at given time $t$ is controlled by the bound on truncated error (Theorem 12.2 and 12.5 in textbook).
\end{itemize}


\begin{exa}\label{Euler} Euler's method applied to $y'=y$, $y(0)=1$\end{exa}

True solution: $y(t)=e^t$.\\

Approximated solution using Euler's method with step-size $h$:
\[z(0)=1, z(h)=1+hz(0)=1+h, z(2h)=z(h)+hz(h)=1+h+h(1+h)=(1+h)^2, \dots, z(nh)=(1+h)^n\]

So the global error at time $t=nh$ is $e^t-(1+h)^n=e^t-(1+h)^{t/h}$, which converges to $0$ as $h\rightarrow 0$.\\

To understand the error created at each step, consider the sequence $z_k(nh)$, such that
\[z_k(kh)=y(kh)=e^{kh}\]
\[z_k((n+1)h)=z_k(nh)+hz_k(nh)\]
So
\[z_k(nh)=e^{kh}(1+h)^{n-k}\]
\[T_n={e^{(n+1)h}-e^{nh}(1+h)\over h}=O(h)\]
So the method is consistent with order of accuracy $1$.\\


\begin{tikzpicture}[yscale=2, xscale=8]
  \draw[->] (0, 0) -- (1, 0) node[right] {$x$};
  \draw[->] (0, 0) -- (0, 3) node[above] {$y$};
  \draw[scale=1, domain=0:1, smooth, variable=\x, black] plot ({\x}, {exp(\x)});
  \draw[-, blue] (0, 1)--(0.25, 1.25)--(0.5, 1.25*1.25)--(0.75, 1.25*1.25*1.25)--(1, 1.25*1.25*1.25*1.25);
  \draw[-, red] (0.25, 1.28402541669)--(0.5, 1.28402541669*1.25)--(0.75, 1.28402541669*1.25*1.25)--(1, 1.28402541669*1.25*1.25*1.25);
  \draw[-, red] (0.5, 1.6487212707)--(0.75, 1.6487212707*1.25)--(1, 1.6487212707*1.25*1.25);
  \draw[-, red] (0.75, 2.11700001661)--(1, 2.11700001661*1.25);
  \end{tikzpicture}

To study global error, we need to understand:

\begin{itemize}
\item What are the truncated errors?
\item How do error introduced in prior steps grow with time?
\end{itemize}


\begin{itemize}
\item By definition of derivative, if $t=nh$,
\[y(t+h)=y(t)+hf(t, y(t))+o(h)\]
So $T_n={o(h)\over h}$, Euler's method is {\bf consistent}.
\item If $f$ is sufficiently smooth, so is $y$, so
\[y(t+h)=y(t)+hf(t, y(t))+{h^2\over 2}y''(t)+o(h^2)\]
So $T_n=O(h)$, Euler's method is of order of accuracy $1$.
\item Furthermore, if we write down the remainder of Taylor's series, we have
  \[y(t+h)=y(t)+hy'(t)+{h^2\over 2}y''(s)\]
  \[=y(t)+hf(t, y(t))+{h^2\over 2}(f_1(s, y(s))+f_2(s, y(s))f(s, y(s)))\]
for some $s\in [t, t+h]$, $f_1$ and $f_2$ are the partial derivative in first and second parameter. So,  if $f$ and its partial derivatives are all bounded, $T$ is uniformly bounded by some $Ch$.
\end{itemize}


Let's now analyze how error grows in Euler's method:

\begin{lem}
  If $f$ is $L$-Lip. on second paramater, $w_1$, $w_2$ two functions defined on $mh, (m+1)h, \dots, nh$, such that
  \[w_i((k+1)h)=w_i(kh)+hf(kh, w_i(kh))\]
  Then 
\[|w_1(nh)-w_2(nh)|\leq e^{L(n-m)h}|w_1(mh)-w_2(mh)|\]
\end{lem}

\begin{proof}
  \[LHS\leq (1+hL)|w_1((n-1)h)-w_2((n-1)h)|\leq\]
    \[\dots\leq (1+hL)^{n-m}|w_1(mh)-w_2(mh)|\]
 And \[(1+hL)^{n-m}\leq e^{L(n-m)h}\]
\end{proof}


Now let's estimate global error at time $t=nh$. Let $z_k(nh)$, $n\geq k$, be
\[z_k(kh)=y(kh)\]
\[z_k((n+1)h)=z_k(nh)+hf(nh, z_k(nh))\]
\[|y(nh)-z(nh)|=|z_{n}(nh)-z_{0}(nh)|\leq \sum_{k=0}^{n-1}|z_k(nh)-z_{k+1}(nh)|\]
(Triangle inequality)
\[\leq \sum_{k=0}^{n-1}e^{Lh(n-k-1)}|z_k((k+1)h)-z_{k+1}((k+1)h)|\]
(The Lemma from previous page)
\[\leq \sum_{k=0}^{n-1}Ch^2 e^{L(n-k-1)h}\leq {Ch^2(e^{Lnh}-1)\over e^{Lh}-1}\]
(Bound on truncated error)\\
So
\begin{thm}
If
\begin{itemize}
\item $f$ is smooth, $f$ and its partial derivatives are bounded.
\item $nh$ is fixed and $n\rightarrow\infty$
\end{itemize}
then the global error of Euler's method at time $nh$ converges to zero at $O(h)$. In other words, Euler's method is {\bf convergent}.
\end{thm}


\subsection{Friday Review and Examples}

\begin{itemize}
\item Composite Gauss Quadrature
\item Euler's method.
\item Truncated error and global error.
\item Consistency and order of accuracy.
\item Convergence.
\end{itemize}



\begin{exa}Midpoint rule\end{exa}
\begin{enumerate}
\item The weight $1$ orthogonal polynomial on $[a, b]$ of degree $1$. It is $x-{b+a\over 2}$. The root of it is ${b+a\over 2}$.
\item Hence, the Gauss-Legendre quadrature with $n=0$ is $x_0={b+a\over 2}$. Quadrature weight is ${b-a}$.
\item If $f$ is differentiable, the error bound is
  \[|\int_a^bfdx-{(b-a)}f({a+b\over 2})|\leq {\max|f''|\int_a^b(x-{a+b\over 2})^2dx\over 2!}={\max|f''|(b-a)^3\over 24}\]
\item Now decompose $f$ into $m$ subintervals, each applying the Gauss-Legendre quadrature, we get
  \[I={b-a\over m}\sum_{i=0}^{m-1} f(a+{(i+1/2)(b-a)\over m})\]
  And the error bound is $\max|f''|(b-a)^3\over 24m^2$.
\end{enumerate}


\begin{exa} $y'=\sin(y)$, $y(0)=1$\end{exa}
\begin{enumerate}
\item True solution: $y(t)=2\tan^{-1}(\tan(1/2)e^x)$\\
\item Euler's method is:
\[z(0)=1, z(nh)=z((n-1)h)+h\sin(z((n-1)h))\]
\item Truncated error for Euler's method at time $t$ is
  \[|{y''(s)h\over 2!}|=|{y'\cos(y)h\over 2!}|\]
  \[=|{\sin(y)\cos(y)h\over 2!}|\leq Ch\]
  What is the number $C$?
\item $\sin(y)$ is $L$-Lipschitz. In other words, $|\sin(a)-\sin(b)|\leq L|a-b|$ for all $a, b$. What's a valid $L$?
\item Now the argument from last lecture shows that the global error bound at time $t=nh$ is ${Ch^2(e^{Lt}-1)\over e^{Lh}-1}$. 
\item Let $t=1$, $h=0.1$, Euler's method get $z(1)=1.95109$. True answer is $y(1)=1.95629$. Error bound as calculated from above, using $C=1/4, L=1$, is $0.04084$.
\end{enumerate}



\begin{exa}Consider IVP $y'=f(y)$, $y(0)=y_0$. Suppose $f$ is {\bf real analytic}, i.e. the Taylor series at every point converges at a neighborhood of the point. Suppose $h$ is a small positive number. The theory of ODE tells us that $y$ is also real analytic.\end{exa}

Suppose $f(x)=\sum_ia_i(x-y_0)^i$.

\begin{enumerate}
\item Write down the Taylor expansion of $y$ at $t=0$, up to $t^2$ term.
\item Write down the result of $z(h)=z(0)+hf(z(0))$, as a power series of $h$.
\item Write down the result of $z(2h)=z(h)+hf(z(h))$, as a power series of $h$.
\item Find a linear combination of $y_0$, $z(h)$, $z(2h)$ which is close to $y(2h)$ up to the $h^2$ term. This is called the 2nd order Runge-Kutta method (rk2).
\end{enumerate}

Answer:

\begin{enumerate}
\item $y(t)=y_0+a_0t+{a_0a_1\over 2}t^2+O(t^3)$
\item $z(h)=y_0+a_0h$
\item $z(2h)=y_0+2a_0h+a_0a_1h^2+O(h^3)$
\item $y(2h)=y_0-2z(h)+2z(2h)+O(h^3)$
\end{enumerate}



\subsection{Ways to get higher order methods}

\subsubsection{Method based on Lagrange Interpolation (12.4, 12.6)}

Firstly some methods based on Lagrange interpolation:

\paragraph{Explicit Methods} Consider the function $g(t)=f(t, y(t))$. One can then use $g(t-h), \dots, g(t-kh)$ as quadrature point to estimate $\int_{t-dh}^{t}g(s)ds$, then use $y(t)=y(t-dh)+\int_{t-dh}^{t}g(s)ds$.

\paragraph{Implicit Methods} One can also use $g(t), g(t-h), \dots, g(t-kh)$ as quadrature point to estimate $\int_{t-dh}^{t}g(s)ds$, then use $y(t)=y(t-dh)+\int_{t-dh}^{t}g(s)ds$. This way $y(t)$ appears on the right hand side and the estimate of $y(t)$ requires solving an equation, hence is called {\bf implicit}.


\begin{exa}\begin{itemize}
  \item Explicit method for $d=1$, $k=1$ is Euler's Method.
  \item Implicit method for $d=1$, $k=1$ is the {\bf trapezium method} as the numerical integration is via trapezium rule:
\[z(t)=z(t-h)+{h\over 2}(f(t, z(t))+f(t-h, z(t-h)))\] 
We can show that it is consistent with order of accuracy $2$.
\item Implicit method for $k=d=2$ will be
  \[z(t)=z(t-2h)+{h\over 3}(f(t, z(t))+4f(t-h, z(t-h))+f(t-2h, z(t-2h)))\]
  When $f(t, y)=f(t)$ this reduces to Simpson's rule. This is called the $2$-step {\bf Milne's method}.
\item When $d=1$, the explicit methods are called $k$-th step {\bf Adams-Bashforth} methods, while the implicit methods are called {\bf Adams-Moulton} methods. 
\end{itemize}
\end{exa}


\begin{exa}Let's deduce the Adams-Bashforth method for $k=4$.\end{exa}

\begin{enumerate}
\item The Lagrange interpolation of $f(s, z(s))$ at $t-h$, $t-2h$, $t-3h$, $t-4h$ is
  \[p(s)=f(t-h, z(t-h)){(s-(t-2h))(s-(t-3h))(s-(t-4h))\over 6h^3}\]
  \[+f(t-2h, z(t-2h)){(s-(t-h))(s-(t-3h))(s-(t-4h))\over -2h^3}\]
  \[+f(t-3h, z(t-3h)){(s-(t-h))(s-(t-2h))(s-(t-4h))\over 2h^2}\]
  \[+f(t-4h, z(t-4h)){(s-(t-h))(s-(t-2h))(s-(t-3h))\over -6h^3}\]
\item
  \[z(t)=z(t-h)+\int_{t-h}^tp(s)ds\]
  \[=z(t-h)+h({55\over 24}f(t-h, z(t-h))-{59\over 24}f(t-2h, z(t-2h))\]
  \[+{37\over 24}f(t-3h, z(t-3h))-{3\over 8}f(t-4h, z(t-4h)))\]
\end{enumerate}



An alternative to implicit method is the {\bf predictor-corrector method}: for example, instead of Trapzsium method, we first estimate $z(t)$ using Euler's method, then ``correct'' it using the trapezium rule formula, and get:
\[z_{p}(t)=z(t-h)+hf(t-h, z(t-h))\]
\[z(t)=z(t-h)+{h\over 2}(f(t, z_p(t))+f(t-h, z(t-h)))\] 
This is called {\bf Heun's method}.


\begin{exa}
  $y'=y$, $y(0)=1$
  \begin{itemize}
  \item Euler's method:
    \[z(nh+h)=z(nh)+hz(nh)\]
    \[z(nh)=(1+h)^n\]
  \item Trapezium rule method:
    \[z(nh+h)=z(nh)+{h\over 2}(z(nh+h)+z(nh))\]
    \[z(nh)={(1+h/2)^n\over (1-h/2)^n}\]
    Truncated error is
    \[O({e^h-{1+h/2\over 1-h/2}\over h})=O(h^2)\]
  \item Heun's rule is
    \[z(nh+h)=z(nh)+{h\over 2}(2z(nh)+hz(nh))\]
    \[z(nh)=(1+h+h^2/2)^n\]
    Truncated error is also $O(h^2)$.
  \end{itemize}
\end{exa}



\subsubsection{Theory of General Linear Multistep Methods (12.7-12.9)}

General Linear $k$-step Method:

\[\sum_{j=0}^k\alpha_jz((n+j)h)=h\sum_{j=0}^k\beta_jf((n+j)h, z((n+j)h))\]

If $\beta_k=0$ it is explicit, otherwise it is implicit. To start the $k$-step method, we need $k$ initial values $z(0), \dots, z((k-1)h)$, then solve the equation to get $z(kh), z((k+1)h, \dots$

\begin{itemize}  
\item We want a linear $k$-step method to be {\bf zero-stable}. In other words, if the equation is $y'=0$, then the $z(nh)$ does not go to infinity as $n\rightarrow\infty$. From the theory in linear difference equations in linear algebra, we know that this is equivalent to the {\bf first characteristic polynomial} $\rho(z)=\sum_{j=0}^k\alpha_jz^j$ having all roots inside the closed unit disc and at most only single roots on the unit circle.
\item All the method obtained via Lagrange interpolation, including Adams-Moulton and Adams-Bashforth, are zero-stable.


\item Why do we need zero-stability? Suppose we let $h\rightarrow 0$ and $t=hn$ remain unchanged, then $n\rightarrow\infty$. If there is no zero stability, error at previous steps will grow indefinitely.
\end{itemize}

\begin{exa}$y'=y$, $y(0)=1$. Use multistep method $z((n+2)h)-3z((n+1)h)+2z(nh)=-hf(nh, z(nh))$\end{exa}
\begin{enumerate}
\item Start with $z(0)=1$, $z(h)=e^h$.
\item One can verify by doing Taylor expansion of $z$, that the method is consistent.
\item Similar to Example~\ref{Euler}, define $z_k(nh)$ be $z_k(kh)=y(kh)$, $z_k((k+1)h)=y((k+1)h)$, and for all $n>k+1$, $z_k(nh)$ are calculated using the multistep method.
\item Then the error created at time $(k+2)h$ is
  \[z_k((k+2)h)-y((k+2)h)=z_k((k+2)h)-z_{k+1}((k+2)h)=O(h^2)\]

\item Now let's see how this error grow with time: Let $w_m=z_k((k+1+m)h)-z_{k+1}((k+1+m)h)$. Then
  \[w_0=0\]
  \[w_1=O(h^2)\]
  \[w_{m+2}-3w_{m+1}+2w_m=O(h)\]
\item So if the linear difference equation
  \[w_{m+2}-3w_{m+1}+2w_m=0\]
  Has solution that grows to infinity, then the error created at time $(k+2)h$, after propagating to $t=nh$, will be $w_{n-k-1}$ which goes to $\infty$ as $h\rightarrow 0$.
\end{enumerate}

This is when $t=1$, $h=0.1$ (See \url{https://github.com/wuchenxi/Math-514/blob/main/zero_stability.py}):\\

\begin{tikzpicture}[yscale=1, xscale=8]

  \draw[->] (0, 0) -- (1, 0) node[right] {$x$};
  \draw[->] (0, -1) -- (0, 3) node[above] {$y$};
\draw[scale=1, domain=0:1, smooth, variable=\x, blue] plot ({\x}, {exp(\x)});
\draw[-](0.1,1.1051709180756477)--(0.2,1.215512754226943)--(0.3,1.3256793347219689)--(0.4,1.424461220289326)--(0.5,1.4894570579518434)--(0.6,1.4770026112479455)--(0.7,1.3031480120449657)--(0.8,0.8077385525142113)--(0.9,-0.3133951677517939)--(1,-2.6364364635352255);
\draw[-](0.2,1.2214027581601699)--(0.3,1.3433493465216495)--(0.4,1.4651022474285917)--(0.5,1.5742731145903117)--(0.6,1.646104624170892)--(0.7,1.6323403318730216)--(0.8,1.4402012848601915)--(0.9,0.8926891576472291)--(1,-0.34635522526471507);
\draw[-](0.3,1.3498588075760032)--(0.4,1.4846306305916523)--(0.5,1.61918839586535)--(0.6,1.73984086335358)--(0.7,1.8192269587435055)--(0.8,1.8040150631879985)--(0.9,1.5916685762026341)--(1,0.9865740959131053);
\draw[-](0.4,1.4918246976412703)--(0.5,1.6407705970142044)--(0.6,1.7894799259959455)--(0.7,1.9228215242580071)--(0.8,2.0105567281825363)--(0.9,1.9937449836057943)--(1,1.7590658216340564);
\draw[-](0.5,1.6487212707001282)--(0.6,1.8133319470537168)--(0.7,1.977681172690881)--(0.8,2.1250464292598377)--(0.9,2.222008825128663)--(1,2.20342897394033);
\draw[-](0.6,1.822118800390509)--(0.7,2.0040417327012583)--(0.8,2.185675717283705)--(0.9,2.3485395131784728)--(1,2.4556995332396374);
\draw[-](0.7,2.0137527074704766)--(0.8,2.21480864159136)--(0.9,2.4155452390860797)--(1,2.5955375699163827);
\draw[-](0.8,2.225540928492468)--(0.9,2.4477420997894033)--(1,2.6695903495340274);
\draw[-](0.9,2.45960311115695)--(1,2.705173383636667);
\end{tikzpicture}



\begin{itemize}
\item To get consistency, let $\sigma(z)=\sum_{j=0}^k\beta_jz^j$ be the {\bf second characteristic polynomial}. When $f$ is smooth, $s<<1$, $f(nh+s, y(nh+s))=f(nh, y(nh))+O(s)$, $y(nh+s)=y(nh)+sf(nh, y(nh))+O(s^2)$. To make method consistent, we want to make sure that if we put $y((n+j)h)$ in place of $z((n+j)h)$, the left hand side and right hand side are off by $o(h)$, hence
  \[\sum_i\alpha_i+\sum_ii\alpha_if(nh, y(nh))h=h\sum_i\beta_if(nh, y(nh))+O(h^2)\]
  So $\rho(1)=\sum_i\alpha_i=0$, $\rho'(1)=\sum_ii\alpha_i=\sum_i\beta_i=\sigma(1)$.
\item All the methods obtained via Lagrange interpolation are consistent, by looking at $f=1$.
\item To get order of accuracy, carry out the same argument as above but do higher order power expansion for $y(nh+s)$ and $f(nh+s, y(nh+s))$.
\end{itemize}


For global convergence of linear multistep methods we have the Dahlquist's theorems:
\begin{itemize}
\item If a linear $k$-step method has zero stability, then it is consistent iff it is convergent, and the truncated error and global error has the same order as $h\rightarrow 0$.
\item (First Dahlquist barrier) If a linear $k$-step method is 0-stable then the order of accuracy is no more than $k+1$ if $k$ is odd (e.g. Adams-Moulton, by Theorem~\ref{bound1}), $k+2$ if $k$ is even (e.g. $d=k$ implicit, by Theorem~\ref{qbound}), and $k$ if it has to be explicit (e.g. Adams-Bashforth, by Theorem~\ref{bound1}).
\end{itemize}


\subsubsection{Runge-Kutta methods (12.5)}

To get initial conditions for linear multistep methods, we need one step methods with higher orders of accuracy. To accomplish that we need to have more evaluations of $f$ per step.\\

Recall Heun's method:
\[z_p((n+1)h)=z(nh)+hf(nh, z(nh))\]
\[z((n+1)h)=z(nh)+{h\over 2}(f(nh, z(nh))+(f((n+1)h, z_p((n+1)h))))\]
We can rewrite it as
\[k_1=f(nh, z(nh))\]
\[k_2=f(nh+h, z(nh)+hk_1)\]
\[z((n+1)h)=z(nh)+h({k_1\over 2}+{k_2\over 2})\]

General Runge-Kutta:
\[k_1=f(nh, z(nh))\]
\[k_j=f(nh+\alpha_jh, z(nh)+\sum_{i<j}\beta_{ij}hk_i)\]
\[z((n+1)h)=z(nh)+h\sum_jc_jk_j\]
Some popular choice of parameters:
\begin{enumerate}
\item Heun's method, or improved Euler method: $\alpha_2=\beta_{12}=1$, $c_1=c_2=1/2$
\item RK2, or modified Euler's method: $\alpha_2=\beta_{12}=1/2$, $c_1=0$, $c_2=1$.
\item RK4: $\alpha_2=\alpha_3=\beta_{12}=\beta_{23}=1/2$, $\alpha_4=\beta_{34}=1$, $c_1=c_4=1/6$, $c_2=c_3=1/3$.   
\end{enumerate}


How to check consistency and order of accuracy: Taylor series expansion for $y$ and $f$.
\begin{exa}Show that Heun's method is consistent and has order of accuracy $2$.\end{exa}
\begin{enumerate}
\item
  \[f(t+s, y(t)+r)=f(t, y(t))+sf_1(t, y(t))+rf_2(t, y(t))+O(r^2+s^2)\]
\item Now calculate the Taylor series of $y(t+s)$:
  \[y(t+s)=y(t)+y'(t)s+{y''(t)\over 2}s^2+O(s^3)\]
  \[=y(t)+f(t, y(t))s+{f_1+ff_2\over 2}s^2+O(s^3)\]
\item Now do Heun's method:
  \[k_1=f(t, y(t))\]
  \[k_2=f(t+h, y(t)+hk_1)=f+h(f_1+ff_2)+O(h^2)\]
  So
  \[z(t+h)=y(t)+{h\over 2}(k_1+k_2)=y(t+h)+O(h^3)\]
\item The method is consistent and has order of accuracy $2$.
\end{enumerate}

Similarly, rk2 can be shown to have order of accuracy $2$ and rk4 order of accuracy $4$.\\

Global error bound can then be obtained in a way similar to Euler's method.\\

When using linear multistep methods of order of accuracy $n$, we can calculate the initial values using one-step methods of order $n-1$. This way the error created in the initial steps will be $O(h^n)$ which is comparable with global error of an $n$-th order method.\\


\begin{exa}$y'=\sin(y)$, $y(0)=1$. $h=0.25$, $t=4h=1$.\end{exa}
True answer is $y(1)=1.9562950$.
\begin{enumerate}
\item Euler's Method:
  \[z(0.25)=y(0)+h\sin(y(0))=1.2103677\]
  \[z(0.5)=y(0.25)+h\sin(y(0.25))=1.4443042\]
  \[z(0.75)=1.6923068, z(1)=1.9404635\]
\item Rouge-Kutta 2nd Order
  \[z(0.25)=y(0)+h\sin(y(0)+h\sin(y(0))/2)=1.2233867\]
  \[z(0.5)=1.4668103, z(0.75)=1.7167586, z(1)=1.9577257\]

\item Improved Euler's Method+Adams-Bashforth 3rd order
  \begin{enumerate}
  \item Improved Euler's Method for $z(0.25)$ and $z(0.5)$:
    \[z(0.25)=y(0)+{h\over 2}(\sin(y(0))+\sin(y(0)+h\sin(y_0)))=1.2221521\]
    \[z(0.5)=1.4638248\]
  \item Adams-Bashforth 3rd order:
    \[z(0.75)=z(0.5)+h({23\over 12}\sin(z(0.5))-{4\over 3}\sin(z(0.25))+{5\over 12}\sin(y(0)))=1.7146269\]
    \[z(1)=1.9553174\]
  \end{enumerate}
  \item Rouge-Kutta 4th Order:
    \begin{enumerate}
    \item Calculate $z(0.25)$:
      \[k_1=\sin(y(0))=0.8414710, k_2=\sin(y(0)+{h\over 2}k_1)=0.8935468\]
      \[k_3=\sin(y(0)+{h\over 2}k_2)=0.8964504, k_4=\sin(y(0)+hk_3)=0.9405047\]
      \[z(0.25)=y(0)+{h\over 6}(k_1+2k_2+2k_3+k_4)=1.2234154\]
    \item Continue with the calculation, we get:
      \[z(0.5)=1.4663981, z(0.75)=1.7156965, z(1)=1.9562859\]
     \end{enumerate}
\end{enumerate}


Let $h=1/n$, $n=4, 5, \dots, 20$, the behavior of the global error at $t=1$ is (see \url{https://github.com/wuchenxi/Math-514/blob/main/ivp.py}):\\


\begin{tikzpicture}
	\begin{axis}[
		xlabel=$log(n)$,
		ylabel=$-log(Error)$]
	\addplot[color=red,mark=x, only marks] coordinates {
		(1.3862943611198906,4.145754760029328)
(1.6094379124341003,4.430625024433864)
(1.791759469228055,4.6560886339731)
(1.9459101490553132,4.842100702500449)
(2.0794415416798357,5.0001313065355)
(2.1972245773362196,5.1373420061095265)
(2.302585092994046,5.258487509184754)
(2.3978952727983707,5.366877743429926)
(2.4849066497880004,5.464905296079924)
(2.5649493574615367,5.554353469858444)
(2.6390573296152584,5.636585656204349)
(2.70805020110221,5.712666777048321)
(2.772588722239781,5.783443979079779)
(2.833213344056216,5.849601900280024)
(2.8903717578961645,5.911701522460055)
(2.9444389791664403,5.9702081132143645)
(2.995732273553991,6.025511727225925)
};

\addplot[color=orange,mark=+, only marks] coordinates {
(1.3862943611198906,6.549539681499379)
(1.6094379124341003,7.005961570384487)
(1.791759469228055,7.375672389599312)
(1.9459101490553132,7.686746757690344)
(2.0794415416798357,7.955419332333807)
(2.1972245773362196,8.19195614767128)
(2.302585092994046,8.403275255915098)
(2.3978952727983707,8.594266276895366)
(2.4849066497880004,8.768515980147896)
(2.5649493574615367,8.928735053587898)
(2.6390573296152584,9.077022853878512)
(2.70805020110221,9.215038910361171)
(2.772588722239781,9.344118084267752)
(2.833213344056216,9.46535027126746)
(2.8903717578961645,9.579637011866296)
(2.9444389791664403,9.687732612310596)
(2.995732273553991,9.790274605691003)
};
	\addplot[color=blue,mark=*, only marks] coordinates {
(1.3862943611198906,5.29904997141513)
(1.6094379124341003,5.734040093549738)
(1.791759469228055,6.091797514682703)
(1.9459101490553132,6.3954812957886285)
(2.0794415416798357,6.659239792615228)
(2.1972245773362196,6.892327667161501)
(2.302585092994046,7.101122869337775)
(2.3978952727983707,7.290204487932382)
(2.4849066497880004,7.462970039881465)
(2.5649493574615367,7.622009297574981)
(2.6390573296152584,7.769341287083899)
(2.70805020110221,7.906570387820401)
(2.772588722239781,8.034992522579467)
(2.833213344056216,8.155669421575428)
(2.8903717578961645,8.26948181802594)
(2.9444389791664403,8.377168358730655)
(2.995732273553991,8.479354596145765)
	};

	\addplot[color=green,mark=*, only marks] coordinates {
(1.3862943611198906,6.930443449440396)
(1.6094379124341003,7.606687874319425)
(1.791759469228055,8.150587158527076)
(1.9459101490553132,8.60285551756825)
(2.0794415416798357,8.99084336938202)
(2.1972245773362196,9.331460239210784)
(2.302585092994046,9.635596734388546)
(2.3978952727983707,9.91067061226246)
(2.4849066497880004,10.161970818744233)
(2.5649493574615367,10.393414824137718)
(2.6390573296152584,10.607995522528059)
(2.70805020110221,10.80805787148315)
(2.772588722239781,10.995477728611041)
(2.833213344056216,11.171781920273448)
(2.8903717578961645,11.33823152549481)
(2.9444389791664403,11.495881284685566)
(2.995732273553991,11.645623011636145)
	};
	\addplot[color=purple,mark=*, only marks] coordinates {
(1.3862943611198906,11.613796344378912)
(1.6094379124341003,12.504194959159026)
(1.791759469228055,13.23238188403467)
(1.9459101490553132,13.848379911579531)
(2.0794415416798357,14.382153985271868)
(2.1972245773362196,14.853074668486146)
(2.302585092994046,15.274387300953613)
(2.3978952727983707,15.655548787330039)
(2.4849066497880004,16.00354681050488)
(2.5649493574615367,16.323690770126053)
(2.6390573296152584,16.620109414260515)
(2.70805020110221,16.896076777445682)
(2.772588722239781,17.154232901786415)
(2.833213344056216,17.396737589478487)
(2.8903717578961645,17.62538021102702)
(2.9444389791664403,17.841659915464817)
(2.995732273553991,18.046845108913505)
	};
      \end{axis}

      
\end{tikzpicture}

The dots from low to high are euler, heun, rk2, ab3, rk4.




\subsection{Stiffness and Absolute Stability}

All the methods we discussed so far can be easily generalized to systems of equations. Just see $y$ as a vector-valued function.

\begin{exa}
  \[y'=\left(\begin{array}{cc} -1 & 0 \\ 1 & -100\end{array}\right)y\]
  \[y(0)=\left(\begin{array}{c}1\\1\end{array}\right)\]
\end{exa}           

Do time step $h=0.1$, $t=10h=1$, using Euler's, improved Euler's and trapezium rule methods:

\[y(1)=\left(\begin{array}{c}1/e\\e^{-100}(e^{99}/99+98/99)\end{array}\right)\]

Let $A=\left(\begin{array}{cc} -1 & 0 \\ 1 & -100\end{array}\right)$.\\

\begin{itemize}
\item Euler's method: 
\[z(1)=(I_2+0.1A)^{10}\left(\begin{array}{c}1\\1\end{array}\right)\]
\[=\left(\begin{array}{c}0.3486784401\\ 3451564356.5489765499 \end{array}\right)\]


\item Heun's method:
\[z(1)=(I_2+0.1A+0.005A^2)^{10}\left(\begin{array}{c}1\\1\end{array}\right)\]
\[=\left(\begin{array}{c}0.368540984834\\ 1.32870768929\times 10^{16} \end{array}\right)\]

\item Trapezium rule method:
\[z(1)=((I_2+0.05A)/(I_2-0.05A))^{10}\left(\begin{array}{c}1\\1\end{array}\right)\]
\[=\left(\begin{array}{c}0.367572542383\\ 0.02087921691 \end{array}\right)\]

\end{itemize}




\begin{itemize}
\item A system of equations is called stiff, if, after linearization into $y'=Ay$, $A$ has eigenvalues with negative real parts, and the ratio between real parts of eigenvalues can be large.
\item Stiffness means there are behavior in different time scale. A numerical method need to take small step size to accommodate for the faster behavior, but also need to calculate till a large $t$ to see the slow behavior, resulting in huge amount of computation.
\item There are other cases of stiffness which are beyond the scope of this course.
\end{itemize}


To deal with stiff equations efficiently, we need numerical methods which does not require a small time scale to get good answers. Usually we use test equation $y'=\lambda y$, $y(0)=1$, where $Re(\lambda)<0$, and the set of values $h\lambda$ that makes $\lim_{n\rightarrow \infty}z(nh)=0$, are called {\bf the region of absolute stability}.

\begin{exa}
  \begin{itemize}
\item For Euler's method,
  \[z((n+1)h)=z(nh)+h\lambda(z(nh))=(1+h\lambda)z(nh)\]
  So the solution goes to $0$ if $|1+h\lambda|<1$, the region of absolute stability is the circle of radius $1$ centered at $-1$.
\item For Trapezium Rule Method,
  \[z((n+1)h)=z(nh)+{h\over 2}(\lambda(z(nh))+\lambda(z((n+1)h))\]
 So
  \[z((n+1)h)={1+h\lambda/2\over 1-h\lambda/2}z(nh)\]
  The region of absolute stability is the left half plane.
  \end{itemize}
\end{exa}

A method whose region of absolute stability is the whole left half plane is called {\bf A-stable}.



\subsection{Review}

\[y'=f(t, y), y(0)=y_0\]

Key idea: estimate $y(h), y(2h), y(3h), \dots$ successively.\\

Methods:
\begin{enumerate}
\item Euler's method
\item First Generalization of Euler's Method: Method based on quadrature rule (Adams-Bashforth, Adams-Moulton), general Linear Multistep methods 
\item Second Generalization of Euler's Method: Rouge-Kutta family
\end{enumerate}


Concepts:
\begin{enumerate}
\item Local
  \begin{enumerate}
    \item Truncated error
    \item Consistency
    \item Order of accuracy
  \end{enumerate}
\item Global
  \begin{enumerate}
   \item Zero stability
   \item Convergence
  \end{enumerate}
\item Efficiency issue
  \begin{enumerate}
   \item Region of absolute stability.
   \item A-stability.
  \end{enumerate}
\end{enumerate}


How to analyze numerical methods:

\begin{enumerate}
\item Locally: Taylor series expansion.
\item Globally: separate the error created at each step, as in the argument for Euler's method. We can summarize it as below:
  \begin{thm}\label{global}(Theorem 12.2 in textbook)
    If $z((n+1)h)=z(nh)+h\Phi(nh, z(nh))$, $z(0)=y(0)$, $|y((n+1)h)-h\Phi(y(nh))-y(nh)|\leq Th$, and $\Phi$ is $L$-Lispchitz with respect to the second parameter. Then
    \[|z(nh)-y(nh)|\leq T\cdot {e^{nhL}-1\over L}\]
 \end{thm}
 \begin{proof}
   Let $z_k(kh)=y(kh)$, $z_k((n+1)h)=z_k(nh)+h\Phi(nh, z_k(nh))$, then
   \[|z_k(nh)-z_{k+1}(nh)|\leq (1+hL)^{n-k-1}|z_k((k+1)h)-z_{k+1}((k+1)h)|\leq Th(1+hL)^{n-k-1}\]
   \[|z(nh)-y(nh)|=|z_0(nh)-z_n(nh)|\leq T\cdot {e^{nhL}-1\over L}\]
 \end{proof}
\end{enumerate}


\begin{exa}Consistency, order of accuracy, and convergence of rk2.\end{exa}
\[y'=f(t, y), y(0)=y_0\]
\[z(t+h)=z(t)+hf(t+{h\over 2}, z(t)+{h\over 2}f(t, z(t)))\]
\begin{enumerate}
\item Consistency and order of accuracy:
  Suppose $z(t)=y(t)$, then
  \[y(t+h)=y(t)+hy'(t)+{h^2\over 2}y''(t)+{h^3\over 6}y'''(t)+\dots\]
  \[=y(t)+f(t, y(t))h+(f_1(t, y(t))+f(t, y(t))f_2(t, y_t)){h^2\over 2}+(f_{11}+ff_{12}+(f_1+ff_2)f_2+f(f_{12}+ff_{22})){h^3\over 6}+\dots\]
  \[z(t+h)=y(t)+hf(t+{h\over 2}, y(t)+{h\over 2}f(t, y(t)))\]
  \[=y(t)+f(t, y(t))h+(f_1+ff_2){h^2\over 2}+(f_{11}+2f_{12}f+f_{22}f^2){h^3\over 8}+\dots\]
  So
  \[|y(t+h)-z(t+h)|=O(h^3)\]
  It is consistent with order of accuracy $2$.
\item If $f$, $f_1$, $f_2$, $f_{11}$, $f_{12}$, $f_{22}$ are all bounded, then there is uniform constant $K$ such that
  \[|y(t+h)-z(t+h)|\leq Kh^3\]
\item $\Phi(t, y)=f(t+{h\over 2}, z(t)+{h\over 2}f(t, z(t)))$, hence it is $L+hL^2/2$-Lipschitz.
\item Hence in this case, by Theorem~\ref{global}, 
  \[|y(t)-z(t)|\leq Kh^2{e^{(L+hL^2/2)t}-1\over L+hL^2/2}=O(h^2)\]
\end{enumerate}


Exercise: Consider $y'=y\cos(t)$, $y(0)=1$.
\begin{enumerate}
\item Find $A$, $B$ such that the linear multistep method
  \[z(t+2h)=z(t)+Ahf(t, z(t))+Bhf(t+h, z(t+h))\]
  is consistent. (Answer: $A+B=2$)
\item Find $A$, $B$ such that the linear multistep method has order of accuracy $2$. (Answer:
  \[y(t+2h)=y(t)+f\cdot 2h+(f_1+ff_2)(2h)^2/2+\dots\]
\[z(t+2h)=y(t)+Ahf+Bh(f+h(f_1+ff_2))+\dots\]
  Hence$A=0$, $B=2$)
  

\item Estimate $y(h)$ using Euler's method, then $y(2h)$ using this linear multistep method. (Answer: $z(h)=1+h$, $z(2h)=1+Ah+Bh(1+h)\cos(h)$.)
\item Find the region of absolute stability of this linear multistep method, using the fact (from linear algebra) that iterative relation
  \[a_{n+2}=ca_{n+1}+da_n\]
  has all solutions converging to $0$ iff all roots of $z^2-(cz+d)$ has absolute value less than $1$. (Answer: apply the algorithm to $y'=\lambda y$, we get
  \[z((n+2)h)=z(nh)+Ah\lambda z(nh)+Bh\lambda z((n+1)h)\]
  so $h\lambda$ is in region of absolute stability iff $z^2-Bh\lambda z-(1+Ah\lambda)=0$ has all roots inside the open unit circle, i.e.
  \[|{Bh\lambda\pm\sqrt{B^2h^2\lambda^2+4+4Ah\lambda}\over 2}|<1\]
\end{enumerate}


\section{Boundary Value Problems}

{\bf This section will not be in final exam.}

Example: $y''=-y$, $y(0)=0$, $y(1)=1$. True answer is $y(x)={\sin(x)\over\sin(1)}$.

We discretize the problem by estimating the solution on a {\bf uniform mesh}: $x_i=i/n$, $i=0, 1, \dots n$. Denote $z$ as the estimation.

\subsection{Finite Difference (Chap. 13)}

$z(0)=0$, $z(1)=1$. For any $x_i=i/n$, $i=1, \dots, n-1$, approximate the second order derivative using idea from Section~\ref{nd}:\\

Lagrange interpolation using $x_{i-1}$, $x_i$ and $x_{i+1}$ as interpolation points, we get
  \[p(x)=z((i-1)/n){(x-i/n)(x-(i+1)/n)\over 2/n^2}-z(i/n){(x-(i-1)/n)(x-(i+1)/n)\over 1/n^2}\]
  \[+z((i+1)/n){(x-i/n)(x-(i-1)/n)\over 2/n^2}\]
  So
  \[p''(x_i)=n^2(z((i+1)/n)+z((i-1)/n)-2z(i/n))\]
  So the question reduces to a system of equations:
  \[z(0)=0\]
  \[z(1)=1\]
  \[n^2(z((i+1)/n)+z((i-1)/n)-2z(i/n))=-z(i/n)\]

  When $n=3$, we get $z(1/3)=81/208$, $z(2/3)=153/208$. Error is about $0.0007$.

  
  \subsection{Finite Element Method (Chap. 14)}
  
Rewrite the differential equation into a {\bf variational problem}, which is minimizing
  
\[\int_0^1y'^2-y^2 dx\]

Now pick values of $z(i/n)$, such that the linear spline $g$ using $x_i$ minimizes $\int_0^1g'^2-g^2dx$.

When $n=3$, this gives us $z(1/3)=3025/7791$, $z(2/3)=5720/7791$. Error is about $0.0007$.


\section{Final review}
\begin{enumerate}
  \item Interpolation
  \begin{enumerate}
   \item Lagrange and Hermite interpolation
   \item Uniqueness
   \item Error Estimate
  \end{enumerate}
\item Orthogonal Polynomials
  \begin{enumerate}
  \item $p_d$ is a degree $d$ orthogonal polynomial of weight $w$ on $[a, b]$, iff for any polynomial $q$ of degree no more than $d-1$, $\int_a^bwp_dqdx=0$. 
    \item Gram-Schmidt process
   \item Example: Legendre and Chebyshev polynomials.
   \end{enumerate}
 \item Numerical integration
   \begin{enumerate}
   \item Quadrature rule
     \begin{enumerate}
     \item Error estimate
     \item Newton-Cotes quadrature. (Special cases: trapezium rule, Simpson's rule)
       \begin{enumerate}
        \item Improved error estimate for $n$ even. (Special case: error bound for Simpson's rule)
        \end{enumerate}
     \item Gauss quadrature (Special case: Gauss-Legendre quadrature)
       \begin{enumerate}
        \item Error bound
        \item Positive weights
        \item Convergence
        \end{enumerate}
     \end{enumerate}
    \item Composite methods
   \end{enumerate}
 \item Numerical solution for IVP
   \begin{enumerate}
   \item Methods
     \begin{enumerate}
     \item Linear Multistep Methods (Special case: AB, AM)
     \item Runge-Kutta methods (Special case: RK2, RK4, Heun)
      \end{enumerate}
    \item Analysis
      \begin{enumerate}
        \item One step: power series expansion w.r.t. $h$
        \item Global error
          \begin{enumerate}
          \item Zero stability and Dahlquist theorem
          \item One step method: $z(t+h)=z(t)+h\Phi(z(t))$, needs $\Phi$ Lipschitz.
          \end{enumerate}
        \item Domain of absolute stability
       \end{enumerate}
   \end{enumerate}

\end{enumerate}

\begin{exa}Consider the function $f(x)={1\over \pi}\cos(\pi x)$. Find the Hermite interpolation polynomial $p$ with interpolation points at $\pm1/2, \pm 3/2$. Find the error bound at $x=0$ using the error bound of Hermite interpolation.\end{exa}

Answer:
\[p=-{(x+{3\over 2})(x-{3\over 2})^2(x^2-1/4)^2\over 36}+{(x+{1\over 2})(x-{1\over 2})^2(x^2-{9\over 4})^2\over 4}\]
\[-{(x-{1\over 2})(x+{1\over 2})^2(x^2-{9\over 4})^2\over 4}+{(x+{3\over 2})(x-{3\over 2})^2(x^2-1/4)^2\over 36}\]
The error bound at $0$ is
\[{\max\|f^{(8)}\|\times {1\over 2}^4\times {3\over 2}^4\over 8!}={9\pi^7\over 1146880}\approx 0.0237\]
The actual error is $0.00972<0.0237$.\\

\begin{rem}When one add two more interpolation points at $\pm (m-{1\over 2})$, the theoretical error bound is multiplied by
  \[{\pi^4(m-{1\over 2})^4\over (4m-3)(4m-2)(4m-1)(4m)}\approx {\pi^4\over 4^4}<1\]
  So if we do interpolation at $\pm 1/2, \dots, \pm (m-1/2)$, as $m\rightarrow \infty$ the value of the interpolation polynomial at $x=0$ does converge to $1\over \pi$, though very slowly.
\end{rem}


\begin{exa}Consider IVP $y'=f(y)$, $y(0)=y_0$, where $|f|, |f^{(k)}|$ are all bounded by $1$. Consider the linear multistep method

  \[z(t+2h)=z(t)+{hf(z(t))\over 3}+{4hf(z(t+h))\over 3}+{hf(z(t+2h))\over 3}\]
    Here $z$ is an approximation of $y$. Find $C$ such that for sufficiently small $h$, if $z(t)=y(t)$, $z(t+h)=y(t+h)$, then
    \[|z(t+2h)-y(t+2h)|<Ch^5\]
  \end{exa}

  Answer:
  \begin{itemize}
  \item Firstly, by error bound of Simpson's rule,
    \[y(t+2h)=y(t)+\int_t^{t+2h}f(y(s))ds\]
    \[=y(t)+{hf(y(t))\over 3}+{4hf(y(t+h))\over 3}+{hf(y(t+2h))\over 3}+E\]
    Here
    \[|E|\leq{\max|(f\circ y)^{(4)}|(2h)^5\over 2880}\]
    \[\leq {4h^5\over 15}\]
    This is because
    \[|(f\circ y)^{(4)}|=|(ff'^4+11f^2f'^2f''+4f^3f''^2+7f^3f'f''+f^4f''')\circ y|\leq 24\]
  \item Now, by assumption,
    \[|y(t+2h)-z(t+2h)|\leq {h|y(t+2h)-z(t+2h)|\over 3}+|E|\]
    So, for example, if $h<1$, we have
    \[|y(t+2h)-z(t+2h)|\leq {2\over 5}h^5\]
  \end{itemize}
  

\begin{exa}Suppose we use quadrature rule to estimate $\int_0^1fdx$, $x_0=-1$, $x_1=0$. Can you find a $x_2$ such that the quadrature rule gives accurate answer for any polynomial of degree $3$?\end{exa}

Answer: For any $x_2$, the quadrature rule with 3 quadrature points $I_3$ will give accurate answer to any polynomial of degree at most $2$. Let $g_3=x(x+1)(x-x_3)$ (you can also choose any other polynomial of degree 3, e.g. $x^3$), then any polynomial of degree $3$ can be written as $cg_3+h$, where $c$ is a constant and $h$ is a polynomial of degree at most $2$. Hence
\[0=I_3(f)-\int_0^1fdx\]
\[=cI_3(g_3)+I_3(h)-c\int_0^1g_3dx-\int_0^1hdx\]
\[=c(I_3(g_3)-\int_0^1g_3dx)\]
So we only need to have $I_3(g_3)=\int_0^1g_3dx$. Because $g_3$ is zero at all three quadrature points, this is equivalent to $\int_0^1g_3dx=0$, by calculation $x_3=7/10$.



\begin{exa}Suppose $f$ and all its partial derivatives are bounded by $1$. Consider IVP: $y'=f(t, y)$, $y(0)=y_0$, and the implicit one-step method
  \[z(0)=y(0), z((n+1)h)=z(nh)+hf((n+1)h, z((n+1)h))\]
  \begin{itemize}
   \item Find the order of accuracy.
   \item Show that this method is A-stable, i.e. the domain of absolute stability covers the whole left half plane.
  \end{itemize}
\end{exa}

Answer: By Taylor series:
\[y(nh+h)=y(nh)+hy'(nh)+h^2{y''(nh)\over 2}+\dots\]
\[=y(nh)+hf(nh, y(nh))+{h^2\over 2}(f_t(nh, y(nh))+f(nh, y(nh))f_y(nh, y(nh))+\dots\]
Now assume $z(nh)=y(nh)$, and do power series expansion for $z(nh+h)$: suppose $z(nh+h)=y(nh)+ah+bh^2+O(h^3)$, then the formula for this implicit method becomes
\[y(nh)+ah+bh^2+O(h^3)=y(nh)+h(f(nh, y(nh))+f_t(nh, y(nh))h+f_y(nh, y(nh))(ah+bh^2+O(h^3))+O(h^2))\]
Compare coefficients on both sides, we get
\[a=f(nh, y(nh))\]
\[b=f_t(nh, y(nh))+f(nh, y(nh))f_y(nh, y(nh))\]
So $y(nh+h)-z(nh+h)=O(h^2)$, the order of accuracy is $1$.\\

To see domain of absolute stability, consider equation $y'=\lambda y$, $\lambda\in\mathbb{C}$ and $Re(\lambda)<0$. The method becomes:
\[z(nh+h)=z(nh)+h\lambda z(nh+h)\]
Hence the solution converges to $0$ iff $|{1\over 1-h\lambda}|<1$, which is true for all $h>0$ and all $Re(\lambda)<0$. 



\begin{exa}Apply Euler's method to the IVP $y'=f(t)$, $y(0)=0$. Bound the error at time $t=1$ using the derivative of $f$ and step size $1/n$.\end{exa}

Answer: \begin{itemize}
\item We can use the error bound for Euler's method discussed in the lecture, which is $Ch{e^L-1\over L}$ where $C$ is ${\max(|f_t|)+\max(|f|)\max(|f_y|)\over 2}={\max|f'|\over 2}$, and $L$ can be any positive number. Now let $L\rightarrow 0$, we get the upper bound ${\max|f'|h\over 2}={\max|f'|\over 2n}$.
\item Alternatively, we can use Euler's method to get the estimate of $y$ at time $1$, which is
  \[z(1)=\sum_{i=0}^{n-1}{1\over n}f({i\over n})\]
  So
  \[|y(1)-z(1)|=\sum_{i=0}^{n-1}\int_{i/n}^{(i+1)/n}|f(s)-f(i/n)|ds\]
  \[\leq n{\max|f'|\over 2n^2}={\max|f'|\over 2n}\]
\end{itemize}


\begin{exa}Consider solving IVP $y'=f(y), y(0)=0$. Here $f\in C^2$, $|f|\leq 1$, $|f'|\leq 1$, $|f''|\leq 1$
  \begin{itemize}
   \item Show that $y(t+h)=y(t)+\int_t^{t+h}f(y(s))ds$.
   \item Write down the Hermite interpolation of $f(y(s))$ using interpolation point $x_0=t$.
   \item Integrate this Hermite interpolation, and make a numerical method based on this integration.
   \item Find the order of accuracy of this method. Bound its truncated error.
  \end{itemize}
\end{exa}

Answer:
\begin{itemize}
\item This is fundamental theorem of calculus.
\item $p(s)=f(y(t))+f(y(t))f'(y(t))(s-t)$
\item $\int_t^{t+h}=hf(y(t))+{h^2\over 2}f(y(t))f'(y(t))$. So we can approximate the IVP using
  \[z(t+h)=z(t)+hf(z(t))+{h^2\over 2}f(z(t))f'(z(t))\]
\item When $z(t)=y(t)$, we have
  \[y(t+h)=y(t)+hf(y(t))+{h^2\over 2}f(y(t))f'(y(t))+{h^3\over 6}(f(y(s))f'^2(y(s))+f(y(s))f''(y(s)))\]
  Where $s\in [t, t+h]$.
  \[z(t+h)=y(t)+hf(y(t))+{h^2\over 2}f(y(t))f'(y(t))\]
  So the truncated error is bounded by
  \[|{y(t+h)-z(t+h)\over h}|\leq {\max|f|\max|f'|^2+\max|f|\max|f''|\over 6}h^2=O(h^2)\]
  So order of accuracy is $2$.

\end{itemize}

\section{HW Solutions}

\subsection{HW 4}

1. Let $f(x)=x^3$, $p$ be the Lagrange interpolation polynomial of $f$ using interpolation points $x=0$, $x=1$.  On the interval $[0, 1]$, find the point $c$ that maximizes the interpolation error $|f(c)-p(c)|$, and find another point $s\in [0, 1]$ such that
\[f(c)-p(c)=f''(s)c(c-1)/2\]

Answer:\\

\[p(x)=0\cdot{x-1\over 0-1}+1\cdot{x-0\over 1-0}=x\]
\[|f-p|=|x^3-x|\]
So this is maximalized at point $c={\sqrt{3}\over 3}$.
\[f(c)-p(c)=c^3-c=3sc(c-1)\]
So
\[s={c+1\over 3}={\sqrt{3}+3\over 9}\]

2. Let $f(x)=e^x$, $p$ be the Lagrange interpolation polynomial of $f$ on interval $[0, 2]$ using interpolation points $x_0=0$, $x_1=1$, $x_2=2$, find an upper bound for the $L^\infty$ norm of $f(x)-p(x)$ on $[0, 2]$, using the error bound of Lagrange polynomial we covered in the lecture (Theorem 6.2 in textbook, Theorem 1.5 in lecture notes).\\

Answer:\\

The error bound of Lagrange polynomial is
\[|f(x)-p(x)|={|f'''(c)||x(x-1)(x-2)|\over 3!}\]
When $c\in [0, 2]$, $|f'''(c)|\leq e^2$.\\
When $x\in [0, 2]$, $|x(x-1)(x-2)|\leq {2\sqrt{3}\over 9}$.\\
Hence an upper bound for this error is $e^2\sqrt{3}\over 27$.\\
It's ok if you get a slightly larger error bound, for example $4e^2/3$.\\

3. Suppose $f$ is continuous and with continuous derivatives of order up to and including 5 on $[a, b]$, and there are three distinct points $x_0$, $x_1$, $x_2$ in $[a, b]$. Let $y_i=f(x_i)$, $i=0, 1, 2$; $z_j=f'(x_j)$, $j=0, 2$.
\begin{enumerate}
\item Find a polynomial $p$ of degree at most $4$, such that $p(x_i)=y_i$, $i=0, 1, 2$; $p'(x_j)=z_j$, $j=0, 2$.
\item Use an argument similar to the error estimate of Hermite interpolation polynomial to show that for any $x\in [a, b]$, there is some number $s\in [a, b]$ such that
  \[f(x)-p(x)=f^{(5)}(s)(x-x_0)^2(x-x_1)(x-x_2)^2/5!\]
\end{enumerate}

Answer:\\

\begin{enumerate}
\item \begin{itemize}
  \item Approach I: We can find five polynomials $p_0$, $p_1$, $p_2$, $q_0$, $q_2$, such that
    \[p_0(x_0)=p_1(x_1)=p_2(x_2)=q'_0(x_0)=q'_2(x_2)=1\]
    \[p_i(x_j)=0\text{ when }i\not=j\]
    \[p_i'(x_j)=0\text{ when }j=0, 2\]
    \[q'_0(x_2)=q'_2(x_0)=0\]
    \[q_i(x_j)=0\]
    Then the answer can be written as
    \[p=\sum_iy_ip_i+z_0q_0+z_2q_2\]
    To get $p_0$, from $p_0(x_1)=p_0(x_2)=p'_0(x_2)=0$ we get $p_0=(x-x_1)(x-x_2)^2(Ax+B)$, now use the remaining two conditions, $p_0(x_0)=1$, $p_0'(x_0)=0$, to solve for $A$ and $B$, we get
    \[p_0={(x-x_1)(x-x_2)^2\over (x_0-x_1)(x_0-x_2)^2}(1-(x-x_0)({1\over x_0-x_1}+{2\over x_0-x_2}))\]
    Similarly,
    \[p_1={(x-x_0)^2(x-x_2)^2\over (x_1-x_0)^2(x_1-x_2)^2}\]
    \[p_2={(x-x_0)^2(x-x_1)\over (x_2-x_0)^2(x_2-x_1)}(1-(x-x_2)({2\over x_2-x_0}+{1\over x_2-x_1}))\]
    \[q_0={(x-x_0)(x-x_1)(x-x_2)^2\over (x_0-x_1)(x_0-x_2)^2}\]
    \[q_2={(x-x_0)^2(x-x_1)(x-x_2)\over (x_2-x_0)^2(x_2-x_1)}\]
    So
    \[p=y_0{(x-x_1)(x-x_2)^2\over (x_0-x_1)(x_0-x_2)^2}(1-(x-x_0)({1\over x_0-x_1}+{2\over x_0-x_2}))\]
    \[+y_1{(x-x_0)^2(x-x_2)^2\over (x_1-x_0)^2(x_1-x_2)^2}\]
    \[+y_2{(x-x_0)^2(x-x_1)\over (x_2-x_0)^2(x_2-x_1)}(1-(x-x_2)({2\over x_2-x_0}+{1\over x_2-x_1}))\]
    \[+z_0{(x-x_0)(x-x_1)(x-x_2)^2\over (x_0-x_1)(x_0-x_2)^2}\]
    \[+z_2{(x-x_0)^2(x-x_1)(x-x_2)\over (x_2-x_0)^2(x_2-x_1)}\]
  \item We can also use Hermite interpolation polynomial: suppose $p'(x_1)=a$, use all information for $p(x_i)$, $i=0, 1, 2$ and $p'(x_i)$, $i=0, 1, 2$, we can write down the Hermite interpolation polynomial which is a polynomial of degree at most $5$. The coefficient for $x^5$ is
    \[-{y_0\over (x_0-x_1)^2(x_0-x_2)^2}({2\over x_0-x_1}+{2\over x_0-x_2})\]
    \[-{y_1\over (x_1-x_0)^2(x_1-x_2)^2}({2\over x_1-x_0}+{2\over x_1-x_2})\]
    \[-{y_2\over (x_2-x_1)^2(x_2-x_0)^2}({2\over x_2-x_1}+{2\over x_2-x_0})\]
    \[+{z_0\over (x_0-x_1)^2(x_0-x_2)^2}+{a\over (x_1-x_0)^2(x_1-x_2)^2}\]
    \[+{z_2\over (x_2-x_0)^2(x_2-x_1)^2}\]
    Since we want $p$ to be of degree no more than $4$, we must set this coefficient to be $0$. Hence
    \[a={y_0(x_1-x_2)^2\over (x_0-x_2)^2}({2\over x_0-x_1}+{2\over x_0-x_2})\]
      \[+y_1({2\over x_1-x_0}+{2\over x_1-x_2})\]
      \[+{y_2(x_1-x_0)^2\over (x_2-x_0)^2}({2\over x_2-x_1}+{2\over x_2-x_0})\]
      \[-{z_0(x_1-x_2)^2\over (x_0-x_2)^2}-{z_2(x_1-x_0)^2\over (x_2-x_0)^2}\]
      Now put this in the formula for Hermite interpolation polynomials, you'll get the exact same answer as above.
 \end{itemize}

    \item If $x=x_i$ it's trivially true. Now suppose $x\not=x_i$ for any $i$, consider
      \[G(t)=f(t)-p(t)-{(f(x)-p(x))(t-x_0)^2(t-x_1)(t-x_2)^2\over (x-x_0)^2(x-x_1)(x-x_2)^2}\]
      $G(x)=G(x_i)=G'(x_0)=G'(x_2)=0$, so $G'$ is zero at at least 5 points, $G^{(5)}$ is zero at at least one point. Let that point be $s$, then $G^{(5)}(s)=0$ implies the equation we need to prove.
\end{enumerate}


4. Let $q_j=(1-x^2)^j$, $\varphi_j=q_j^{(j)}$, show that $\varphi_j$ are orthogonal to each other in $L^2([-1, 1])$. In other words, if $j\not=j'$, $\int_{-1}^1\varphi_j\varphi_{j'}dx=0$. \\

Answer:\\

Firstly we show that if $i<j$, then $q_j^{(i)}$ has a factor $(1-x^2)^{j-i}$. Do induction on $i$. It is trivially true for $i=0$. Now, suppose $q_j^{(i)}=(1-x^2)^{j-i}h(x)$ where $h$ is a polynomial, then, by product rule,
\[q_j^{(i+1)}=((1-x^2)^{j-i}h(x))'=-2(j-i)x(1-x^2)^{j-i-1}h(x)+(1-x^2)^{j-i}h'(x)\]
\[=(1-x^2)^{j-i-1}(-2(j-i)xh(x)+(1-x^2)h'(x))\]
Hence by induction this statement is proved.\\

Now, because $\varphi_i$ are all non-zero, they all have non-zero $L^2$ norms on $[-1, 1]$. We only need to show that when $i\not=j$, $\int_{-1}^1\varphi_i\varphi_jdx=0$. Without loss of generality assume $i<j$, then by integration by parts and the conclusion in the previous step,
\[\int_{-1}^1\varphi_i\varphi_jdx=\int_{-1}^1 q_i^{(i)}q_j^{(j)}dx\]
\[=-\int_{-1}^1q_i^{(i+1)}q_j^{(j-1)}dx\]
\[=\int_{-1}^1q_i^{(i+2)}q_j^{(j-2)}dx\]
\[=\dots=(-1)^j\int_{-1}^1q_i^{(i+j)}q_jdx\]
However the degree of $q_i$ is $2i<i+j$, hence $q_i^{(i+j)}=0$, which implies that the integration is zero.\\


5. Find three distinct points $x_0$, $x_1$ and $x_2$ in $(-1, 1)$, such that for any polynomial function $f$ of degree $3$, the best approximation of $f$ under $L^2$ norm on $[-1, 1]$ of degree at most $2$ coincides with the Lagrange interpolation polynomial of $f$ using interpolation points $x_0$, $x_1$ and $x_2$.\\

Answer:\\

Suppose $f$ is the degree 3 Legendre polynomial $f_3=x^3-{3\over 5}x$, then, because it is orthogonal to the degree 0, 1, and 2 Legendre polynomials under $L^2([-1, 1])$, and these three Legendre polynomials form an orthogonal basis of the space $V_2$ of polynomials of degree no more than 2, the best approximation formula in inner product space implies that the best approximation of $f$ on $V_2$ under $L^2([-1, 1])$ norm must be $0$. By assumption, the Legendre interpolation of $f_3$ at $x_0$, $x_1$ and $x_2$ must also be zero, so these three points can only be the three roots of $x^3-{3\over 5}x$, which are $0, \pm\sqrt{3\over 5}$.\\

Now suppose $f=\sum_{i=0}^3a_ix^i$ is any degree $3$ polynomial. Then, because $f-a_3f_3$ is of degree at most $2$ and is identical to $f$ at $0, \pm\sqrt{3\over 5}$, the Lagrange interpolation of $f$ at $x_i$ is $f-a_3f_3$. On the other hand, let $e_0, e_1, e_2$ be any orthogonal basis of $V_2$, then the best approximation of $f$ on $V_2$ under $L^2([-1, 1])$ norm is $\sum_i(f, e_i)e_i$. However, because $f_3$ is orthogonal to $V_2$, $(f, e_i)=(f-a_3f_3, e_i)$, so the best approximation of $f$ is the same as the best approximation of $f-a_3f_3$, which must be $f-a_3f_3$ itself as $f-a_3f_3\in V_2$. This proves that $x_i$ being $0, \pm\sqrt{3\over 5}$ satisfies the requirement in the problem.\\


6. Let $f$ be a continuous function on $[0, 1]$, $p_n$ be the polynomial of best approximation of degree no more than $n$ under the $L^2$ norm. Then, after studying Theorem 9.5 in the textbook, which proved that $f-p_n$ is zero at at least $n+1$ distinct points in $(0, 1)$, find a function $f$ such that $f-p_2$ is zero at $4$ distinct points in $(0, 1)$.\\

Answer:\\

Let $V_2$ be the space of polynomials of degree no more than $2$. If we pick $f$ to be anything orthogonal to $V_2$ under the $L^2([0, 1])$ norm, then the best approximation of $f$ on $V_2$ must be zero, so we just need to pick such a $f$ with $4$ or more zeros. So, for example, we can pick the degree $4$ orthogonal polynomials with weight $1$ on $[0, 1]$, which is $70x^4-140x^3+90x^2-20x+1$.

\subsection{HW 5}

1. (Problem 7.6 in textbook) Consider the trapezium and Simpson's rule applied to $\int_0^1(x^5-Cx^4)dx$.
\begin{itemize}
\item Write down the error for trapezium and Simpson's rule, as functions of $C$.
\item Find $C$ that makes the error under trapezium rule is $0$.
\item Find the range of $C$ where the trapezium rule is more accurate than Simpson's rule.
\end{itemize}

Answer:
\begin{itemize}
\item The true answer is $1/6-C/5$. The result of the trapezium rule is ${1-c\over 2}$, and the result of Simpson's rule is ${2\over 3}(1/32-C/16)+{1\over 6}(1-c)=3/16-{5c/24}$. So the error under trapezium rule is $|1/3-3C/10|$, the error under Simpson's rule is $|1/48-C/120|$. It's ok if you do not write the absolute value.
\item We need to have $|1/3-3C/10|=0$, hence $C=10/9$.
\item This is the range of $C$ such that $|1/3-3C/10|<|1/48-C/120|$, hence $C\in (15/14, 85/74)$.
\end{itemize}


2. (Problem 7.11 in textbook) Suppose $f\in C^4([-1, 1])$. Let $p$ be the Hermite interpolation polynomial of $f$ at $x_0=-1$, $x_1=1$.
\begin{itemize}
\item Calculate $\int_{-1}^1pdx$ and write it as a linear combination of $f(\pm 1)$, $f'(\pm 1)$.
\item Prove that
  \[|\int_{-1}^1fdx-\int_{-1}^1pdx|\leq {2\over 45}\max_{c\in [-1, 1]}|f^{(4)}(c)|\]
\end{itemize}

Answer:
\begin{itemize}
\item
  \[p(x)=f'(-1){(x+1)(x-1)^2\over 4}+f'(1){(x-1)(x+1)^2\over 4}\]
  \[+f(-1){(x-1)^2(1+(x+1))\over 4}+f(1){(x+1)^2(1-(x-1))\over 4}\]
  So
  \[\int_{-1}^1 pdx={f'(-1)\over 3}-{f'(1)\over 3}+f(-1)+f(1)\]
\item The error bound for Hermite interpolation tells us
  \[|f(x)-p(x)|\leq {\max|f^{(4)}|(x+1)^2(x-1)^2\over 4!}\]
  So
  \[|\int_{-1}^1fdx-\int_{-1}^1pdx|\leq \int_{-1}^1|f-p|dx\]
  \[\leq \max|f^{(4)}|\cdot \int_{-1}^1 {(x+1)^2(x-1)^2\over 4!}dx\]
    \[={2\over 45}\max_{c\in [-1, 1]}|f^{(4)}(c)|\]
\end{itemize}



3. (Problem 10.3 in textbook) Show that if $f\in C^2([0, 1])$, then there is some point $c\in (0, 1)$ such that
\[\int_0^1xfdx={1\over 2}f(2/3)+{1\over 72}f''(c)\]
Hint: use Gauss quadrature with weight $x$.\\

Answer: Because for any constant function $C$, $\int_0^1xC(x-2/3)dx=0$, $x-2/3$ is the degree-$1$ orthogonal polynomial on $[0, 1]$ with weight $x$. Hence the weight $x$ Gauss quadrature for $\int_0^1xfdx$ should be the $I_0(f)=w_0f(x_0)$, where $x_0$ is the root of $x-2/3$ which is $2/3$, and
\[w_0=\int_0^1w(x){\prod_{i\not=0}(x-x_i)\over \prod_{i\not=0}(x_0-x_i)}dx=\int_0^1wdx={1\over 2}\]
Now the error formula for Gauss quadrature tells us
\[\int_0^1xfdx-I_0(f)=f''(c)\int_0^1{x(x-2/3)^2\over 2!}dx={f''(c)\over 72}\]

4. \begin{itemize}
\item Suppose $f$ is continuous on $[0, 1]$. Let $I_n$ be the estimate of $\int_0^1fdx$ using composite trapezium rule with $n$ subintervals. Show that
  \[\lim_{n\rightarrow\infty}|\int_0^1fdx-I_n|=0\]
 Hint: There are many possible approaches. You can use the fact that any continuous function on a closed interval is uniformly continuous (for any $\epsilon>0$, there is some $\delta$ such that $|x-y|<\delta$ implies $|f(x)-f(y)|<\epsilon$), or use the Weierstrass approximation theorem.
\item (Optional) Find a continuous function $f$, such that there is $C>0$ such that 
  \[|\int_0^1fdx-I_n|\geq {C\over n}\]
  Hint: if $f$ has bounded second derivative then the error decays like $O(1/n^2)$, so you need to find some $f$ that doesn't have second order derivative or has unbounded second order derivative.
\end{itemize}

Answer:
\begin{itemize}
\item \begin{itemize}
    \item Approach I: $f$ is continuous on $[0, 1]$ hence uniformly continuous. For any $\epsilon>0$, find $\delta$ such that $|x-y|<\delta$ implies $|f(x)-f(y)|<\epsilon$. Now let $N$ be some integer larger than $1/\epsilon$. For any $n>N$, consider the composite trapezium rule $I_n$, then $f$ sends each of the $n$ subintervals to an interval of length no more than $\epsilon$, hence the error of the trapezium rule on this subinterval is no more than $\epsilon/n$, and the error of the composite trapezium rule is no more than $\epsilon$, hence $\lim_{n\rightarrow\infty}I_n(f)=\int_0^1fdx$ by definition of limit.
    \item Approach II: For any $\epsilon>0$, find polynomial $p$ such that $|f-p|_\infty<\epsilon/3$, then $|\int_0^1fdx-\int_0^1pdx|<\epsilon/3$, and for any $n$, $|I_n(f)-I_n(p)|<\epsilon/3$. Now let $N$ be large enough such that ${\max|p''|\over 12N^2}<\epsilon/3$, then for any $n>N$, $|I_n(p)-\int_0^1pdx|<\epsilon/3$, hence
      \[|I_n(f)-\int_0^1fdx|\leq |\int_0^1fdx-\int_0^1pdx|+|I_n(p)-\int_0^1pdx|+|I_n(f)-I_n(p)|<\epsilon\]
   \end{itemize}
 \item Let $f(x)=\sum_{i=1}^\infty{1\over i(i+1)}\cos(2\pi i!x)$. Then $\int_0^1fdx=0$, and by trigonometry, $I_n(f)={1\over n}$. 
 \end{itemize}

 \subsection{HW 6}

 
1. (Problem 12.7 in textbook) Consider solving $y'=f(t, y), y(0)=y_0$ using the trapezium method
\[z((n+1)h)=z(nh)+{h\over 2}(f((n+1)h, z((n+1)h))+f(nh, z(nh)))\]
Suppose further that $|y'''|$ is uniformly bounded by $M$.
\begin{enumerate}
\item Prove that
  \[|{y((n+1)h)-y(nh)\over h}-{1\over 2}(f((n+1)h, y((n+1)h))+f(nh, y(nh)))|\leq {Mh^2\over 12}\]
  Hint: You can prove it by applying integration by parts to $\int_{nh}^{(n+1)h}(x-hn)(x-nh-h)y'''(x)dx$. 
\item Let $e_n=z(nh)-y(nh)$, and assume that $f$ is $L$-Lipschitz with respect to the second parmeter, then
  \[|e_{n+1}|\leq |e_n|+{1\over 2}hL(|e_{n+1}|+|e_n|)+{h^3M\over 12}\]
\end{enumerate}

Answer:

\begin{enumerate}
\item\begin{itemize}
  \item Approach I: Left hand side is the error for applying trapezium rule to calculate $\int_{nh}^{(n+1)h}f(s, y(s))ds$, and right hand side is the error bound we learned in class.
  \item Approach II:
    \[{h^3M\over 6}\geq |\int_{nh}^{(n+1)h}(x-hn)(x-nh-h)y'''(x)dx|\]
    \[=|(x-hn)(x-nh-h)y''|_{nh}^{(n+1)h}-\int_{nh}^{(n+1)h}(2x-2nh-h)y''(x)dx|\]
    \[=|\int_{nh}^{(n+1)h}(2x-2nh-h)y''(x)dx|\]
    \[=|(2x-2nh-h)y'|_{nh}^{(n+1)h}-\int_{nh}^{(n+1)h}2y'dx|\]
    \[=|h(f((n+1)h, y((n+1)h))+f(nh, y(nh)))-2(y((n+1)h)-y(nh))|\]
    Divide $h$ on both sides we get the required inequality.
    
   \end{itemize}
 \item From the inequality proved above, we have
   \[y((n+1)h)=y(nh)+{h\over 2}(f((n+1)h, y((n+1)h))+f(nh, y(nh)))+E\]
   Where $|E|\leq {Mh^3\over 12}$. Hence,
   \[|e_{n+1}|=|y((n+1)h)-z((n+1)h)|\leq |y(nh)-z(nh)|+{hL\over 2}(|y((n+1)h)-z((n+1)h)|+|y(nh)-z(nh)|)+|E|\]
   \[\leq |e_n|+{hL\over 2}(|e_{n+1}|+|e_n|)+{h^3M\over 12}\]
\end{enumerate}

2. (Problem 12.12 in textbook) Consider solving the initial value problem $y'=f(t, y), y(0)=y_0$ via linear multistep method:
\[z((n+3)h)+bz((n+1)h)+az(nh)=hf((n+2)h, z((n+2)h))\]
\begin{enumerate}
\item Find $a, b$ such that the method is consistent.
%\item Verify that for such $a, b$, the order of accuracy is $1$.
\item Show that for such $a, b$, the method is not zero stable.
\end{enumerate}

Answer:
\begin{enumerate}
\item $1+b+a=0$, $3+b=1$, so $b=-2$, $a=1$.
\item The first characteristic polynomial is now $z^3-2z+1$, which has a root ${-1-\sqrt{5}\over 2}$ hence is not zero-stable.
\end{enumerate}

\subsection{Honors Assignment 2}

1. (Exercise 7.13) Show that the composite Trapezium rule always give accurate answer to $\int_0^{2\pi}\sin(x)dx$.\\  

Answer: The composite trapezium rule with $n$ subintervals is
\[I_n={1\over n}\sum_{k=1}^{n-1}\sin({2\pi k\over n})={1\over n}\sum_{k=1}^{n-1}(\sin({2\pi k\over n})+\sin({2\pi(n-k)\over n}))/2\]
\[=0=\int_0^{2\pi}\sin(x)dx\]

2. (Exercise 10.7) Let $[a, b]=[-1, 1]$, let $p_{n-1}$ be the degree $n-1$ orthogonal polynomial of weight $1-x^2$, and let $I_n$ be the quadrature rule where the quadrature points are roots of $(x^2-1)p_{n-1}(x)$.
\begin{itemize}
\item Show that if $q$ is a polynomial of degree no more than $2n-1$, then $\int_{-1}^1qdx=I_n(q)$.
\item Show that all quadrature weights are positive.
\item Suppose $f$ is smooth, find a constant $C$ such that
  \[|\int_{-1}^1fdx-I_n(f)|\leq C\max_{x\in [-1, 1]}|f^{(2n)}(x)|\]
\end{itemize}

Answer:\\

\begin{itemize}
\item $I_n$ has $n+1$ quadrature points hence gives accurate answer to any polynomial of degree up to $n$. If $q$ is of degree no more than $2n-1$, by long division of polynomials we have $q=(x^2-1)p_{n-1}q_1+r$, where $r$ is of degree at most $n$, and $q_1$ is a polynomial of degree no more than $n-2$. Hence $I_n(q)=I_n(r)=\int_{-1}^1rdx=\int_{-1}^1qdx$.  
\item The $j$-th quadrature weight is
  \[w_j=\int_{-1}^1 {\prod_{i\not=j}(x-x_i)\over \prod_{i\not=j}(x_j-x_i)}dx\]
  If $j=0$ or $j=n$, the function being integrated is non negative, hence $w_j>0$. Now suppose $0<j<n$, then by calculation,
  \[w_j=\int_{-1}^1{x^2-1\over x_j^2-1}\cdot {\prod_{i\not=j, 1<i<n}(x-x_i)^2\over \prod_{i\not=j, 1<i<n}(x_j-x_i)^2}dx\]
  Which are all positive.
\item Let $p$ be the polynomial of degree no more than $2n-1$ such that $p(x_i)=f(x_i)$, and for all $1<i<n$, $p'(x_i)=f'(x_i)$. Then by a similar argument to the error bound of Hermite interpolation polynomials we have
  \[|f(x)-p(x)|\leq \max|f^{(2n)}|{(1-x^2)\prod_{1<i<n}(x-x_i)^2\over (2n)!}\]
  So
  \[C=\int_{-1}^1 {(1-x^2)\prod_{1<i<n}(x-x_i)^2\over (2n)!}dx\]
\end{itemize}

%3. Suppose $f$ is smooth and periodic with period $1$, $|f^{(4)}|\leq 1$. Let $I_n$ be the result of composite trapezium rule for $\int_0^1fdx$ using $n$ subintervals. Find the largest integer $d$, and a number $C$, such that
%\[|\int_0^1fdx-I_n(f)|\leq {C\over n^d}\]
%Hint: You can do it using Hermit cubic spline. The integral of the linear spline is the result from composite trapezium rule, show that this integral is the same as the integral of the Hermit cubic spline.\\

3. Consider the initial value problem $y'=\sin(y)$, $y(0)=1$.

\begin{itemize}
\item Write down the formula for two step Adams-Bashforth.
\item Show that the two step Adams-Bashforth has order of accuracy $2$ for this problem.
\item Suppose we use starting points $z(0)=1$, $z(h)=1+h\sin(1)$ to carry out Adams-Bashforth till time $t=nh=1$. Find number $C$ such that
  \[|z(1)-y(1)|\leq Ch^2\]
\end{itemize}

Answer:

\begin{itemize}
\item The quadrature weights for $\int_{t+h}^{t+2h}$, using $x_0=t$, $x_1=t+h$, are
  \[w_0=\int_{t+h}^{t+2h}{(s-t-h)\over -h}ds=-h/2\]
  \[w_1=\int_{t+h}^{t+2h}{(s-t)\over h}ds=3h/2\]
  So the 2nd order Adams-Bashforth is
  \[z(t+2h)-z(t+h)=h({3\over 2}f(t+h, z(t+h))-{1\over 2}f(t, z(t)))\]
\item This can be done by doing power series expansion on both sides, or via the error formula for quadrature rules.
\item Suppose $z_k(nh)$ satisfies
  \[z_k(nh)=\begin{cases}y(nh) & n\leq k\\ z_k((n-1)h)+{3h\over 2}\sin(z_k((n-1)h))-{h\over 2}\sin(z_k((n-2)h)) & n>k, n\geq 2\\ 1+h & n=1, k=0 \end{cases}\]
  Then by analyzing the truncated error for Euler's and Adams-Bashforth methods, we get
  \[|z_k((k+1)h)-z_{k+1}((k+1)h)|=|z_k((k+1)h)-y((k+1)h)|\leq \begin{cases}{h^2\over 2} & k=0 \\ {5h^3\over 6} & k>0\end{cases}\]
  You may be able to find better bounds.\\

  Now we prove by induction on $m$ that $|z_k((k+1+m)h)-z_{k+1}((k+1+m)h)|\leq (1+2h)^m|z_k((k+1)h)-z_{k+1}((k+1)h)|$: when $m=0$ or $m=1$ one can verify it directly. If $m>1$, the left hand side is bounded by
  \[|z_k((k+m)h)-z_{k+1}((k+m)h)|+{3h\over 2}|z_k((k+m)h)-z_{k+1}((k+m)h)|+{h\over 2}|z_k((k+m-1)h)-z_{k+1}((k+m-1)h)|\]
  \[\leq ((1+2h)^{m-1}+{3h\over 2}(1+2h)^{m-1}+{h\over 2}(1+2h)^{m-2})|z_k((k+1)h)-z_{k+1}((k+1)h)|\]
  \[\leq (1+2h)^m|z_k((k+1)h)-z_{k+1}((k+1)h)|\]
  So
  \[|z(nh)-y(nh)|\leq \sum_k|z_k(nh)-z_{k+1}(nh)|\]
  \[\leq\sum_{k=0}^{n-1}(1+2h)^{n-k-1}h^2\]
  \[\leq e^2{h^2\over 2}+{e^2-1\over 2h}{5h^3\over 6}\]
  \[={11e^2-5\over 12}h^2\]
\end{itemize}

\subsection{Final Review Questions}

\subsubsection{Basic Problems}

1. Write down the Hermite interpolation polynomial $p(x)$ of $f(x)=\sin(x)$ at $x_0=0$, $x_1=\pi$, and find an upper bound of $|f(x)-p(x)|$ using the error bound of Hermite interpolation.\\

Answer: The Hermite interpolation polynomial is
\[p(x)={1\over \pi^2}(x(x-\pi)^2+x^2(\pi-x))\]
And
\[|f(x)-p(x)|={|-\sin(s)|x^2(x-\pi)^2\over 4!}\leq{x^2(x-\pi)^2\over 24}\]

2. Find two points $x_0$ and $x_1$, such that for any polynomial $f$ of degree no more than $3$,
\[\int_0^\pi\sin(x)f(x)dx=f(x_0)+f(x_1)\]
And find constant $c$ such that if $g\in C^6([0, \pi])$,
\[|\int_0^\pi\sin(x)g(x)dx-(g(x_0)+g(x_1))|\leq C\max|g^{(6)}|\]

Answer: These two points are the Gauss quadrature points on interval $[0, \pi]$ with weight function $\sin(x)$, hence must be the root of the degree-2 orthogonal polynomial on $[0, \pi]$ with weight $\sin(x)$. Suppose this polynomial is $p_2=x^2+ax+b$, then
\[0=\int_0^\pi \sin(x)p_2(x)dx=\pi^2-4+a\pi+2b\]
\[0=\int_0^\pi \sin(x)xp_2(x)dx=\pi^3-6\pi+a(\pi^2-4)+b\pi\]
So $a=-\pi$, $b=2$, $x_0={\pi-\sqrt{\pi^2-8}\over 2}$, $x_1={\pi+\sqrt{\pi^2-8}\over 2}$.
And by Theorem 10.1 from the textbook or 3.19(iv) in the Lecture notes,
\[C={\int_0^\pi\sin(x)(x^2-\pi x+2)^2dx\over 6!}={10-\pi^2\over 180}\]

3. Estimate the solution of $y'=\sin(y)$, $y(0)=1$ at time $0.1$ using Euler's method, improved Euler's method, and rk4, using time step $h=0.1$. \\

Answer:
\begin{itemize}
\item Euler's method gets $z(0.1)=1+0.1\times\sin(1)=1+\sin(1)/10\approx 1.0841471$.
\item Improved Euler's method gets $z(0.1)=1+{1\over 20}(\sin(1)+\sin(1+\sin(1)/10))\approx 1.0862688$
\item Runge-Kutta 4-th order gets $k_1=\sin(1)$, $k_2=\sin(1+k_1/20)$, $k_3=\sin(1+k_2/20)$, $k_4=\sin(1+k_3/10)$
  \[z(0.1)=1+{\sin(1)\over 60}+{\sin(1+\sin(1)/20)\over 30}\]
  \[+{\sin(1+\sin(1+\sin(1)/20)/20)\over 30}\]
  \[+{\sin(1+\sin(1+\sin(1+\sin(1)/20)/20)/10)\over 60}\approx 1.0863557\]
\end{itemize}
The accurate answer is $1.0863558$.\\


4. Consider explicit 2-step method for $y'=f(t, y)$:
\[z((n+2)h)=az((n+1)h)+bz(nh)+chf((n+1)h, z((n+1)h))+dhf(nh, z(nh))\]
Where $h$ is step size and $z(t)$ is the estimate for $y(t)$. Find all real numbers $a, b, c, d$ such that the method is zero stable and has order of accuracy at least $2$.

Answer: The first characteristic polynomial is
\[\rho(z)=z^2-az-b\]
To make it consistent, $\rho(1)=0$, $c+d=2-a$, so $1-a-b=0$, $b=1-a$, and the other root must be $a-1$, so $0\leq a<2$ and $b=1-a$.

Now let's calculate the order of accuracy. Firstly, let $t=nh$, suppose $y$ is the solution of the IVP, then
\[y'(t)=f(t, y(t))\]
\[y''(t)=\partial_tf(t, y(t))+\partial_yf(t, y(t))y'(t)\]

Now let's do power series expansion, with respect to $h$, for
\[y(t+2h)-ay(t+h)-(1-a)y(t)-chf(t+h, y(t+h))-dhf(t, y(t))\]
And after cancelling some terms, we get
\[2y''(t)h^2-ay''(t)h^2/2-c\partial_t f(t, y(t))h^2-c\partial_y f(t, y(t))y'(t)h^2+O(h^3)\]
So $c=2-a/2$, $d=-a/2$. Note that when $a=1$ this is 2-step Adams-Bashforth.\\


\subsubsection{More advanced problems}

Problems like the ones below will account for no more than 10\% of the final exam, so don't worry about them unless you have a lot of time during final review.\\

5. Suppose $f$ is smooth and periodic with period $1$, $|f^{(4)}|\leq 1$. Let $I_n$ be the result of composite trapezium rule for $\int_0^1fdx$ using $n$ subintervals. Find a number $C$, such that
\[|\int_0^1fdx-I_n(f)|\leq {C\over n^4}\]

Answer: Consider the function $f_n(x)=\sum_{i=0}^{n-1}(x+i/n)$. Then $f_n$ is periodic with period $1/n$, and it is easy to see that the composite trapezium rule for $\int_0^1fdx$ using $n$ subintervals is the same as the trapezium rule for $\int_0^{1/n}f_ndx$.\\

Now let $p_n$ be the Hermite interpolation of $f_n$ at $0$ and $1/n$. Then because $f_n(0)=f_n(1/n)$, $f_n'(0)=f_n'(1/n)$, we have
\[p_n(x)=f_n(0)+2f'_n(0)n^2x(x-{1\over 2n})(x-{1\over n})\]
\[\int_0^{1/n}p_n(x)dx=f_n(0)/n=I_n(f)\]
So
\[|\int_0^1fdx-I_n(f)|\leq \int_0^{1/n}|f_n(x)-p_n(x)|dx\leq \int_0^{1/n}{\max|f_n^{(4)}|x^2(x-1/n)^2\over 24}dx\leq {1\over 720n^4}\]

6. Consider the 3-step Adams-Bashforth method for $y'=\cos(y)$:
\[z(t+3h)=z(t+2h)+{23h\over 12}f(z(t+2h))-{4h\over 3}f(z(t+h))+{5h\over 12}f(z(t))\]
Suppose $z(t)=y(t)$, $z(t+h)=y(t+h)$, $z(t+2h)=y(t+2h)$, find $C$ such that
\[|z(t+3h)-y(t+3h)|\leq Ch^4\]

Answer: Let $g(t)=y'(t)=\cos(y(t))$, $p_3$ be the Lagrange interpolation of $g$ at $t$, $t+h$, $t+2h$, then the 3-step Adams-Bashforth can be written as
\[z(t+3h)=y(t+2h)+\int_{t+2h}^{t+3h}p_3(s)ds\]
So
\[|z(t+3h)-y(t+3h)|\leq \int_{t+2h}^{t+3h}|g(s)-p_3(s)|ds\]
Now by error estimate of Lagrange interpolation,
\[|g(s)-p_3(s)|\leq{\max|g^{(3)}|(s-t)(s-t-h)(s-t-2h)\over 6}\]
So after integration we get
\[|z(t+3h)-y(t+3h)|\leq \max|g^{(3)}|\cdot{3h^4\over 8}\]

\[g'=-y'\sin(y)=-\cos(y)\sin(y)=-{\sin(2y)\over 2}\]
\[g''=-\cos(y)\cos(2y)=-{\cos(3y)+\cos(y)\over 2}\]
\[g'''={3\sin(3y)\cos(y)+\sin(y)\cos(y)\over 2}\]
So $|g'''|\leq 7/4$, or you can use a better bound, and $C=21/32$.


\section{Notes on Prior Subjects}

\subsection{Notes on linear algebra}

Recall that a (real) {\bf Vector space} $V$ is a set with an element $0$, a ``scalar multiplication'' map $\mathbb{R}\times V\rightarrow V$ and an ``addition'' map $V\times V\rightarrow V$, such that, for any $x, y, z\in V$, any $a, b\in \mathbb{R}$, the followings are true:
\begin{enumerate}
\item $x+y=y+x$
\item $(x+y)+z=x+(y+z)$
\item $0+x=x$
\item $1x=x$
\item $0x=0$
\item $(a+b)x=ax+bx$
\item $a(x+y)=ax+ay$
\item $(ab)x=a(bx)$
\end{enumerate}

If $V$ is a vector space, any non empty subset $V'\subset V$ which is closed under addition and scalar multiplication is called a {\bf subspace}.\\

The {\bf span} of a set $S\subset V$ is the subset of $V$ consisting of finite linear combinations of elements of $S$. We call $S\subset V$ a {\bf linearly independent set} if for any finite collection of vectors $s_1, \dots s_n\in S$, $\sum_ia_is_i=0\implies a_i=0\forall i$. We call $S\subset V$ a {\bf basis} of $V$ if $S$ is linearly independent and $V=span(S)$.\\

Any two basis of the same vector space have the same cardinality (number of elements). This cardinality is called the {\bf dimension} of $V$.\\

\begin{exa}
  \begin{itemize}
  \item The set of $n$ dimensional column vectors $\mathbb{R}^n$, under the usual addition and scalar multiplication, is a vector space, and it has dimension $n$.
  \item The set of polynomials of degree no more than $n$, under the usual addition and scalar multiplication, is also a vector space. A basis is $\{1, x, \dots, x^n\}$ hence its dimension is $n+1$.
  \end{itemize}
\end{exa}


If $B=\{b_1, \dots, b_n\}$ is a basis of a vector space $V$ of dimension $n$, $v\in V$, the {\bf coordinate} of $v$ under $B$ is a vector $x\in\mathbb{R}^n$ such that $v=\sum_ix_ib_i$ where $x_i$ is the $i$-th entry of $x$.\\

A map between two vector spaces $T: V\rightarrow W$ is called a {\bf linear transformation}, if
\begin{itemize}
\item $T(x+y)=T(x)+T(y)$
\item $T(cx)=cT(x)$
\end{itemize}

If $T: V\rightarrow W$ is a linear transformation between two linear spaces, $x$ is the coordinate of $v$ in basis $B$, $y$ is the coordinate of $T(v)$ under basis $C$, then $y=Ax$ where $A=[a_{ij}]$, and $T(b_j)=\sum_ia_{ij}c_i$.\\

The {\bf inner product} on $\mathbb{R}^n$ is defined as $(x, y)=x^Ty=\sum_ix_iy_i$. It is easy to check that this inner product satisfies the following properties:
\begin{enumerate}
\item Symmetry: $(x, y)=(y, x)$
\item Bilinearity: $(ax+a'x', y)=a(x, y)+a'(x', y)$, $(x, by+b'y')=b(x, y)+b'(x, y')$.
\item Positive definiteness: $(x, x)\geq 0$ and $(x, x)=0$ iff $x=0$.
\end{enumerate}

Two vectors are {\bf orthogonal} to each other iff their inner product is $0$. A set of vectors is orthogonal to another set if every vector in the first set is orthogonal to every vector in the second.

Let $V$ be a subspace of $\mathbb{R}^n$. We call a basis of $V$ {\bf orthogonal} if the inner product of distinct basis vectors are all $0$, {\bf orthonomal} if in addition, the inner product of any basis vector with itself is $1$.\\

Given any basis $\{x_1, \dots, x_d\}$ of a subspace $V\subset \mathbb{R}^n$, we can make it into an orthogonal or orthonormal basis via the {\bf Gram-Schmidt process}:
\[y_1=x_1\]
\[y_i=x_i-\sum_{j<i}((y_j, x_i)/(y_j, y_j))y_j\]
Then $\{y_i\}$ is an orthogonal basis, and $\{(y_i, y_i)^{-1/2}y_i\}$ is an orthonormal basis.\\

If $V$ is a subspace of $\mathbb{R}^n$, $x\in \mathbb{R}^n$, we call the {\bf orthogonal projection} of $x$ on $V$, denoted as $P_V(x)$, the unique vector that satisfies $P_V(x)\in V$ and $(x-P_V(x), y)=0$ for all $y\in V$.\\

For any $x'\in V$, $(x-P_V(x), x-P_V(x))\leq (x-x', x-x')$ and equality happens iff $x'=P_V(x)$.\\

To calculate $P_V(x)$, we can use either of these formulas:

\begin{enumerate}
\item If $\{x_i\}$ is an orthonormal basis of $V$, then $P_V(x)=\sum_i(x, x_i)x_i$.
\item If $\{x_i\}$ is an orthogonal basis of $V$, then $P_V(x)=\sum_i((x, x_i)/(x_i, x_i))x_i$.
\item If $\{x_i\}$ is just a basis of $V$, let $X=[x_1, \dots x_d]$ be a $n\times d$ matrix, then
  \[P_V(x)=X(X^TX)^{-1}X^Tx=\sum_i(\sum_j a_{ij}(x_j, x)) x_i\]
  Where $A=[a_{ij}]=[(x_i, x_j)]^{-1}$ is a $d\times d$ matrix. 
\end{enumerate}

If one replace $(x, y)$ with $(x, y)_A$ defined as $x^TAy$, where $A$ is a symmetric matrix with all eigenvalues positive, then $(\cdot, \cdot)_A$ still satisfies symmetry, bilinearity and positive definiteness, and all the conclusions about $(\cdot, \cdot)$ above are still valid.\\

Furthermore, if $V$ is any vector space and $(\cdot, \cdot)$ is a $\mathbb{R}$-valued function on $V\times V$ which is symmetric, bilinear and positive definite, all the conclusions above are valid as well.

\subsection{Notes on ODE}

This is a review on some basics of the theory of ordinary differential equations.

An {\bf ordinary differential equation} is an equation relating a function on $\mathbb{R}$ and its derivatives. For example, the followings are ordinary differential equations:

\[y'=t\sin(y)\]
\[y'=sin(t)\]
\[y'=y\cos(t)+e^t\]

We can also have systems of equations like the following
\[y_1'=y_2, y_2'=-y_1\]


The {\bf initial value problem} of an ordinary differential equation means finding a solution after specifying the value of the solution at some time $t_0$, which, for convenience, we can choose to be $0$. For example
\[y'=y\cos(t)+e^t, y(0)=0\]

In general the solution of an ODE can not be written down explicitly, however, in some situations we can get explicit solutions. For example, if the equation is of the form $y'=f(t)g(y)$, then the general solution is of the form
\[\int_0^y {ds\over g(s)}=\int_0^t f(s)ds+C\]
This is called {\bf separation of variables}.

The most important result in the theory of ODE is Picard's theorem:

\begin{thm}
If $f(t, y)$ is continuous and Lipschitz in the second parameter with Lipschitz constant $L$, then $y'=f(t, y)$, $y(0)=a$ always has a unique solution.
\end{thm}

\begin{proof}
 Firstly let's show uniqueness: if $y_1$ and $y_2$ are two solutions, then $|y_1(t)-y_2(t)|e^{-L|t|}$ is non increasing when $t>0$ and non decreasing when $t<0$, hence must always be $0$.\\
  
  Now let's show existence: consider a sequence of functions defined as below:
  \[y_0(t)=a\]
  \[y_i(t)=a+\int_0^t f(s, y_{i-1}(s))ds\]
  Then $f$ being Lipschitz implies that
  \[|y_i(t)-y_{i-1}(t)|\leq \int_0^t L|y_{i-1}(s)-y_{i-2}(s)|ds\]
  \[\leq \int_0^t L^2(t-s)|y_{i-2}(s)-y_{i-3}(s)|ds\]
  \[\leq \int_0^t L^3{(t-s)^2\over 2}|y_{i-3}(s)-y_{i-4}(s)|ds\]
  \[\leq \dots \leq \int_0^t L^{i-1}{(t-s)^{i-2}\over (i-2)!}|y_1(s)-y_0(s)|ds\]
  \[\leq {\max(|y_1-y_0|)L^{i-1}t^{i-1}\over (i-1)!}\]

  Hence the sequence converges uniformly on any finite interval, and fundamental theorem of calculus implies that the limiting function $y$ is the solution.
\end{proof}

The argument above can be used to show that if $f$ is real analytic (i.e. has Taylor series convergent to itself), so is $y$.

\subsection{Notes on Linear difference and differential equations and Taylor's theorem}

\subsubsection{Linear Difference Equations}

A homogeneous linear difference equation is an iterative relationship:
\[z_{n+k}+a_{k-1}z_{n+k-1}+\dots+a_0z_n=0\]

The general solution of a homogeneous linear difference equation can be obtained as follows:
\begin{itemize}
\item Firstly, define the characteristic polynomial $\chi(z)=z^k+a_{k-1}z^{k-1}+\dots+a_0$.
\item Let $\lambda_1, \dots, \lambda_l$ be its distinct roots, $m_1, \dots, m_l$ their multiplicities (hence $\sum_im_i=k$).
\item Then, the general solution can be written as

\[z_n=\sum_ip_i(n)\lambda_i^n\]

Where $p_i(n)$ is any polynomial of degree no more than $m_i-1$.
\end{itemize}

\subsubsection{Linear Differential Equations}

Similarly, a homogeneous linear differential equation is
\[y^{(k)}+a_{k-1}y^{(k-1)}+\dots+a_1y'+a_0y=0\]

The general solution of a homogeneous linear differential equation is as follows:
\begin{itemize}
\item Firstly, define the characteristic polynomial $\chi(z)=z^k+a_{k-1}z^{k-1}+\dots+a_0$.
\item Let $\lambda_1, \dots, \lambda_l$ be its distinct roots, $m_1, \dots, m_l$ their multiplicities (hence $\sum_im_i=k$).
\item Then, the general solution can be written as

\[y(t)=\sum_ip_i(t)e^{\lambda_i t}\]

Where $p_i(n)$ is any polynomial of degree no more than $m_i-1$.
\end{itemize}


\subsubsection{Taylor Series}

If $f\in C^{k+1}$, then the Taylor series of $f$ at $a$, with Lagrange remainder, is

\[f(x)=f(a)+\sum_{j=1}^{k}{f^{(j)}(a)(x-a)^j\over j!}+{f^{(j+1)}(c)(x-a)^{j+1}\over(j+1)!}\] 

Where $c$ is in the closed interval between $x$ and $a$.


\end{document}
