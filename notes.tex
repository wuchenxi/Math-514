\documentclass[20pt]{article} % USenglish for autoref
\usepackage[paper=screen, centering, papersize=20cm]{geometry}
\usepackage{cmap}		% to search and copy ligatures
\usepackage[utf8]{inputenc}	% for Linux computer and Mac
%\usepackage[latin1]{inputenc}	% fÃ¼r Windows computer
\usepackage[T1]{fontenc}	% to search for ligatures in the pdf
\usepackage[USenglish]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[]{algorithm2e}
%\usepackage{stmaryrd}		% for \mapsfrom
\usepackage{aliascnt}		% for Aliascounter so that autoref gives the thms the right names
\usepackage[pdfborder={0 0 0}]{hyperref}   % if you want to have math environment in captions you have to use \texorpdfstring{$x^2$}{x2}; without frame around the links
\usepackage[figure]{hypcap}		% to make autoref link to figures and not the captions of figures
%\usepackage{paralist}		% for compactitem
\usepackage{mathtools}		% for \coloneqq
\usepackage[]{todonotes}	% use option disable to disable all the todos
\usepackage{float}
\usepackage{lipsum}
\theoremstyle{break}
\newtheorem{definition}{Definition}[section]  
\newtheorem{exa}[definition]{Example}
\newtheorem{cor}[definition]{Corollary}
\newtheorem{lem}[definition]{Lemma}
\newtheorem{conj}[definition]{Conjecture}
\newtheorem{quest}[definition]{Research question}
\newtheorem{thm}[definition]{Theorem}  
\newtheorem{prop}[definition]{Proposition}
\newtheorem{rem}[definition]{Remark}


\renewcommand{\labelenumi}{(\roman{enumi})} % roman numbers in enumerations

\usepackage{graphicx,tikz,pgfplots} % Allows including images

\usetikzlibrary{calc,decorations.markings}
\usetikzlibrary{shapes,snakes}

\DeclareMathOperator{\Aff}{Aff}
\DeclareMathOperator{\der}{der}
\DeclareMathOperator{\Trans}{Trans}
\DeclareMathOperator{\dir}{dir}
\DeclareMathOperator{\Sing}{Sing}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\NN}{\mathbb{N}}


%For L Cal and L Twidle:
\newcommand{\LC}{\mathcal{L}} 
\newcommand{\LT}{\widetilde{\mathcal{L}}}

%For X bar
\newcommand{\XB}{\overline{X}}

%For the real and complex numbers
\newcommand{\RR}{\mathbb{R}} 
\newcommand{\CC}{\mathbb{C}} 


\newcommand{\remark}[2][]{\todo[color=green!50, #1]{#2}}

\title{Math 514}


\begin{document}
\maketitle
  \begin{itemize}
  \item Instructor: Chenxi Wu cwu367@wisc.edu
 \item Section 1: 9:55-10:45 am Section 2: 12:05-12:55 pm  
  \item Office hours: 10:45am-noon Monday, Wednesday or by appointment
  \end{itemize}

  \newpage
  
  Recall that in the first half of the semester, we covered the following topics:
  \begin{enumerate}
  \item Methods for root finding: Newton's method etc.
  \item Numerical Linear Algebra: LU decomposition, QR algorithm etc.
  \end{enumerate}
  
  The following are the new topics we will cover in the second half of the semester:
  \begin{enumerate}
  \item Interpolation and approximation: how to get the formula of a function using discrete date.
  \item Numerical integration: how to integrate a function knowing only its value on a discrete set. 
  \item Numerical solution for differential equations: numerical solution for ODE and PDE.
  \end{enumerate}
  The first topic will be the foundation of the second and third topic.

  \newpage
\tableofcontents

\newpage
  
  \section{Polynomial interpolation (Chapter 6)}
  
  \begin{definition}
   The {\bf polynomial interpolation} of a function $f$ at points $x_0, \dots x_n$, is a polynomial $p$ that shares some properties of $f$ at those points, e.g. has the same value or same derivatives.
  \end{definition}

  We will focus on single variable functions, and discuss two kinds of polynomial interpolation problems:
  \begin{enumerate}
  \item Lagrange interpolation: find a polynomial $p$ of degree at most $n$, such that $p(x_i)=f(x_i)$.
  \item Hermite interpolation: find a polynomial $p$ of degree at most $2n+1$, such that $p(x_i)=f(x_i)$, $p'(x_i)=f'(x_i)$.
  \end{enumerate}

  An application for Hermite interpolation is for approximating smooth curves where the direction of the curve at certain points are also specified.

  \newpage
  
  \subsection{Lagrange interpolation}
  \subsubsection{Existence}
  \begin{thm}\label{ext_lag} (Lemma 6.1 in textbook) The Lagrange interpolation polynomial exists. In other words, for any $n+1$ distinct real numbers $x_0,\dots, x_n$, and $n+1$ real numbers $y_0, \dots, y_n$, there is a polynomial $p$ of degree at most $n$ such that $p(x_i)=y_i$.
  \end{thm}
  How do we find such a $p$?\\
  
  Firstly, we observe that the map
  \[T: p\mapsto [p(x_0), \dots p(x_n)]^T\in \mathbb{R}^{n+1}\]
  is linear. In other words, $(cp+dq)(x_i)=cp(x_i)+dq(x_i)$ for all $i$.
  Hence, finding $p$ is like solving a system of non homogenous linear equations. Recall from linear algebra, let $e_i$ be the standard basis vector of $\mathbb{R}^{n+1}$ corresponding to $y_i=1$ and $y_j=0$ for all $j\not=i$, then, if we can find some $p_i$ of degree at most $n$ such that $T(p_i)=e_i$, (i.e. $p_i(x_j)=\begin{cases} 1 & i=j\\ 0 &i\not=j\end{cases}$)  then
  \[T(\sum_iy_ip_i)=\sum_iy_ie_i=[y_0, \dots y_n]^T\ .\]

  \newpage
  
  Now we try and find the $p_i$: $p_i(x_j)=0$ for all $j\not=i$, so $(x-x_j)$ must be a factor of $p_i$. So $p_i$ must be something times
  \[\prod_{j\not=i}(x-x_j)\ .\]

  On the other hand, $p_i(x_i)=1$, so the ``something'' should be
  \[{1\over\prod_{j\not=i}(x_i-x_j)}\ .\]

  Now we have a proof of Theorem 1:

  \begin{proof}
    Let
    \[p(x)=\sum_i\left( y_i\cdot {\prod_{j\not=i}(x-x_j)\over \prod_{j\not=i}(x_i-x_j)}\right)\]
    Then $p(x_k)=\sum_i\left( y_i\cdot {\prod_{j\not=i}(x_k-x_j)\over \prod_{j\not=i}(x_i-x_j)}\right)y_i$. The $k$-th term is $y_k\times 1$ while all other terms are zero, hence the answer is $y_k$.
  \end{proof}

  \newpage

  \subsubsection{Uniqueness}
  
  The problem of finding Lagrange interpolation polynomial is one with $n+1$ conditions and $n+1$ unknowns, so intuitively there should be a discrete set of solutions. Actually the solution can be shown to be unique:

  \begin{thm}\label{uniq_lag} (Theorem 6.1 in textbook)
  The Lagrange interpolation polynomial is unique. In other words, given $x_0, \dots, x_n$ and $y_0, \dots, y_n$ with $x_i$ distinct, there is a single polynomial $p$ of degree at most $n$ such that $p(x_i)=y_i$
  \end{thm}

  \begin{proof}
    The first step of the proof is to reduce the problem to the case where $y_i=0$. Suppose $p$ and $q$ are two such polynomials, then $(p-q)(x_i)=0$ for all $i$. So, to prove the theorem, we only need to show that if a polynomial $r=p-q$ of degree at most $n$ vanishes at $n+1$ distinct points, then $r=0$. This fact follows from the ``fundamental theorem of algebra'' and can be proved using long division. Here we provide two other alternative proofs:

    \newpage
    \begin{enumerate}
    \item Approach I: use the mean value theorem in calculus. Firstly we show the following fact:
      \begin{lem}\label{key_lem} If $f \in C^{m-1}$ ($f$ is $m-1$-th order differentiable with $m-1$-th derivative continuous), and $f=0$ at $m$ distinct points, then for any $0\leq k\leq m-1$, $f^{(k)}=0$ at at least $m-k$ points.
      \end{lem}
      \begin{proof}
       By mean value theorem, between two consecutive zeros of $f$ there must be a zero of $f'$. Hence $f'$ vanishes at at least $m-1$ points. Now let $f'$ take the role of $f$ and continue the process, we get $f''$ vanishes at at least $m-2$ points, etc. 
      \end{proof}
      Suppose $r$ vanishes at $x_0, \dots, x_n$, and $r$ is of degree $d>0$. Then $r^{(d)}$ is a non zero constant. Apply Lemma~\ref{key_lem} with $m=n+1$ and $k=d$, we see a contradiction. Hence $r=const$, which implies $r=0$.
      \newpage
  \item Approach II: Let $r=\sum_ja_jx^j$, then $a_j$ are solutions of a system of linear equation $\sum_ja_jx_i^j=0$. However from linear algebra, 
  \[\left|\begin{array}{cccc}1 & 1& \dots & 1\\ x_1 & x_2 &\dots &x_{n+1}\\ \dots & \dots & \dots & \dots \\ x_1^n & x_2^n & \dots & x_{n+1}^n\end{array}\right|=\prod_{i<j}(x_j-x_i)\not=0\]
Hence $a_j=0$ for all $j$, which implies that $r=0$.  
    \end{enumerate}
  \end{proof}

  \newpage

  \subsubsection{Error Estimate}
  
  We know that $f^{(n+1)}=0$ iff $f$ is a polynomial of degree at most $n$, so one may guess that if $f^{(n+1)}$ is small, $f$ should be close to a polynomial of degree at most $n$, hence probably close to its Lagrange interpolation polynomial at $n+1$ points. To make this more precise, we have the following theorem on error estimate of Lagrange interpolation:
\begin{thm}\label{err_lag} (Theorem 6.2 in textbook) If $f\in C^{n+1}$, $p$ is the Lagrange interpolation of $f$ at $n+1$ distinct points $x_0,\dots x_n$. Then for any $x$, there is some $s\in[\min\{x_i, x\}, \max\{x_i, x\}]$, such that
  \[f(x)-p(x)={f^{(n+1)}(s)\prod_i(x-x_i)\over (n+1)!}\]
\end{thm}

\newpage

 \begin{proof} When $x=x_i$ it's obvious. Now suppose $x$ is distinct from all $x_i$. Consider the auxiliary function:
  \[G(t)=f(t)-p(t)-(f(x)-p(x))\cdot{\prod_i(t-x_i)\over \prod_i(x-x_i)}\]
  Then $G=0$ at $x_i$ and $x$, hence by Lemma~\ref{key_lem} (let $m=n+2$, $k=n+1$), there must be some point $s\in[\min\{x_i, x\}, \max\{x_i, x\}]$ where
  \[G^{(n+1)}(s)=f^{(n+1)}(s)-{(f(x)-p(x))(n+1)!\over \prod_i(x-x_i)}=0\]
  Hence
  \[f(x)-p(x)={f^{(n+1)}(s)\prod_i(x-x_i)\over (n+1)!}\]
\end{proof}

\newpage

When the set $\{x_i\}$ becomes denser, $\prod_i(x-x_i)$ decreases, and $(n+1)!$ increases. However, when $n\rightarrow \infty$, the Lagrange interpolation polynomial may not converge to $f$ even if $f$ is smooth, if $f^{(n)}$ increases too fast.

\begin{exa}\label{exa1}$f(x)=\cos(x)$, $x_i=5i/n$, $i=0, 1, 2, \dots n$\\
\begin{figure}[H]
\begin{tikzpicture}[yscale=2, xscale=2]
  \draw[->] (0, 0) -- (5, 0) node[right] {$x$};
  \draw[->] (0, -1) -- (0, 1) node[above] {$y$};
  \draw[scale=1, domain=0:5, smooth, variable=\x, black, very thick, dashed] plot ({\x}, {cos(deg(\x))});
  \draw[scale=1, domain=0:5, smooth, variable=\x, red] plot ({\x}, {cos(deg(0))*(\x-5)/(0-5)+cos(deg(5))*(\x-0)/(5-0)});
  \draw[scale=1, domain=0:5, smooth, variable=\x, green] plot ({\x}, {cos(deg(0))*(\x-5)*(\x-2.5)/(0-5)/(0-2.5)-cos(deg(2.5))*(\x-0)*(\x-5)/2.5/2.5+cos(deg(5))*(\x-0)*(\x-2.5)/(5-0)/2.5});
  \draw[scale=1, domain=0:5, smooth, variable=\x, blue] plot({\x}, {cos(deg(0.0))*(\x-0.5)/(0.0-0.5)*(\x-1.0)/(0.0-1.0)*(\x-1.5)/(0.0-1.5)*(\x-2.0)/(0.0-2.0)*(\x-2.5)/(0.0-2.5)*(\x-3.0)/(0.0-3.0)*(\x-3.5)/(0.0-3.5)*(\x-4.0)/(0.0-4.0)*(\x-4.5)/(0.0-4.5)*(\x-5.0)/(0.0-5.0)+cos(deg(0.5))*(\x-0.0)/(0.5-0.0)*(\x-1.0)/(0.5-1.0)*(\x-1.5)/(0.5-1.5)*(\x-2.0)/(0.5-2.0)*(\x-2.5)/(0.5-2.5)*(\x-3.0)/(0.5-3.0)*(\x-3.5)/(0.5-3.5)*(\x-4.0)/(0.5-4.0)*(\x-4.5)/(0.5-4.5)*(\x-5.0)/(0.5-5.0)+cos(deg(1.0))*(\x-0.0)/(1.0-0.0)*(\x-0.5)/(1.0-0.5)*(\x-1.5)/(1.0-1.5)*(\x-2.0)/(1.0-2.0)*(\x-2.5)/(1.0-2.5)*(\x-3.0)/(1.0-3.0)*(\x-3.5)/(1.0-3.5)*(\x-4.0)/(1.0-4.0)*(\x-4.5)/(1.0-4.5)*(\x-5.0)/(1.0-5.0)+cos(deg(1.5))*(\x-0.0)/(1.5-0.0)*(\x-0.5)/(1.5-0.5)*(\x-1.0)/(1.5-1.0)*(\x-2.0)/(1.5-2.0)*(\x-2.5)/(1.5-2.5)*(\x-3.0)/(1.5-3.0)*(\x-3.5)/(1.5-3.5)*(\x-4.0)/(1.5-4.0)*(\x-4.5)/(1.5-4.5)*(\x-5.0)/(1.5-5.0)+cos(deg(2.0))*(\x-0.0)/(2.0-0.0)*(\x-0.5)/(2.0-0.5)*(\x-1.0)/(2.0-1.0)*(\x-1.5)/(2.0-1.5)*(\x-2.5)/(2.0-2.5)*(\x-3.0)/(2.0-3.0)*(\x-3.5)/(2.0-3.5)*(\x-4.0)/(2.0-4.0)*(\x-4.5)/(2.0-4.5)*(\x-5.0)/(2.0-5.0)+cos(deg(2.5))*(\x-0.0)/(2.5-0.0)*(\x-0.5)/(2.5-0.5)*(\x-1.0)/(2.5-1.0)*(\x-1.5)/(2.5-1.5)*(\x-2.0)/(2.5-2.0)*(\x-3.0)/(2.5-3.0)*(\x-3.5)/(2.5-3.5)*(\x-4.0)/(2.5-4.0)*(\x-4.5)/(2.5-4.5)*(\x-5.0)/(2.5-5.0)+cos(deg(3.0))*(\x-0.0)/(3.0-0.0)*(\x-0.5)/(3.0-0.5)*(\x-1.0)/(3.0-1.0)*(\x-1.5)/(3.0-1.5)*(\x-2.0)/(3.0-2.0)*(\x-2.5)/(3.0-2.5)*(\x-3.5)/(3.0-3.5)*(\x-4.0)/(3.0-4.0)*(\x-4.5)/(3.0-4.5)*(\x-5.0)/(3.0-5.0)+cos(deg(3.5))*(\x-0.0)/(3.5-0.0)*(\x-0.5)/(3.5-0.5)*(\x-1.0)/(3.5-1.0)*(\x-1.5)/(3.5-1.5)*(\x-2.0)/(3.5-2.0)*(\x-2.5)/(3.5-2.5)*(\x-3.0)/(3.5-3.0)*(\x-4.0)/(3.5-4.0)*(\x-4.5)/(3.5-4.5)*(\x-5.0)/(3.5-5.0)+cos(deg(4.0))*(\x-0.0)/(4.0-0.0)*(\x-0.5)/(4.0-0.5)*(\x-1.0)/(4.0-1.0)*(\x-1.5)/(4.0-1.5)*(\x-2.0)/(4.0-2.0)*(\x-2.5)/(4.0-2.5)*(\x-3.0)/(4.0-3.0)*(\x-3.5)/(4.0-3.5)*(\x-4.5)/(4.0-4.5)*(\x-5.0)/(4.0-5.0)+cos(deg(4.5))*(\x-0.0)/(4.5-0.0)*(\x-0.5)/(4.5-0.5)*(\x-1.0)/(4.5-1.0)*(\x-1.5)/(4.5-1.5)*(\x-2.0)/(4.5-2.0)*(\x-2.5)/(4.5-2.5)*(\x-3.0)/(4.5-3.0)*(\x-3.5)/(4.5-3.5)*(\x-4.0)/(4.5-4.0)*(\x-5.0)/(4.5-5.0)+cos(deg(5.0))*(\x-0.0)/(5.0-0.0)*(\x-0.5)/(5.0-0.5)*(\x-1.0)/(5.0-1.0)*(\x-1.5)/(5.0-1.5)*(\x-2.0)/(5.0-2.0)*(\x-2.5)/(5.0-2.5)*(\x-3.0)/(5.0-3.0)*(\x-3.5)/(5.0-3.5)*(\x-4.0)/(5.0-4.0)*(\x-4.5)/(5.0-4.5)});
\end{tikzpicture}
\caption{Black dashed line: $y=\cos(x)$. Red line: Lagrange interpolation with 2 points. Green line: Lagrange interpolation with 3 points.
Blue line: Lagrange interpolation with 11 points.}
\end{figure}
\end{exa}

\newpage

\begin{exa}\label{exa2} $f(x)=1/(1+2(x-2)^2)$, $x_i=5i/n$,  $i=0, 1, 2, \dots n$.
  \begin{figure}[H]
\begin{tikzpicture}[yscale=2, xscale=2]
  \draw[->] (0, 0) -- (5, 0) node[right] {$x$};
  \draw[->] (0, -1) -- (0, 1.3) node[above] {$y$};
  \draw[scale=1, domain=0:5, smooth, variable=\x, black, very thick, dashed] plot ({\x}, {1/(1+2*(\x-2)*(\x-2))});
  \draw[scale=1, domain=0:5, smooth, variable=\x, red] plot ({\x}, {1/9*(\x-5)/(0-5)+1/19*(\x-0)/(5-0)});
  \draw[scale=1, domain=0:5, smooth, variable=\x, green] plot ({\x}, {1/9*(\x-2.5)*(\x-5)/(0-2.5)/(0-5)+1/1.5*(\x-0)*(\x-5)/(2.5-0)/(2.5-5)+1/19*(\x-0)*(\x-2.5)/5/2.5});
  
  \draw[scale=1, domain=0:5, smooth, variable=\x, blue] plot ({\x}, {1/(1+2*(0.0-2)*(0.0-2))*(\x-0.5)/(0.0-0.5)*(\x-1.0)/(0.0-1.0)*(\x-1.5)/(0.0-1.5)*(\x-2.0)/(0.0-2.0)*(\x-2.5)/(0.0-2.5)*(\x-3.0)/(0.0-3.0)*(\x-3.5)/(0.0-3.5)*(\x-4.0)/(0.0-4.0)*(\x-4.5)/(0.0-4.5)*(\x-5.0)/(0.0-5.0)+1/(1+2*(0.5-2)*(0.5-2))*(\x-0.0)/(0.5-0.0)*(\x-1.0)/(0.5-1.0)*(\x-1.5)/(0.5-1.5)*(\x-2.0)/(0.5-2.0)*(\x-2.5)/(0.5-2.5)*(\x-3.0)/(0.5-3.0)*(\x-3.5)/(0.5-3.5)*(\x-4.0)/(0.5-4.0)*(\x-4.5)/(0.5-4.5)*(\x-5.0)/(0.5-5.0)+1/(1+2*(1.0-2)*(1.0-2))*(\x-0.0)/(1.0-0.0)*(\x-0.5)/(1.0-0.5)*(\x-1.5)/(1.0-1.5)*(\x-2.0)/(1.0-2.0)*(\x-2.5)/(1.0-2.5)*(\x-3.0)/(1.0-3.0)*(\x-3.5)/(1.0-3.5)*(\x-4.0)/(1.0-4.0)*(\x-4.5)/(1.0-4.5)*(\x-5.0)/(1.0-5.0)+1/(1+2*(1.5-2)*(1.5-2))*(\x-0.0)/(1.5-0.0)*(\x-0.5)/(1.5-0.5)*(\x-1.0)/(1.5-1.0)*(\x-2.0)/(1.5-2.0)*(\x-2.5)/(1.5-2.5)*(\x-3.0)/(1.5-3.0)*(\x-3.5)/(1.5-3.5)*(\x-4.0)/(1.5-4.0)*(\x-4.5)/(1.5-4.5)*(\x-5.0)/(1.5-5.0)+1/(1+2*(2.0-2)*(2.0-2))*(\x-0.0)/(2.0-0.0)*(\x-0.5)/(2.0-0.5)*(\x-1.0)/(2.0-1.0)*(\x-1.5)/(2.0-1.5)*(\x-2.5)/(2.0-2.5)*(\x-3.0)/(2.0-3.0)*(\x-3.5)/(2.0-3.5)*(\x-4.0)/(2.0-4.0)*(\x-4.5)/(2.0-4.5)*(\x-5.0)/(2.0-5.0)+1/(1+2*(2.5-2)*(2.5-2))*(\x-0.0)/(2.5-0.0)*(\x-0.5)/(2.5-0.5)*(\x-1.0)/(2.5-1.0)*(\x-1.5)/(2.5-1.5)*(\x-2.0)/(2.5-2.0)*(\x-3.0)/(2.5-3.0)*(\x-3.5)/(2.5-3.5)*(\x-4.0)/(2.5-4.0)*(\x-4.5)/(2.5-4.5)*(\x-5.0)/(2.5-5.0)+1/(1+2*(3.0-2)*(3.0-2))*(\x-0.0)/(3.0-0.0)*(\x-0.5)/(3.0-0.5)*(\x-1.0)/(3.0-1.0)*(\x-1.5)/(3.0-1.5)*(\x-2.0)/(3.0-2.0)*(\x-2.5)/(3.0-2.5)*(\x-3.5)/(3.0-3.5)*(\x-4.0)/(3.0-4.0)*(\x-4.5)/(3.0-4.5)*(\x-5.0)/(3.0-5.0)+1/(1+2*(3.5-2)*(3.5-2))*(\x-0.0)/(3.5-0.0)*(\x-0.5)/(3.5-0.5)*(\x-1.0)/(3.5-1.0)*(\x-1.5)/(3.5-1.5)*(\x-2.0)/(3.5-2.0)*(\x-2.5)/(3.5-2.5)*(\x-3.0)/(3.5-3.0)*(\x-4.0)/(3.5-4.0)*(\x-4.5)/(3.5-4.5)*(\x-5.0)/(3.5-5.0)+1/(1+2*(4.0-2)*(4.0-2))*(\x-0.0)/(4.0-0.0)*(\x-0.5)/(4.0-0.5)*(\x-1.0)/(4.0-1.0)*(\x-1.5)/(4.0-1.5)*(\x-2.0)/(4.0-2.0)*(\x-2.5)/(4.0-2.5)*(\x-3.0)/(4.0-3.0)*(\x-3.5)/(4.0-3.5)*(\x-4.5)/(4.0-4.5)*(\x-5.0)/(4.0-5.0)+1/(1+2*(4.5-2)*(4.5-2))*(\x-0.0)/(4.5-0.0)*(\x-0.5)/(4.5-0.5)*(\x-1.0)/(4.5-1.0)*(\x-1.5)/(4.5-1.5)*(\x-2.0)/(4.5-2.0)*(\x-2.5)/(4.5-2.5)*(\x-3.0)/(4.5-3.0)*(\x-3.5)/(4.5-3.5)*(\x-4.0)/(4.5-4.0)*(\x-5.0)/(4.5-5.0)+1/(1+2*(5.0-2)*(5.0-2))*(\x-0.0)/(5.0-0.0)*(\x-0.5)/(5.0-0.5)*(\x-1.0)/(5.0-1.0)*(\x-1.5)/(5.0-1.5)*(\x-2.0)/(5.0-2.0)*(\x-2.5)/(5.0-2.5)*(\x-3.0)/(5.0-3.0)*(\x-3.5)/(5.0-3.5)*(\x-4.0)/(5.0-4.0)*(\x-4.5)/(5.0-4.5)});
\end{tikzpicture}
\caption{Black dashed line: $y=1/(1+2(x-2)^2)$. Red line: Lagrange interpolation with 2 points. Green line: Lagrange interpolation with 3 points. Blue line: Lagrange interpolation with 11 points.}
\end{figure}
\end{exa}

The reason that the Lagrange interpolation polynomials in Example \ref{exa1} converges but those in Example \ref{exa2} don't, is that the higher order derivatives of $\cos$ is $\pm \sin$, $\pm \cos$ hence all bounded, while it is not true for the function in Example \ref{exa2}. As a practice, calculate the $k$-th derivative of $1/(1+2(x-2)^2)$ at $x=2$.

\newpage

\subsection{Hermite interpolation polynomial}

\subsubsection{Existence}

Similar to the Lagrange case, we can construct the Hermite interpolation polynomial as follows:
\begin{thm}\label{ext_her} (Existence part of Theorem 6.3 in textbook) There is a polynomial $p$ of degree at most $2n+1$, such that $p(x_i)=y_i$, $p'(x_i)=z_i$, $i=0, \dots n$, where $x_i$ are distinct.
\end{thm}
Use the same strategy as the Lagrange case, possibly via a few trials and errors, one can find the formula of $p$ as below:
\begin{proof}
  Let
  \[p(x)=\sum_i\left( z_i\cdot {(x-x_i)\prod_{j\not=i}(x-x_j)^2\over \prod_{j\not=i}(x_i-x_j)^2}\right. \]
  \[\left. +y_i\cdot \left(1-(x-x_i)\sum_{j\not=i}{2\over x_i-x_j}\right)\cdot {\prod_{j\not=i}(x-x_j)^2\over \prod_{j\not=i}(x_i-x_j)^2} \right)\]
  Then by calculation,
  \[p(x_k)=\sum_i\left( z_i\cdot {(x_k-x_i)\prod_{j\not=i}(x_k-x_j)^2\over \prod_{j\not=i}(x_i-x_j)^2}\right. \]
  \[\left. +y_i\cdot \left(1-(x_k-x_i)\sum_{j\not=i}{2\over x_i-x_j}\right)\cdot {\prod_{j\not=i}(x_k-x_j)^2\over \prod_{j\not=i}(x_i-x_j)^2} \right)\]
  In the first sum, all terms have a factor $(x_k-x_k)$, so it must be zero. In the second sum, all but the $k$-th term is zero, and the $k$-th term is $y_k$.
  Similarly, by taking derivative and let $x=x_k$, we can show that $p'(x_k)=z_k$.
  \end{proof}

\newpage
  
\subsubsection{Uniqueness and Error Estimate}
  
  The mean value theorem argument (i.e. Lemma~\ref{key_lem}) can also be used to show the uniqueness and error estimate for Hermite interpolation polynomials:

  \begin{thm}\label{uniq_her} (Uniqueness part of Theorem 6.3 in textbook)
 The Hermite interpolation polynomial is unique. In other words, there is a unique $p$ of degree at most $2n+1$ such that $p(x_i)=y_i$, $p'(x_i)=z_i$, $i=0, \dots n$, where $x_i$ are distinct.
\end{thm}
\begin{proof} Similar to the proof of Theorem~\ref{uniq_lag}, if we have two Hermite interpolation polynomials $p$ and $q$, then $r=p-q$ satisfies $r(x_i)=r'(x_i)=0$ and $r$ has degree at most $2n+1$. However, if $r$ is non zero, it can not have $n+1$ distincts roots $x_i$ with multiplicity at least $2$ each, hence $r=0$.\\

  We can also prove $r=0$ using analysis like in Theorem~\ref{uniq_lag}. If $r$ has degree at most $2n+1$, $r'=r=0$ at $n+1$ points, then there must be $n$ other points where $r'=0$. Now suppose $r$ has degree $d>0$. Apply Lemma~\ref{key_lem}, let $m=2n+2$, $k=d$, then we get $r^{(d)}$ vanishes at $2n+2-d$ points, which contradicts with the fact that $r^{(d)}$ is a non zero constant. Hence $r=const$ which implies that $r=0$.
\end{proof}

\newpage

\begin{thm}\label{err_her} (Theorem 6.4 in textbook) If $f\in C^{(2n+2)}$, there is $s\in [\min\{x_i, x\}, \max\{x_i, x\}]$, such that
  \[f(x)-p(x)={f^{(2n+2)}(s)\prod_i(x-x_i)^2\over (2n+2)!}\]
\end{thm}

\begin{proof}
  If $x=x_i$ then it is trivially true. Now assume $x$ is not in $\{x_i\}$. Let
  \[G(t)=f(t)-p(t)-{(f(x)-p(x))\prod_i(t-x_i)^2\over \prod_i(x-x_i)^2}\]
  Then $G$ vanishes at the $n+2$ points $x, x_0, \dots x_n$, and $G'$ vanishes at $n+1$ of them $x_0, \dots, x_n$. By the same argument as above, $G'$ vanishes at $n+1$ more points, hence it is zero at at least $2n+2$ points. Now use Lemma~\ref{key_lem} on $G'$ for $m=2n+2$, $k=2n+1$.
\end{proof}

\newpage

\subsection{Applications}
\subsubsection{Numerical Differentiation}
Suppose $p$ is the Lagrange interpolation of $f$ at $n+1$ points. By mean value theorem, $f'-p'$ is zero at $n$ points $d_1, \dots d_n$, so $p'$ can be seen as the Lagrange interpolation polynomial with condition $p'(d_i)=f'(d_i)$ (see Theorem 6.5 in textbook). Now one can get an estimate for $f'(x)-p'(x)$ using Theorem~\ref{err_lag}.\\

\begin{exa}
  For example, if we know the value of $f$ at $x+ih$ for $i=-1, 0, 1$ as $y_{-1}$, $y_0$ and $y_1$, then the Lagrange interpolation polynomial is:
  \[p(x+t)=y_{-1}t(t-h)/(2h^2)-y_0(t+h)(t-h)/h^2+y_1t(t+h)/(2h^2)\]
  So
  \[p'(0)={y_1-y_{-1}\over 2h}={f(x+h)-f(x-h)\over 2h}\]
  As $h\rightarrow 0$ this indeed converges to $f'(x)$.
 \end{exa}

However, this approach is generally unstable. If $f$ is complex analytic one can use complex analysis to do it which is stable, which we will not cover in this class.\\

Numerical differentiation is useful in optimization or root finding via Newton's method.

\newpage

\subsubsection{Cubic B{\'e}zier curves}
The {\bf cubic B{\'e}zier curve} is a curve parametrized by cubic functions: $\gamma: [0, 1]\rightarrow \mathbb{R}^2$, $\gamma(t)=(\gamma_1(t), \gamma_2(t))$, where $\gamma_1$ and $\gamma_2$ are both of degree at most $3$, and $\gamma(0)=P_0$, $\gamma'(0)=3(P_1-P_0)$, $\gamma(1)=P_3$, $\gamma'(1)=3(P_3-P_2)$, where $P_0$, $P_1$, $P_2$ and $P_3$ are the four ``control points''.

To find the formula for cubic B{\'e}zier curve, we can apply the formula for Hermite interpolation polynomial for $n=1$. B{\'e}zier curves has many applications in computer graphics and font design, and you might have already used it in applications that generate or edit vector graphics. Below is an example (drawn using LaTeX/TikZ):
\begin{figure}[H]
  \begin{tikzpicture}
    \node at (0, 2) {$P_0$};
    \node at (-1, 0) {$P_1$};
    \node at (1, 3) {$P_2$};
    \node at (3, 1) {$P_3$};
    \draw (0, 2) .. controls (-1, 0) and (1, 3) .. (3, 1);
  \end{tikzpicture}
\end{figure}
\newpage

\subsubsection{Linear and Hermite splines (Chapter 11)}

From the Example 2 above we see that polynomial interpolation with high degree is not guaranteed to work well. Hence, in practice, we often try to keep the degree of the polynomial low, which means that we will need to use piecewise functions for interpolation. We will discuss two kinds of piecewise polynomial interpolation: {\bf linear spline} and {\bf Hermite cubic spline}. The textbook also covered the {\bf natural cubic spline}.

\paragraph{Linear Spline}

\begin{definition}
  Let $f$ be a single variable function on $[a, b]$, $a=x_0<x_1\dots <x_n=b$ $n+1$ distinct points. The {\bf Linear Spline} $s_L$ with {\bf knots} at $x_i$ is defined as
  \[s_L(x)={x_i-x\over x_i-x_{i-1}}f(x_{i-1})+{x-x_{i-1}\over x_i-x_{i-1}}f(x_i), \text{ where }x_{i-1}\leq x\leq x_i\]
\end{definition}
In other words, use the 2-point Lagrange interpolation for each interval $[x_{i-1}, x_i]$.\\

\newpage

\begin{thm}\label{err_spline_lin} (Theorem 11.1 in textbook)
Let $f\in C^2$, $h=\max\{x_i-x_{i-1}\}$, $M=\max|f''|$, then for any $x\in [a, b]$, $|f(x)-s_L(x)|\leq {1\over 8}h^2M$.
\end{thm}

\begin{proof}
  Suppose $x$ is between $x_{i-1}$ and $x_i$. Theorem~\ref{err_lag} implies that
  \[f(x)-s_L(x)={f''(c)(x-x_{i-1})(x-x_i)\over 2!}\]
  for some $c\in [x_{i-1}, x_i]$. From assumption, $|f''(c)|<M$ and
  \[|(x-x_{i-1})(x-x_i)|\leq|(x_i-x_{i-1})/2|^2\leq h^2/4\ .\]
\end{proof}

The linear spline formula can be alternatively written as $s_L=\sum_if(x_i)\phi_i$, where $\phi_i$ are the ``hat functions'', where, if $i=1, \dots, n-1$,
\[\phi_i(x)=\begin{cases} (x-x_{i-1})/(x_i-x_{i-1}) & x\in [x_{i-1}, x_i]\\
    (x-x_{i+1})/(x_i-x_{i+1}) & x\in [x_i, x_{i+1}] \\ 0 & otherwise \end{cases}\]
$\phi_0$ and $\phi_n$ can be written down similarly. As a consequence, $s_L$ lies in the span of $\phi_n$. \\

\newpage

\paragraph{Hermite cubic spline}

\begin{definition} Let $f\in C^1[a, b]$,  $a=x_0<x_1\dots <x_n=b$ $n+1$ distinct points. The {\bf Hermite Cubic Spline} $s_H$ with {\bf knots} at $x_i$ is defined as $s_H(x)=p_i(x)$ for $x\in [x_{i-1}, x_i]$, where $p_i$ is the Hermite interpolation polynomial defined using $\{x_{i-1}, x_i\}$.
\end{definition}


\begin{thm}\label{err_spline_her} (Theorem 11.4 in textbook)
Let $f\in C^2$, $h=\max\{x_i-x_{i-1}\}$, $M=\max|f^{(4)}|$, then for any $x\in [a, b]$, $|f(x)-s_H(x)|\leq {1\over 384}h^4M$.
\end{thm}
The proof is similar to Theorem~\ref{err_spline_lin}. Note that $384=4!2^4$.\\

One can also find a set of basis functions for $s_H$.

% \newpage

% \subsubsection{Natural cubic spline}

% \begin{definition} Let $f\in C[a, b]$, $a=x_0<x_1\dots <x_n=b$ $n+1$ distinct points. The {\bf Natural Cubic Spline} $s_2$ is a piecewise polynomial function which is cubic on each $[x_{i-1}, x_i]$, in $C^2$, and $s_2''(x_0)=s_2''(x_n)=0$.
% \end{definition}

% \begin{thm} The natural cubic spline exists and is unique.\end{thm}
% \begin{proof}
% To calculate $s_2$, firstly note that $s_2''$ is piecewise linear with linear pieces on each $[x_{i-1}, x_i]$, hence one only need to find $\sigma_i=s_2''(x_i)$ for all $i$, then integrate the resulting linear spline twice. By assumption $\sigma_0=\sigma_n=0$, and for $i=1, \dots, n-1$, by integrating twice, we have
% \[f(x_{i\pm 1})=f(x_i)+(x_{i\pm 1}-x_i)f'(x_i)+{1\over 2}(x_{i\pm 1}-x_i)^2\sigma_i\]
% \[+{1\over 6}(x_{i\pm 1}-x_i)^3\left({\sigma_{i\pm 1}-\sigma_i\over x_{i\pm 1}-x_i}\right)\ .\]

% \newpage

% Now eliminate $f'(x_i)$, we get
% \[(x_i-x_{i-1})\sigma_{i-1}+2(x_{i+1}-x_{i-1})\sigma_i+(x_{i+1}-x_i)\sigma_{i+1}\]
% \[=6\left({f(x_{i+1}-f(x_i)\over x_{i+1}-x_i}-{f(x_i)-f(x_{i-1})\over x_i-x_{i-1}}\right)\ .\]
% So we get a system of linear equations for $\sigma_i$. From linear algebra, we know that the solution exists and is unique if the coefficient matrix is non-singular, i.e. if the system of linear equations:
% \[y_0=y_n=0\]
% \[(x_i-x_{i-1})y_{i-1}+2(x_{i+1}-x_{i-1})y_i+(x_{i+1}-x_i)y_{i+1}=0\]
% has only non zero solution.\\

% Suppose it has a non-zero solution $(y'_0, \dots, y'_n)$, let $j$ be the first index such that $|y'_j|$ reaches its maximum, then
% \[|2(x_{j+1}-x_{j-1})y_j|>|(x_j-x_{j-1})y_{j-1}|+|(x_{j+1}-x_j)y_{j+1}|\]
% A contradiction.
% \end{proof}

% There is a similar error estimate for natural cubic spline as well, with a larger constant.

% \newpage

% \subsection{Basis function for spline}


% There are analogous basis function for the other two splines as well.

% The basis for $s_H$ is similar.\\

% The natural cubic spline $s_2$ also lies in a finite dimensional vector space.  When $x_i$ are equally spaced, $x_{i}-x_{i-1}=h$, $j=2, \dots, n-2$, a basis function can be chosen as
% \[\psi_j(x)=\begin{cases}{1\over 4h^4}\sum_{k=0}^4(-1)^k{4 \choose k}(x-(x_j-(2+k)h))^3 & |x-x_j|<2h \\ 0 & otherwise \end{cases}\]

\newpage

\subsection{Review}

\begin{itemize}
\item Definition and formula of Lagrange/Hermite interpolation polynomials.
\item Uniqueness.
\item Error estimate.
\end{itemize}

\section{Approximation Theory (Chapter 8, 9)}

We can see that the Lagrange interpolation polynomial, Hermite interpolation polynomial, and the splines all lie in a vector space spanned by finitely many functions. In other words, all these algorithms can be seen as a way to {\bf approximate a function using the linear combination of simpler functions}.\\

Recall that a set of functions form a vector space if it is closed under addition and scalar multiplication.

\newpage

\subsection{Approximation in normed vector space}

\begin{definition} Let $V$ be a vector space. A {\bf norm} on $V$ is a function: $\|\cdot \|: V\rightarrow \mathbb{R}_{\geq 0}$ such that:
  \begin{itemize}
  \item $\|x\|=0$ iff $x=0$
  \item $\|x+y\|\leq \|x\|+\|y\|$
  \item $\|cx\|=|c|\|x\|$.
  \end{itemize}
\end{definition}

\begin{exa}
  \begin{enumerate}
   \item $V=C([a, b])$ (continuous functions on $[a, b]$), $\|f\|_\infty=\max|f|$. This is called the $L^\infty$ norm.
   \item $V=L^2([a, b])$, $\|f\|_2=(\int_a^b|f(x)|^2dx)^{1/2}$. This is called the $L^2$ norm.
   \item Replace $2$ with $p\geq 1$ we get $L^p$ norm. If $p<1$, the triangle inequality is no longer satisfied.  
  \end{enumerate}
\end{exa}

\newpage

\begin{definition}
Let $L=span\{x_1, \dots x_m\}$ be a $m$-dimensional subspace of $V$, $x\in V$. The {\bf best approximation} of $x$ is the element $x'\in L$ that minimizes $\|x-x'\|$.  
\end{definition}

\begin{thm} (Theorem 8.2 in textbook) The best approximation always exists.\end{thm}

The proof has two steps:

\begin{enumerate}
\item $\|\cdot-x\|$ is continuous on $L$.
\item $\|\cdot-x\|$ goes to infinity at infinity.
\end{enumerate}

Key idea: if a function is defined on a finite dimensional vector space, continuous, and goes to infinity at infinity, then it has a minimum.

\newpage

Please ignore the proof below if you are not interested.

\begin{proof}
  Let $x_1, \dots x_m$ be a basis of $L$. Consider a function $F_x: \mathbb{R}^m\rightarrow \mathbb{R}$ defined as
  \[F_x((t_1, \dots, t_m))=\|x-\sum_i t_ix_i\|\]
  The first step of the proof is to show that $F$ is continuous:
  \begin{lem}\label{cont} $F_x$ is continuous.\end{lem}
  \begin{proof}
  Suppose $t'\in\mathbb{R}^m$ satisfies that $|t_i'|<\epsilon$ for all $i$, then by triangle inequality,
  \[|F_x(t+t')-F_x(t)|\leq |\sum_i t_i'x_i|\leq \epsilon\sum_i |x_i|\]
  This implies that if $t'$ is sufficiently small, $F_x(t+t')$ can be arbitrarily close to $F_x(t)$, hence $F_x$ is continuous.
\end{proof}

  \newpage
  
  Now, let $D_R\subset\mathbb{R}^m=\{t:|t_i|\leq R\text{ for all }i\}$. It is a closed set, hence compact (recall the definition of compactness in your analysis class), hence a continuous function $F_x$ takes minimum at some point $x^*_R$ on $D_R$. We just need to show that if $R$ is large enough, $x^*_R$ is also the minimum of $F_x$.\\

  Let $g>0$ be the minimum of $F_0$ on the set $D_1$. Now we set $R_0=(2|x|+1)/g$. Then for any $y$ outside $D_{R_0}$, then $F_x(y)=\|y-x\|\geq \|x\|+1>F_x(0)\geq F_x(x^*_{R_0})$
\end{proof}

\newpage

\subsection{Stone-Weiersterass theorem}

\begin{thm} (Theorem 8.1 in textbook) For continuous function $f\in C([a, b])$, any $\epsilon>0$, there is some polynomial $p$ such that $\|f-p\|_\infty<\epsilon$.
\end{thm}

There are many proofs, some work for more general settings. An easy proof is first use linear spline to approximate $f$, then use polynomials to approximate the basis function (which is a linear combination of absolute values, which can be approximated by $(x^2+\epsilon')^{1/2}$, which can be approximated using Taylor expansion).


\newpage

\subsection{Approximation in inner product space}

Sometimes the norm on a vector space arises from a inner product (a symmetric, positive definite, bilinear form) $(\cdot, \cdot): V\times V\rightarrow \mathbb{R}$, by $||x||=\sqrt{(x, x)}$. If so, we call it an {\bf inner product space}.\\

\begin{exa}The $L^2$ norm on $L^2([a, b])$ arises from inner product $(f, g)=\int_a^b fg dx$. Let $w$ be a non negative, continuous and integrable ``weight function'' on $[a, b]$, we can also defined the ``weighted $L^2$ norm'' which is from $(f, g)_w=\int_a^bwfg dx$.\end{exa}

It's easy to see that the $L^2_w$ norm satisfies:
\[\|f\|_{w}\leq(\int_a^bwdx)^{1/2}\|f\|_\infty\]

\begin{exa}On $C^1([a, b])$ we can define the $(1, 2)$ Sobolev norm $\|f\|_{1, 2}=(\int_a^b|f(x)|^2+|f'(x)|^2dx)^{1/2}$. This norm also come from an inner product
  \[(f, g)_{1, 2}=\int_a^b f(x)g(x)+f'(x)g'(x)dx\]
\end{exa}

\newpage

Let $L=span\{x_1,\dots,  x_m\}$, then we can use Gram-Schmidt process to get an orthonormal basis of $L$ under $(\cdot, \cdot)$, called $\{e_1,\dots, e_m\}$. Then we have:

\begin{thm}The best approximation of $x\in V$ by an element of $L$ is unique, and it is
  \[x^*=\sum_i(x, e_i)e_i\]
\end{thm}

\begin{proof}
  For any other $x'=\sum_i t_ie_i\in L$,
  \[\|x'-x\|^2=((x'-x^*)+(x^*-x), (x'-x^*)+(x^*-x))\]
  \[=\|x'-x^*\|^2+\|x^*-x\|^2+2(\sum_i(t_i-(x, e_i))e_i, \sum_i(x, e_i)e_i-x)\]
  \[=\|x'-x^*\|^2+\|x^*-x\|^2+2\sum_j(t_i-(x, e_j))(e_j, \sum_i(x, e_i)e_i-x)\]
  \[=\|x'-x^*\|^2+\|x^*-x\|^2+2\sum_j(t_i-(x, e_j))(\sum_i(x, e_j)(e_j, e_j)-(x, e_j))\]
  \[=\|x'-x^*\|^2+\|x^*-x\|^2\geq \|x^*-x\|^2\]
  And equality is reached only when $x'=x^*$.
\end{proof}

\newpage

When the inner product is the $L^2_w$ inner product, the integrals in the formula for best approximation will often be calculated numerically (cf. next Section).\\

The proof is the same as the finite dimensional case you have seen in linear algebra.\\

If $x_i$ are only orthogonal and not orthonormal, the formula becomes
\[x^*=\sum_i{(x, x_i)\over (x_i, x_i)}x_i\]

If $x_i$ are not known to be orthogonal either, the formula becomes
\[x^*=\sum_i(\sum_j(x, x_j)(A^{-1})_{i, j})x_i\]
Where
\[A_{i,j}=(x_i, x_j)\]

\newpage

\subsection{Orthogonal polynomials}

\begin{definition}We call $\phi_j$, $j=0, 1, 2, \dots$ a system of {\bf orthogonal polynomials} with weight $w$, if 
  \begin{enumerate}
   \item $\phi_j$ is of degree $j$.
   \item $\phi_j$ are orthogonal to each other in $L^2_w$ norm.
  \end{enumerate}
\end{definition}

\begin{thm}\label{orth_exist} If $w$ is positive, continuous and integrable on $(a, b)$ then a system of {\bf orthogonal polynomials} with weight $w$ exists. \end{thm}

\begin{proof}
  This is Gram-Schmidt applied to $\{1, x, x^2, x^3, \dots\}$.
  \[\phi_0=1\]
  \[\phi_j=x^j-\sum_{i=0}^{j-1}{\int_a^b wt^j\phi_i(t)dt\over \int_a^b w\phi_i^2(t)dt}\phi_i(x)\] 
\end{proof}

\newpage

From linear algebra, we know that the system of orthogonal polynomials is unique up to scaling, since $\phi_j$ is the basis vector of the orthogonal complement of $span\{1, x, \dots, x^{j-1})$ in $span\{1, x, \dots, x^j\}$.\\


 \begin{rem}Stone-Weiersterass theorem implies that as degree increases, optimal approximation in $L^2_w([a, b])$ can become arbitrarily accurate. In other words, the orthogonal polynomials form an orthonormal basis of $L^2_w([a, b])$. (Which is NOT a basis in the sense of linear algebra. In algebra there is only finite sum.)
 \end{rem}

\newpage

\begin{exa}
  Let $(a, b)=(-1, 1)$.
  \begin{itemize}
  \item If $w=1$, the resulting orthogonal polynomials are called the {\bf Legendre polynomials} $L_j$.
  \item If $w(x)=(1-x^2)^{-1/2}$, the resulting orthogonal polynomials are called the {\bf Chebyshev polynomials} $T_j$. 
  \end{itemize}
 \end{exa}

 \begin{rem}
The Chebyshev polynomials have a particularly nice formula:
\[T_j=\cos(j\cos^{-1}x)\ .\]
They are polynomials because
\[T_0=1, T_1=x, T_2=2x^2-1\]
\[T_j=\cos(j\cos^{-1}x)=x\cos((j-1)\cos^{-1}x)-\sin(\cos^{-1}x)\sin((j-1)\cos^{-1}x)\]
\[=x\cos((j-1)\cos^{-1}x)-\sin^2(\cos^{-1}(x))\cos((j-2)\cos^{-1}x)\]
\[-\sin(\cos^{-1}x)\sin((j-2)\cos^{-1}x)\cos(\cos^{-1}x)\]
\[=xT_{j-1}-(1-x^2)T_{j-2}-x(T_{j-3}-T_{j-1})/2\ .\]

\newpage

 They are othogonal, because $j\not=j'$,
\[\int_{-1}^1 \cos(j\cos^{-1}x)\cos(j'\cos^{-1}x)(1-x^2)^{-1/2}dx\]
\[=-\int_{-1}^{1}\cos(j\cos^{-1}x)\cos(j'\cos^{-1}x)d\cos^{-1}(x)\]
\[=\int_0^\pi\cos(jt)\cos(j't)dt=0\]
\end{rem}

\begin{rem}Furthermore, if $T_j=\cos(j\cos^{-1}x)$, $2^{-j}T_{j+1}$ is the degree $j+1$ monic (leading coefficient being 1) polynomial with the smallest $L^\infty$ norm. This tells us that the term $\prod_i(x-x_i)$ in Theorem~\ref{err_lag} can be minimized (in $L^\infty$) if $x_i$ are chosen as the roots of Chebyshev polynomials, or, in other words, if $\prod_i(x-x_i)=2^{-n}T_{n+1}$. This is proved in Chapter 8 of the textbook.
\end{rem}

\newpage

\begin{figure}[H]
\begin{tikzpicture}[yscale=1, xscale=1]
  \draw[->] (-1, 0) -- (1, 0) node[right] {$x$};
  \draw[->] (0, -1.5) -- (0, 1.5) node[above] {$y$};
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot ({\x}, {1});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot ({\x}, {\x});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot ({\x}, {2*\x*\x-1});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot ({\x}, {4*\x*\x*\x-3*\x});

    \draw[->] (2, 0) -- (4, 0) node[right] {$x$};
  \draw[->] (3, -1.5) -- (3, 1.5) node[above] {$y$};
  \draw[scale=1, domain=2:4, smooth, variable=\x, blue] plot ({\x}, {1});
  \draw[scale=1, domain=2:4, smooth, variable=\x, blue] plot ({\x}, {\x-3});
  \draw[scale=1, domain=2:4, smooth, variable=\x, blue] plot ({\x}, {(\x-3)*(\x-3)-1/3});
  \draw[scale=1, domain=2:4, smooth, variable=\x, blue] plot ({\x}, {(\x-3)*(\x-3)*(\x-3)-0.6*(\x-3)});
  
\end{tikzpicture}
\caption{Chebyshev polynomials and Lagrange polynomials}
\end{figure}


\begin{itemize}
\item First Legendre polynomials (which I calculated using Gram-Schmidt, another alternative calculation can be found in the exercises, and also HW 4)
  \[L_0=1, L_1=x, L_2=x^2-{1\over 3}\]
  \[L_3=x^3-{3\over 5}x\]

\item First Chebyshev polynomials:

  \[T_0=1, T_1=x, T_2=2x^2-1, T_3=4x^3-3x\]

\end{itemize}



\newpage


\begin{thm}\label{orth_roots}
  If the weight function $w$ is positive, continuous and integrable on $(a, b)$, then $\phi_j$ has $j$ distinct real roots in $(a, b)$. 
\end{thm}
\begin{proof}
Suppose not, then $\phi_j$ switches sign fewer than $j$ times in $(a, b)$. Suppose $x_1, \dots, x_k$ are the points in $(a, b)$ where $\phi_j$ changes sign, then $(\phi_j, \prod_{i=1}^k(x-x_i))$ is non zero. However $\prod_{i=1}^k(x-x_i)\in span\{\phi_0, \dots \phi_{j-1}\}$, hence a contradiction.
\end{proof}

This Theorem will be used in the next section when we discuss Gauss's method for numerical integration.

\subsection{Review}

\begin{itemize}
\item Normed vector space, inner product space, $L^\infty$, $L^2$ and $L^2_w$ norms.
\item Existence of optimal approximation. Calculation of optimal approximation for inner product space.
\item Concept of orthogonal polynomials.
\end{itemize}

\newpage

\begin{exa} Consider the function $y=e^x$ on $[-1, 1]$.
  \begin{itemize}
  \item Find the Lagrange interpolation polynomial, interpolating at $0$, $\pm 1$.
  \item Find the Hermite interpolation polynomial, interpolating at $\pm 1$.
  \item Find the best approximation via a polynomial of degree at most $2$, under the $L^2$ norm.
  \end{itemize}
\end{exa}

  Answer:
  \begin{itemize}
  \item Use formula $p_L=\sum_iy_i\prod_{j\not=i}(x-x_j)/\prod_{j\not=i}(x_i-x_j)$:
    \[p_L(x)=e^{-1}\cdot {x(x-1)\over -1\cdot -2}+1\cdot {(x+1)(x-1)\over 1\cdot -1}+e\cdot {x(x+1)\over 1\cdot 2}\]
    \[=(e^{-1}/2+e/2-1)x^2+(e/2-e^{-1}/2)x+1\]
  \item Use formula $p_H=\sum_iz_i(x-x_i)\prod_{j\not=i}(x-x_j)^2/\prod_{j\not=i}(x_i-x_j)^2+\sum_iy_i(1-(x-x_i)\sum_{j\not=i}(2/(x_i-x_j)))\prod_{j\not=i}(x-x_j)^2/\prod_{j\not=i}(x_i-x_j)^2$:
    \[p_H(x)=e^{-1}\cdot{(x+1)(x-1)^2\over (-1-1)^2}+e\cdot {(x-1)(x+1)^2\over (1+1)^2}\]
    \[+e^{-1}\cdot (1+(x+1))\cdot {(x-1)^2\over (-1-1)^2}+e\cdot (1-(x-1))\cdot {(x+1)^2\over (1+1)^2}\]
    \[=(e^{-1}/2)x^3+(e/4-e^{-1}/4)x^2+(e/2-e^{-1})x+e/4+3e^{-1}/4\]

    \newpage
    
    \item Use formula $x^*=\sum_i((x, x_i)/(x_i, x_i))x_i$:
  \[p_2(x)={\int_{-1}^1e^tdt\over \int_{-1}^1 1^2 dt}\cdot 1+{\int_{-1}^1 te^tdt\over \int_{-1}^1 t^2}\cdot x+{\int_{-1}^1(t^2-1/3)e^tdt\over \int_{-1}^1 (t^2-1/3)^2dt}\cdot (x^2-1/3)\]

  \[={(e-e^{-1})\over 2}\cdot 1+{2e^{-1}\over 2/3}\cdot x+{2e/3-14e^{-1}/3\over 8/45}(x^2-1/3)\]
  \[={15e-105e^{-1}\over 4}x^2+3e^{-1}x+{-3e+33e^{-1}\over 4}\]
  \end{itemize}

\newpage


\begin{figure}[H]
\begin{tikzpicture}[yscale=2, xscale=2]
  \draw[->] (-1, 0) -- (1, 0) node[right] {$x$};
  \draw[->] (0, -0.5) -- (0, 3) node[above] {$y$};
  \draw[scale=1, domain=-1:1, smooth, variable=\x, thick, black] plot ({\x}, {exp(\x)});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, blue] plot({\x}, {(exp(-1)/2+exp(1)/2-1)*\x*\x+(exp(1)/2-exp(-1)/2)*\x+1});
\draw[scale=1, domain=-1:1, smooth, variable=\x, green] plot({\x}, {(exp(-1)/2)*\x*\x*\x+(exp(1)/4-exp(-1)/4)*\x*\x+(exp(1)/2-exp(-1))*\x+exp(1)/4+3*exp(-1)/4});
  \draw[scale=1, domain=-1:1, smooth, variable=\x, red] plot({\x}, {(30*exp(1)-210*exp(-1))/8*\x*\x+3*exp(-1)*\x+(-3*exp(1)+33*exp(-1))/4});
\end{tikzpicture}
\caption{Black line: $y=e^x$. Blue line: Lagrange interpolation. Green line: Hermite interpolation. Red line: $L^2$ best approximation}
\end{figure}

\newpage


\section{Numerical Integration (Chapter 7, 10)}

\subsection{Quadrature rule}

Question: Estimate $\int_a^bf(x)dx$.\\

Let $x_0=a<x_1<\dots<x_n=b$ be $n+1$ distinct points in $[a, b]$, then we can use the Lagrange interpolation polynomial to estimate $f$, and hence 
\[\int_a^b f(x)dx\approx \sum_kw_kf(x_k)\]
  Where
  \[w_k=\int_a^b {\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)} dx\]
To estimate the integration.\\

The points $x_i$ are called {\bf quadrature points}, and $w_i$ called {\bf quadrature weights}. The formula still works if some $x_i$ is outside $[a, b]$.

\newpage

\subsection{Newton-Cotes method}

\begin{definition} When $a=x_0<x_1<\dots<x_n=b$ are $n+1$ evenly spaced points, the formula above is called the {\bf Newton-Cotes formula}, the evenly spaced $x_i$ the {\bf Newton-Cotes quadrature}.
 \end{definition}

\begin{exa} When $n=1$, $w_0=w_1={b-a\over 2}$, this is called the {\bf Trapezium rule} (as it's like calculating the area of a collection of trapeziums). When $n=2$, $w_0=w_2={b-a\over 6}$, $w_1={2(b-a)\over 3}$. This is called {\bf Simpson's rule}.
\end{exa}

\begin{exa} $\int_0^1\sin(x)dx$. Using Trapezium rule, the estimate is
  \[{\sin(0)+\sin(1)\over 2}=0.4207\]
  Using Simpson's rule, the estimate is
  \[\sin(0)/6+\sin(0.5)*2/3+\sin(1)/6=0.4599\]
  The true value is $1-\cos(1)=0.4597$.
\end{exa}

\newpage

\begin{thm} \label{bound1}(Theorem 7.1 in the textbook)
The error for the quadrature is bounded by
\[{\max|f^{(n+1)}|\over (n+1)!}\int_a^b\prod_i|x-x_i| dx\]
\end{thm}

The proof is follows immediately from the error estimate of Lagrange interpolation. When $x_i$ are Newton-Cotes quadrature, the error bound is $O(\max|f^{(n+1)}|(b-a)^{n+2})$, because when we scale the interval $[a, b]$ by $C$, the function being integrated is scaled by $C^{n+1}$ while the range of the integration is also scaled by $C$.

\newpage

For Newton-Cotes, when $n$ is even (in order words when we have odd number of quadrature points), the error bound can be improved to $\max|f^{(n+2)}|\cdot O((b-a)^{n+3})$ provided $f\in C^{n+2}$:

\begin{thm}\label{qbound} Let $n$ be an even number, $f\in C^{n+2}([a, b])$, $I_n(f)$ the Newton-Cotes formula using $n+1$ evenly spaced points on $[a, b]$, then there is some $C_n$ (depending on $n$) such that
  \[|\int_a^bf(x)dx-I_n(f)|\leq C_n\max|f^{(n+2)}|(b-a)^{n+3}\]
\end{thm}
\begin{proof}
  The uniqueness of Lagrange interpolation implies that if $f$ is a polynomial of degree at most $n$, $\int_a^bf(x)dx=I_n(f)$. Now consider the polynomial $g=\prod_i(x-x_i)$. Because $x_i$ are evenly spaced, the graph of $\prod_i(x-x_i)$ is symmetric with respect to the point $(x_{n/2}, 0)$ where $x_{n/2}=(a+b)/2$. So $\int_a^bgdx=0=I_n(g)$. However, any polynomial of degree at most $n+1$ can be written in the form $cg+h$, where $h$ is a polynomial of degree at most $n$. Hence, if $f$ is any polynomial of degree at most $n+1$, $\int_a^bf(x)dx=I_n(f)$.\\

\newpage
  
  Now suppose $f\in C^{n+2}$. Let $x_{n+1}$ be the midpoint of $[x_0, x_1]$, let $p'$ be the Lagrange interpolation polynomial of $f$, then $f-p'$ vanishes at $x_0, \dots, x_n, x_{n+1}$, hence the quadrature formula using $x_0, \dots x_{n+1}$ is $0$. Apply Theorem~\ref{bound1} we get
  \[|\int_a^b(f-p')dx|\leq C_n\max|f^{n+2}|(b-a)^{n+3}\]
  However, because $p'(x_i)=f(x_i)$ for $i=0, 1, 2, \dots, n$,
  \[\int_a^bp'dx=I_n(p')=I_n(f)\]
  Which proves the theorem.
\end{proof}

\newpage

We can also use mean value theorem to get finer bounds. As an example, when $n=2$, we have

\begin{thm}(Theorem 7.2 in the textbook) If $f\in C^4$, there is some $c\in [a, b]$ such that
  \[\int_a^b f(x)dx-(b-a)\cdot(f(a)/6+2f((a+b)/2)/3+f(b)/6)=-{f^{(4)}(c)(b-a)^5\over 2880}\]
\end{thm}

\begin{proof}
  Let
  \[G_1(t)=\int_{(a+b)/2-t}^{(a+b)/2+t}f(s)ds-2t\cdot(f((a+b)/2-t)/6\]
  \[+2f((a+b)/2)/3+f((a+b)/2+t)/6)\]
  \[G(t)=G_1(t)-({t\over (b-a)/2})^5G_1((b-a)/2)\]
  Then $G(0)=G((b-a)/2)=G'(0)=G''(0)=0$. So there is $0<c_1<(b-a)/2$ such that $G'(c_1)=0$, $0<c_2<c_1$ such that $G''(c_2)=0$, $0<c_3<c_2$ such that $G'''(c_3)=0$.

\newpage

By calculation, $G_1'''(c_3)={c_3\over 3}\cdot(f'''((a+b)/2-c_3)-f'''((a+b)/2+c_3))$, so
\[{c_3\over 3}\cdot(f'''((a+b)/2-c_3)-f'''((a+b)/2+c_3))-{1920\over (b-a)^5}c_3^2G_1((b-a)/2)=0\]
Hence
\[{\cdot(f'''((a+b)/2+c_3)-f'''((a+b)/2-c_3))\over 2c_3}=-{2880\over (b-a)^5}G_1((b-a)/2)\]
Now apply mean value theorem for $f'''$ on $[(b+a)/2-c_3, (b+a)/2+c_3]$, we get the $c$. 
\end{proof}

\begin{proof}[Alternative proof]
  Following the same argument as Theorem~\ref{qbound}, we know that if $f$ is a polynomial of degree $3$, $\int_a^bfdx$ equals the result of Simpson's rule.\\

  Now let $p$ be the Lagrange interpolation of $f$ at four distinct points $a$, $b$, $a+b\over 2$ and $d$. Then
  \[\int_a^bpdx=(b-a)({p(a)\over 6}+{2p((a+b)/2)\over 3}+{p(b)\over 6})\]
  \[=(b-a)({f(a)\over 6}+{2f((a+b)/2)\over 3}+{f(b)\over 6})\]
  However, by error bound of Lagrange polynomials, 
  \[f(x)-p(x)={f^{(4)}(c)(x-a)(x-b)(x-(a+b)/2)(x-d)\over 4!}\]
  Now push $d$ towards $c$, which makes $(x-a)(x-b)(x-(a+b)/2)(x-d)$ non positive on $[a, b]$. Now integrate for $x\in [a, b]$ and use integration mean value theorem, we get the theorem.
\end{proof}

\newpage

\subsection{Composite Method}

For the same reason as in Example~\ref{exa2}, when $n\rightarrow\infty$ the error can not be guaranteed to decay to $0$. So we often evenly decompose the interval $[a, b]$ into subintervals then carry out low order Newton-Cotes.\\

Let $a=x_0<\dots<x_n=b$ be $n+1$ evenly spaced points on $[a, b]$. If $n=dm$, we can cut $[a, b]$ into $m$ subintervals each with $d+1$ quadrature points, and apply Newton-Cotes on each.\\

For example, if $d=1$, we cut $[a, b]$ into $n$ subintervals and apply trapezium rule on each we get
\[{b-a\over n}(f(x_0)/2+\sum_{i=1}^{n-1}f(x_i)+f(x_n)/2)\]
If $d=2$, we cut $[a, b]$ into $n/2$ subintervals, and apply simpson's rule on each, we get
\[{b-a\over 3n}(f(x_0)+4\sum_{i=1}^{n/2}f(x_{2i-1})+2\sum_{i=1}^{n/2-1}f(x_{2i})+f(x_n))\]

\newpage


When $f$ is smooth with bounded higher order derivatives, the error estimate for each subinterval is $O(n^{-3})$ and $O(n^{-5})$ using the trapezium and Simpson's rule. Hence, the error composite trapezium and composite Simpson's rules decay like $O(n^{-2})$ and $O(n^{-4})$ respectively.


\begin{exa} $\int_0^1\sin(x)dx$. For this case, the error for composite trapezium \& Simpson's rule can be calculated explicitly. \end{exa}

\begin{itemize}
\item Composite trapezium rule with $n+1$ points:
  \[I_n={\sum_{i=1}^{n-1}\sin({i\over n})+sin(1)/2\over n}\]
  \[={\sum_{i=1}^{n-1}(\cos({i-1/2\over n})-\cos({i+1/2\over n}))+\sin({1\over 2n})\sin(1)\over 2n\sin({1\over 2n})}\]
  \[={\cos({1\over 2n})-\cos(1)\cos({1\over 2n})\over 2n\sin({1\over 2n})}=(1-\cos(1))\cdot{\cos({1\over 2n})\over 2n\sin({1\over 2n})}\]
  And it is easy to see that
  \[{\cos({1\over 2n})\over 2n\sin({1\over 2n})}=1-{1\over 3}(2n)^{-2}+\dots\]
  So the error decay at $O(n^{-2})$.

\newpage

\item Composite Simpson's rule with $2m+1$ points:
  \[I_m={4\sum_{i=1}^{m}\sin({2i-1\over 2m})+2\sum_{i=1}^{m-1}\sin({i\over m})+sin(1)\over 6m}\]
  \[={2(1-\cos(1))+\cos({1\over 2m})-\cos({2m-1\over 2m})+\sin(1)\sin({1\over 2m})\over 6m\sin({1\over 2m})}\]
  \[=(1-\cos(1))\cdot{2+\cos({1\over 2m})\over 6m\sin({1\over 2m})}\]
  \[=(1-\cos(1))\cdot{3-(2m)^{-2}/2+(2m)^{-4}/24+O((2m)^{-6})\over 3-(2m)^{-2}/2+(2m)^{-4}/40+O((2m)^{-6})}\]
  \[=(1-\cos(1))+O(m^{-4})\]
\end{itemize}

\newpage

\begin{exa}$\int_{-0.5}^{0.5}\sqrt{1-x^2}dx$. The right answer should be
  \[\sqrt{3}/4+\pi/6=0.9566114774905181\]
\end{exa}

\begin{enumerate}
  \item Trapezium rule:
    \[(\sqrt{3/4}+\sqrt{3/4})/2=0.8660254037844386\]
  \item Simpson's rule:
    \[\sqrt{3/4}/6+1\times 2/3+\sqrt{3/4}/6=0.9553418012614795\]
  \end{enumerate}

 \newpage
\RestyleAlgo{boxruled}
\LinesNumbered
 \begin{algorithm}[H]
 $r\leftarrow f(a)+f(b)$\;  
 \For{$i=1, \dots n-1$}{
   $r\leftarrow r+2\times f({(n-i)a+ib\over n})$\;
 }
 The answer is ${(b-a)r\over 2n}$\;
 \caption{Composite Trapezium rule}
\end{algorithm}


\RestyleAlgo{boxruled}
\LinesNumbered
 \begin{algorithm}[H]
 $r\leftarrow f(a)+f(b)$\;  
 \For{$i=1, \dots n-1$}{
   \uIf{$i$ is odd}{
     $r\leftarrow r+4\times f({(n-i)a+ib\over n})$\;
   }
   \Else{
     $r\leftarrow r+2\times f({(n-i)a+ib\over n})$\;
   }
 }
 The answer is ${(b-a)r\over 3n}$\;
 \caption{Composite Simpson's rule}
\end{algorithm}


 \newpage
 
\begin{lstlisting}[language=Python]
from math import *
f=lambda x : (1-x*x)**0.5
def composite_trapezium(n, a, b, f):
    r=0
    r+=0.5*(f(a)+f(b))
    for i in range(1, n):
        r+=f(((n-i)*a+i*b)/n)
    return r*(b-a)/n
def composite_simpsons(n, a, b, f):
    r=0
    r+=f(a)+f(b)
    for i in range(1, n, 2):
        r+=4*f(((n-i)*a+i*b)/n)
    for i in range(2, n, 2):
        r+=2*f(((n-i)*a+i*b)/n)
    return r*(b-a)/3/n
\end{lstlisting}

\newpage

\begin{tikzpicture}
	\begin{axis}[
		xlabel=$log(n)$,
		ylabel=$-log(Error)$]
	\addplot[color=red,mark=x, only marks] coordinates {
		(0.6931471805599453,3.746560449724477)
(1.0986122886681098,4.547453810338138)
(1.3862943611198906,5.118965503519308)
(1.6094379124341003,5.5633925579666705)
(1.791759469228055,5.927002052110047)
(1.9459101490553132,6.2346716549051004)
(2.0794415416798357,6.501320810552323)
(2.1972245773362196,6.73660161087667)
(2.302585092994046,6.947117728247236)
(2.3978952727983707,7.137586004803848)
(2.4849066497880004,7.311492816297971)
(2.5649493574615367,7.4714878376833544)
(2.6390573296152584,7.6196319546081686)
(2.70805020110221,7.757559684401888)
(2.772588722239781,7.886589202729626)
(2.833213344056216,8.00779902926391)
(2.8903717578961645,8.12208280373383)
(2.9444389791664403,8.23018925813059)
(2.995732273553991,8.332751940018328)
(3.044522437723423,8.43031168655338)
(3.091042453358316,8.52333387219665)
(3.1354942159291497,8.612221823753858)
(3.1780538303479458,8.697327381267385)
(3.2188758248682006,8.778959303716851)
(3.258096538021482,8.857390026592949)
(3.295836866004329,8.932861144496894)
(3.332204510175204,9.00558789696487)
(3.367295829986474,9.075762867305217)
(3.4011973816621555,9.143559054538375)
};

	\addplot[color=blue,mark=+, only marks] coordinates {
	(1.3862943611198906,9.114856038771517)
(1.791759469228055,10.641478993908981)
(2.0794415416798357,11.752422282602218)
(2.302585092994046,12.624884254357372)
(2.4849066497880004,13.342684719274926)
(2.6390573296152584,13.95214413140681)
(2.772588722239781,14.481537873892268)
(2.8903717578961645,14.949379792923526)
(2.995732273553991,15.368444387441553)
(3.091042453358316,15.7479127618939)
(3.1780538303479458,16.09460251736614)
(3.258096538021482,16.413713532022914)
(3.332204510175204,16.70930151946475)
(3.4011973816621555,16.98459025133957)
	};
      \end{axis}
      
\end{tikzpicture}

\newpage

If we do $[a, b]=[-1, 1]$, with the same function as above, we get:

\begin{tikzpicture}
	\begin{axis}[
		xlabel=$log(n)$,
		ylabel=$-log(Error)$]
	\addplot[color=red,mark=x, only marks] coordinates {
(0.6931471805599453,0.560722828587799)
(1.0986122886681098,1.1592620458654719)
(1.3862943611198906,1.5858633733845402)
(1.6094379124341003,1.9175974979571417)
(1.791759469228055,2.1890810890660632)
(1.9459101490553132,2.418874071480326)
(2.0794415416798357,2.6180935155697216)
(2.1972245773362196,2.793928232434429)
(2.302585092994046,2.9512961008710596)
(2.3978952727983707,3.0937101353866443)
(2.4849066497880004,3.2237673778435036)
(2.5649493574615367,3.3434420342796574)
(2.6390573296152584,3.4542699820784497)
(2.70805020110221,3.557469639536541)
(2.772588722239781,3.6540238458643706)
(2.833213344056216,3.744736933949296)
(2.8903717578961645,3.8302755020496635)
(2.9444389791664403,3.9111981709248282)
(2.995732273553991,3.9879777148660915)
(3.044522437723423,4.061017798022754)
(3.091042453358316,4.130665820977959)
(3.1354942159291497,4.197222914402594)
(3.1780538303479458,4.260951807849626)
(3.2188758248682006,4.322083093796923)
(3.258096538021482,4.380820264323835)
(3.295836866004329,4.437343798158711)
(3.332204510175204,4.491814505168301)
(3.367295829986474,4.544376284523874)
(3.4011973816621555,4.595158415724219)
};

	\addplot[color=blue,mark=+, only marks] coordinates {
(1.3862943611198906,2.491780761461278)
(1.791759469228055,3.105837770018371)
(2.0794415416798357,3.5405025767733957)
(2.302585092994046,3.8771701458682797)
(2.4849066497880004,4.151981616128501)
(2.6390573296152584,4.384170395855168)
(2.772588722239781,4.585196798057869)
(2.8903717578961645,4.762442852040901)
(2.995732273553991,4.920943489214822)
(3.091042453358316,5.064286761613613)
(3.1780538303479458,5.195120026882598)
(3.258096538021482,5.315452488738959)
(3.332204510175204,5.426845119840145)
(3.4011973816621555,5.530534801579673)
	};
      \end{axis}
    \end{tikzpicture}

    
   Because the derivatives go to infinity at $x$ close to $\pm 1$, both methods only have $O(n^{-1})$ error bound.

   \newpage

   Review:\\
   
   \begin{itemize}
   \item Interpolation
     \begin{itemize}
      \item Lagrange interpolation
      \item Hermite interpolation
      \item Uniqueness and error estimate
      \end{itemize}
    \item Approximation theory
      \begin{itemize}
        \item Normed vector space and inner product space
        \item Gram-Schmidt
        \item Orthogonal projection
        \end{itemize}
      \item Numerical integration
        \begin{itemize}
        \item Quadrature rule: $I=\sum_{i=0}^nw_if(x_i)$
        \item Newton-Cotes quadrature, $n=1, 2$
        \item Error estimate via error estimate of Lagrange interpolation
        \item Composite methods
        \end{itemize}
      \end{itemize}
      \newpage
      Example: $f(x)=\sin(x)$, $x\in [0, \pi]$, $x_0=0, x_1=\pi/2, x_2=\pi$.
      \begin{itemize}
      \item The Lagrange interpolation polynomial is
        \[p(x)=0\cdot {(x-\pi/2)(x-\pi)\over \pi^2/2}+1\cdot {x(x-\pi)\over -\pi^2/4}+0\cdot {(x-\pi/2)x\over \pi^2/2}\]
      \item The numerical integration using Simpson's rule is
        \[I(f)=\int_0^\pi p(x)dx=0\cdot {\pi\over 6}+1\cdot {2\pi\over 3}+0\cdot {\pi\over 6}={2\pi\over 3}\]
      \item The error estimate for Lagrange interpolation:
        \[|f(x)-p(x)|=\left|{f'''(s)x(x-{\pi/2})(x-\pi)\over 3!}\right|\leq {1\over 6}|x(x-\pi/2)(x-\pi)|\]
      \item Integrate the error estimate above, we get
        \[|\int_0^\pi f(x)dx-I(f)|\leq {1\over 6}\int_0^\pi|x(x-\pi/2)(x-\pi)|={1\over 6}\cdot \pi^4 \cdot{1\over 32}={\pi^4\over 192}\approx 0.507\]
      \item We can find an alternative error estimate via the following procedure
        \begin{enumerate}
         \item We see that if $g$ is a polynomial of degree $2$, $g$ equals its Lagrange interpolation at $x_0, x_1, x_2$, hence $I(g)=\int_0^\pi gdx$.
         \item Furthermore, if $f_3=x(x-\pi/2)(x-\pi)$, $I(f_3)=0=\int_0^\pi f_3dx$.
         \item Hence, if $g_3$ is a polynomial of degree 3, then $g_3=af_3+g$ for some $a\in\mathbb{R}$, and $g$ is a polynomial of degree $2$. Hence
           \[I(g_3)=aI(f_3)+I(g)=\int_0^\pi g(x)dx=\int_0^\pi (af_3+g)dx=\int_0^\pi g_3dx\]
         \item Now Let $p$ be the Lagrange interpolation polynomial of $f$ at $x_0, x_1, x_2, x_3=c$, then $\int_0^\pi pdx=I(p)=I(f)$.
         \item The error estimate for Lagrange interpolation gives us
           \[|f(x)-p(x)|=\left|{f''''(s)x(x-c)(x-{\pi/2})(x-\pi)\over 4!}\right|\]
           \[\leq {1\over 24}|x(x-c)(x-\pi/2)(x-\pi)|\]
         \item Integrate the error estimate, let $c\rightarrow {\pi/2}$, we get
           \[|\int_0^\pi f(x)dx-I(f)|\leq {1\over 24}\int_0^\pi |x(x-\pi/2)^2(x-\pi)| dx={\pi^5\over 2880}\approx 0.106\]
           This is the bound in Theorem 7.2 in the textbook.
         \end{enumerate}
       \item The actual error is $2\pi/3-2=0.094$.
      \end{itemize}
      \newpage
      Exercises 1: Let $f(x)=x^3(1-x)$, $x\in [0, 1]$. $x_0=0$, $x_1=c$, $x_2=1$, $c\in (0, 1)$.
      \begin{itemize}
      \item Find the Lagrange interpolation of $f$ at $x_0$, $x_1$, $x_2$.
      \item Integrate the Lagrange interpolation on $[0, 1]$ to get an estimate of $\int_0^1f(x)dx$. 
      \item Find $c$ such that the estimate you found above is optimal.
      \end{itemize}
      
      Exercise 2: Let $[a, b]=[0, 1]$, $x_0=0$, $x_1=0.5$, $x_2=1$.
      \begin{itemize}
      \item Write down the formula for Simpson's rule and the composite trapezium rule using $x_i$.
      \item Find a continuous function on $[a, b]$ where the Simpson's rule works better than the composite trapezium rule, and vice versa.
      \end{itemize}
      
       Answer:\\

       Exercises 1:\\
       \begin{itemize}
       \item $p=c^3(1-c)\cdot {x(x-1))\over c(c-1)}=-c^2x(x-1)$.
       \item $I=c^2/6$. 
       \item $\int_0^1f(x)dx=1/20$, so $c$ should be $\sqrt{0.3}$.
       \end{itemize}
      
       Exercise 2:\\
       \begin{itemize}
       \item Simpson's rule: $f(0)/6+2f(0.5)/3+f(1)/6$. Composite trapezium rule: $f(0)/4+f(0.5)/2+f(1)/4$.
       \item $f(x)=x^2$. $f(x)=|x-0.5|$. 
       \end{itemize}

\newpage
       
       Correction for the lecture on 11/6: We can also estimate the integral by decomposing the interval into $n$ subintervals with the same length, and calculate the sum of the areas of rectangles with those subintervals as base. If the height is taken as the value of the function at end points (i.e. $I={b-a\over n}\sum_{i=0}^{n-1}f(x_i)$), or use the maximum or minimum on the interval as in the definition of Riemann integration, then the error grows as $O(1/n)$ if $f$ is smooth, where $n$ is the number of subintervals, which is worse than composite trapezium rule. However, if the height is taken as the value of the function at midpoints (i.e. $I={b-a\over n}\sum_{i=0}^{n-1}f({x_i+x_{i+1}\over 2})$) the error grows as $O(1/n^2)$ if $f$ is smooth (so it's about as accurate as the trapezium rule).
       
  \newpage     
  \subsection{Gauss quadrature (Also see Sections 10.2-10.4 in the textbook)}
  Motivation:
  \begin{enumerate}
  \item From the error bound of composite rules, we know that if the quadrature has error bound $O(|b-a|^k)$, the composite rule will have an error bound of $O(n^{-k+1})$.
  \item So suppose $f$ is sufficiently smooth, it might be good to try and make the number $k$ as large as possible.
  \item From the proof of Theorem~\ref{qbound}, we see that if a quadrature rule gives accurate answer to polynomials of degree $d$, then the error bound is $O(|b-a|^{d+2})$. For example, for Simpson's rule $d=3$. 
  \item So, it may be good to {\bf strategically choose the quadrature points} such that the quadrature rule works for polynomials of high degrees. 
  \end{enumerate}

  Recall from the definition of Legendre polynomial, if $L_j$ are the Legendre polynomials, then
  \[L_j\perp span\{L_0, L_1, \dots L_{j-1}\}\]
  Under $L^2([-1, 1])$ norm.

  \newpage

  \begin{thm}\label{high_poly} Let $I(f)$ be the result of quadrature rule on $[-1, 1]$, using quadrature points  $x_i$, $i=0, \dots, n$, which the roots of $L_{n+1}$. For any polynomial $g$ of degree no more than $2n+1$, $I(g)=\int_{-1}^1gdx$.\end{thm}

  \begin{proof}
  It's easy to see that
  \[span\{1, x, \dots x^{2n+1}\}=span\{L_0, \dots L_{n+1}, L_{n+1}L_1, \dots, L_{n+1}L_n\}\]
  (A way to see it is by first recognizing that $span\{1, x, \dots x^{2n+1}\}$ has dimension $2n+2$, and show, from definition, that
  \[\{L_0, \dots L_{n+1}, L_{n+1}L_1, \dots, L_{n+1}L_n\}\]
  is a linearly independent set of $2n+2$ elements (because they all have different degrees) hence must be a basis.)

  \newpage

  Because both $I$ and $\int_{-1}^1$ are linear on the vector space consisting of polynomials of degree no more than $2n+1$, and two linear transformations are the same if and only if they are identical on the basis vectors, we only need to prove it when $g$ is $L_j$, $0\leq j\leq n$, as well as when $g$ is $L_jL_{n+1}$, $0\leq j\leq n$.
  \begin{itemize}
  \item Case 1: $g=L_j$ for some $j\leq n$. In this case, $p$ is of degree no more than $n$, hence $p$ is identical to its Lagrange interpolation at $n+1$ points. Hence $I(g)=\int_{-1}^1gdx$.
   \item Case 2: $g=L_jL_{n+1}$, $j\leq n$. Note that $L_{n+1}$ is proportional to $L_{n+1}L_0$ as $L_0$ is of degree $0$ hence a constant. Because $g$ has a factor $L_{n+1}$, $g(x_i)=0$ for all $i$, hence $I(g)=0$. On the other hand, because $L_ {n+1}$ and $L_j$ are orthogonal on $L^2([-1, 1])$, $\int_{-1}^1gdx=0$
  \end{itemize}
  \end{proof}

\newpage
  
  \begin{definition}
  The Gauss-Legendre quadrature points $x_0,\dots, x_n$ on $[a, b]$ is defined as $x_j={a+b\over 2}+c_j{b-a\over 2}$, where $c_j$ is the $j+1$-th root of the $n+1$-th Legendre polynomial. In other words, $x_j$ is the $j+1$-th root of the weight-$1$ orthogonal polynomial on $[a, b]$ with index $n+1$, $\psi_{n+1}$.
\end{definition}

  Now we can use Theorem~\ref{high_poly} to prove an error bound for Gauss-Legendre quadrature:
  \begin{thm}(Theorem 10.1 in textbook)
    Let $f\in C^{2n+2}$, then there is some $c\in [a,  b]$ such that
    \[\int_a^bfdx-I(f)={f^{(2n+2)}(c)\over (2n+2)!}\int_a^b\prod_i(x-x_i)^2 dx\]
  \end{thm}

  \begin{proof}
    Let $H$ be the Hermite interpolation polynomial of $f$ at $x_i$, then Theorem~\ref{high_poly} implies that $\int_a^bHdx=I(H)=I(f)$. Hence the result follows from the error bound of Hermite interpolation (Theorem~\ref{err_her}).

    \newpage

  More precisely, if $M$ and $m$ are the upper and lower bound of $f^{(2n+2)}$ on $[a, b]$ respectively, we have
    \[{m\prod_i(x-x_i)^2\over(2n+1)!}\leq f(x)-H(x)\leq {M\prod_i(x-x_i)^2\over(2n+1)!}\]

  Hence
    \[{m\over (2n+2)!}\int_a^b\prod_i(x-x_i)^2 dx\leq \int_a^bfdx-I(f)\leq {M\over (2n+2)!}\int_a^b\prod_i(x-x_i)^2\]
    And the existence of $c$ follows from intermediate value theorem in analysis.
  \end{proof}

  
  \begin{rem}
As $|b-a|\rightarrow 0$ the error decay at $|b-a|^{2n+3}$, which is better than the $|b-a|^{n+2}$ or $|b-a|^{n+3}$ in Newton-Cotes.
    \end{rem}
  
    \newpage

    \begin{exa} Use $n=1$ Gauss-Legendre quadrature to estimate $\int_0^\pi \sin(x)dx$.
    \end{exa}

    Firstly find the quadrature points. The first method is by using the formula of Legendre polynomials in HW4 problem 4, which is ${(1-x^2)^2}''=12x^2-4$, so roots are $\pm{1\over\sqrt{3}}$, so $x_0={\pi/2}(1-1/\sqrt{3})=0.6639$, $x_1={\pi/2}(1+1/\sqrt{3})=2.4777$.

    The second method is by finding a monic quadratic polynomial orthogonal to both $1$ and $x$ under $L^2([0, \pi])$. In other words, we need $a$ and $b$ such that
\[\int_0^\pi (x^2+ax+b)dx={\pi^3\over 3}+{a\pi^2\over 2}+b\pi=0\]
\[\int_0^\pi (x^2+ax+b)xdx={\pi^4\over 4}+{a\pi^3\over 3}+{b\pi^2\over 2}=0\]
Solve for $a$ and $b$ then find the roots.

     \newpage

    The quadrature weights are 
\[w_0=\int_0^\pi{x-x_1\over x_0-x_1}dx={x_1\pi-\pi^2/2\over x_1-x_0}=\pi/2=1.5708\]
\[w_1=\int_0^\pi{x-x_0\over x_1-x_0}dx={\pi^2/2-x_0\pi\over x_1-x_0}=\pi/2=1.5708\]
    So 
    \[I_1(\sin)=w_0\sin(x_0)+w_1\sin(x_1)=1.9358\]
    Error is $0.064$.

    \newpage

Another key property of the Gauss-Legendre quadrature is that all its weights are positive, because we have
\begin{thm}\label{pos_wt}
  The weight for Gauss-Legendre quadrature is
  \[w_k=\int_a^b{\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}dx=\int_a^b\left({\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}\right)^2dx\]
\end{thm}

\begin{proof}
Suppose $\psi_{n+1}=c\prod_i(x-x_i)$ is the weight 1 orthogonal polynomial on $[a, b]$ with index $n+1$, then
\[\int_a^b{\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}-\left({\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}\right)^2dx\]
\[={1\over(\prod_{j\not=k}(x_k-x_j)^2}\int_a^b\prod_{j\not=k}(x-x_j)(\prod_{j\not=k}(x_k-x_j)-\prod_{j\not=k}(x-x_j))dx\]
Let $h(x)=\prod_{j\not=k}(x_k-x_j)-\prod_{j\not=k}(x-x_j)$, then $\deg(h)=n$, and $h(x_k)=0$, hence $h=(x-x_k)h_1$ where $\deg(h_1)=n-1$. Now we have
\[\int_a^b\prod_{j\not=k}(x-x_j)(\prod_{j\not=k}(x_k-x_j)-\prod_{j\not=k}(x-x_j))dx=\int_a^b(\psi_{n+1}\cdot{h_1\over c})dx=0\]
\end{proof}

\newpage

\begin{rem} For comparison, when $n=8$, the Newton-Cotes quadrature weights are not all positive.\end{rem}

As a consequence, we have:
\begin{thm} (Theorem 10.2 in textbook) Given any continuous function $f$, the Gauss-Legendre quadrature result $I_n(f)$ converges to $\int_a^bfdx$ as $n\rightarrow\infty. $\end{thm}
\begin{proof}
  By Weierstrass approximation theorem there is some $p$ such that $|f-p|<\epsilon$ on $[a, b]$, hence for any $n>\deg(p)$,
  \[|\int_a^bfdx-I_n(f)|\leq |\int_a^bfdx-\int_a^bpdx|+|\int_a^bpdx-I_n(p)|+|I_n(p)-I_n(f)|\]
  \[\leq \epsilon(b-a)+0+\epsilon(b-a)\]
  The last bit is due to
  \[|I_n(p)-I_n(f)|=|\sum_iw_i(p(x_i)-f(x_i))|\leq \epsilon\sum_i|w_i|=\epsilon\sum_iw_i\]
  And $I_n(1)=\int_a^b1dx=b-a$ so $\sum_iw_i=b-a$.  Now let $\epsilon\rightarrow 0$ we get the convergence.
\end{proof}
If $w_i$ are not all positive, one can not remove the absolute value, hence this may not be true for other quadrature rules.

\newpage  

    \begin{rem} The Gauss-Legendre quadrature can be generalized to deal with the problem of estimating $\int_a^bfwdx$ where $w$ is a given weight function not depend on $f$, by replacing Legendre polynomials with other orthogonal polynomials, which are called ``Gauss quadrature''.
    \end{rem}

 \begin{definition}
  The Gauss quadrature points with weight $w$, $x_0,\dots, x_n$ on $[a, b]$ is defined as $x_j$ is the $j+1$-th root of the weight-$w$ orthogonal polynomial on $[a, b]$ with index $n+1$, $\psi_{n+1}$. The quadrature weights are chosen as $w_k=\int_a^bw{\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}dx$.
\end{definition}

 For example, if $[a, b]=[-1, 1]$, and $w$ is $(1-x^2)^{-1/2}$ we call it Chebyshev-Gauss quadrature.

\begin{thm}
  \begin{enumerate}
  \item $w_k=\int_a^bw\left({\prod_{j\not=k}(x-x_j)\over \prod_{j\not=k}(x_k-x_j)}\right)^2dx$ hence is always positive.
\item If $f$ is a polynomial of degree no more than $2n+1$, then $\int_a^bwfdx=I_n(f)$, where $I$ is the Gauss quadrature rule with $n+1$ quadrature points.
\item If $f$ is continuous on $[a, b]$, $\lim_{n\rightarrow\infty}I_n(f)=\int_a^bwfdx$ (Theorem 10.2 in textbook).
\item If $f\in C^{2n+2}$, then there is some $c\in [a, b]$, where $\int_a^bwfdx-I(f)={f^{(2n+2)}(c)\over (2n+2)!}\int_a^bw\prod_i(x-x_i)^2 dx$. (Theorem 10.1 in textbook)
  \end{enumerate}
\end{thm}

The proof is very similar to the Gauss-Legendre case.

\newpage

Review:

\begin{itemize}
\item Definition of Gauss Legendre quadrature.
\item Error bound.
\item Convergence.
\item General Gauss quadrature.
\end{itemize}

Key ideas:

\begin{itemize}
\item Error bound for Lagrange and Hermite interpolation.
\item Quadrature and integration as linear transformation.
\item Inner product and orthogonal polynomials.
\end{itemize}

\newpage

\begin{exa}$I=\int_0^\pi{1\over \sqrt{\sin x}}dx$. The correct answer is 5.2441151\end{exa}

\begin{itemize}
\item By change of variable,
  \[I={\pi\over 2}\int_{-1}^1{1\over\sqrt{\cos (\pi x/2)}}dx\]
\item Let $w=(1-x^2)^{-1/2}$, $f={(1-x^2)^{-1/2}\over\sqrt{\cos (\pi x/2)}}$. Then $I=\int fwdx$.
\item Now do Chebyshev-Gauss formula for $f$. Suppose $n=2$, then the quadrature points, which are the roots of $T_3=\cos(3\cos^{-1}x)$, are $0, \pm{\sqrt{3}\over 2}$.
\item The quadrature weights are
  \[w_1=\int_{-1}^1{(x^2-3/4)/(-3/4)\over\sqrt{1-x^2}}dx=\pi/3\]
  \[w_0=w_2=\pi/3\]
  (One can check that for Chebyshev-Gauss, the weights are always $\pi\over n+1$.)
\item The Chebyshev-Gauss for $n=2$ is
  \[I_2=\sum_{i=0}^2w_if(x_i)=3.3384\]
\item The final answer is $5.2439$, difference is $0.00018$.
\end{itemize}

\newpage

Exercises:

\begin{enumerate}
\item \begin{enumerate}
  \item Write down the Lagrange interpolation polynomial $p$ of a function $f$, with interpolation points $-1$, $0$ and $1$.
  \item Calculate $\int_0^1p(x)dx$, write it into the form of $w_0f(-1)+w_1f(0)+w_2f(1)$.
\item Show that the formula $I(f)=w_0f(-1)+w_1f(0)+w_2f(1)$ will give accurate answer if $f$ is a polynomial of degree $2$.
  \item Use the error estimate for Lagrange interpolation polynomial, find an upper bound for $\int_0^1f(x)dx-I(f)$.
  \end{enumerate}
  \item \begin{enumerate}
    \item Let $w=e^{-x}$, $L^2_w([0, \infty))$ be the space of functions $f$ such that $\int_0^\infty wf^2dx$ exists. Show that $1, x, x^2\in L^2_w([0, \infty))$.
     \item Use the inner product $(f, g)=\int_0^\infty wfgdx$, carry out Gram-Schmidt process for $\{1, x, x^2\}$ (the results are called {\bf Laguerre polynomials}).
\item Find the roots of the third polynomial you get from the above step, call them $x_0$ and $x_1$.
    \item Find $w_0$ and $w_1$ such that $\int_0^\infty wgdx=w_0g(x_0)+w_1g(x_1)$ for any polynomial $g$ of degree 1. This is called the Gauss-Laguerre quadrature.
\end{enumerate}

\end{enumerate}
\newpage
Answer:

\begin{enumerate}
\item
 \begin{enumerate}
  \item $p(x)=f(-1)(x(x-1)/2)+f(0)(1-x^2)+f(1)(x(x+1)/2)$
  \item $-f(-1)/12+2f(0)/3+5w_2f(1)/12$.
\item Because in this case, due to uniqueness of Lagrange interpolation, $f=p$.
  \item The error bound for Lagrange interpolation polynomial is $|f(x)-p(x)|\leq\max|f'''||x^3-x|/3!$, so the error bound for $I$ is $\max|f'''|/24$ Here maximum is taken on $[-1, 1]$.
  \end{enumerate}
 
\item Note that $\int_0^\infty x^ne^{-x}dx=n!$
\begin{enumerate}
    \item Because the weighted $L^2$ norms of them are $1, 2, 24$ respectively.
     \item $\{1, x-1, x^2-4x+2\}$
\item $2\pm\sqrt{2}$.
\item \[{1+\sqrt{2}\over 2\sqrt{2}}g(2-\sqrt{2})+{\sqrt{2}-1\over 2\sqrt{2}}g(2+\sqrt{2})\]
\end{enumerate}
\end{enumerate}

\newpage

\subsection{Composite Gauss quadrature (Section 10.5)}

One can combine composite method and Gauss quadrature: divide the interval into $m$ subintervals of equal length, then apply Gauss quadrature on each. For example, if we apply Gauss-Legendre quadrature with $k+1$ quadrature points to each subinterval, when the function is in $C^{2k+2}$, the error decay at a speed of $O(m^{-2k-2})$.

\newpage

\begin{exa} $\int_{-0.5}^{0.5}\sqrt{1-x^2}dx$, using composite Gauss quadrature with $k=1$\end{exa}

Roots of degree $2$ Legendre polynomials are $\pm1/\sqrt{3}$.

\begin{lstlisting}[language=Python]
def composite_gauss(n, a, b, f):
    r=0
    m=int((n+1)/2)
    for i in range(m):
        l0=(i*b+(m-i)*a)/m
        l1=((i+1)*b+(m-i-1)*a)/m
        x0=(l0+l1)/2-(l1-l0)/2/(3**0.5)
        x1=(l0+l1)/2+(l1-l0)/2/(3**0.5)
        r+=f(x0)+f(x1)
    return r*(b-a)/(n+1)
\end{lstlisting}


\begin{tikzpicture}
	\begin{axis}[
		xlabel=$log(n)$,
		ylabel=$-log(Error)$]
	\addplot[color=red,mark=x, only marks] coordinates {
		(0.6931471805599453,3.746560449724477)
(1.0986122886681098,4.547453810338138)
(1.3862943611198906,5.118965503519308)
(1.6094379124341003,5.5633925579666705)
(1.791759469228055,5.927002052110047)
(1.9459101490553132,6.2346716549051004)
(2.0794415416798357,6.501320810552323)
(2.1972245773362196,6.73660161087667)
(2.302585092994046,6.947117728247236)
(2.3978952727983707,7.137586004803848)
(2.4849066497880004,7.311492816297971)
(2.5649493574615367,7.4714878376833544)
(2.6390573296152584,7.6196319546081686)
(2.70805020110221,7.757559684401888)
(2.772588722239781,7.886589202729626)
(2.833213344056216,8.00779902926391)
(2.8903717578961645,8.12208280373383)
(2.9444389791664403,8.23018925813059)
(2.995732273553991,8.332751940018328)
(3.044522437723423,8.43031168655338)
(3.091042453358316,8.52333387219665)
(3.1354942159291497,8.612221823753858)
(3.1780538303479458,8.697327381267385)
(3.2188758248682006,8.778959303716851)
(3.258096538021482,8.857390026592949)
(3.295836866004329,8.932861144496894)
(3.332204510175204,9.00558789696487)
(3.367295829986474,9.075762867305217)
(3.4011973816621555,9.143559054538375)
};

	\addplot[color=blue,mark=+, only marks] coordinates {
	(1.3862943611198906,9.114856038771517)
(1.791759469228055,10.641478993908981)
(2.0794415416798357,11.752422282602218)
(2.302585092994046,12.624884254357372)
(2.4849066497880004,13.342684719274926)
(2.6390573296152584,13.95214413140681)
(2.772588722239781,14.481537873892268)
(2.8903717578961645,14.949379792923526)
(2.995732273553991,15.368444387441553)
(3.091042453358316,15.7479127618939)
(3.1780538303479458,16.09460251736614)
(3.258096538021482,16.413713532022914)
(3.332204510175204,16.70930151946475)
(3.4011973816621555,16.98459025133957)
};
	\addplot[color=brown,mark=*, only marks] coordinates {
(1.0986122886681098,9.533573517620844)
(1.6094379124341003,11.053629765917707)
(1.9459101490553132,12.161876761968388)
(2.1972245773362196,13.032985223792194)
(2.3978952727983707,13.750015347951813)
(2.5649493574615367,14.358996558690196)
(2.70805020110221,14.888073888245788)
(2.833213344056216,15.3556959536854)
(2.9444389791664403,15.774601761766425)
(3.044522437723423,16.15395181051019)
(3.1354942159291497,16.50055106888232)
(3.2188758248682006,16.819591361689714)
(3.295836866004329,17.115123042359492)
(3.367295829986474,17.390366204550023)
	};
      \end{axis}
      
\end{tikzpicture}


\newpage

\subsection{Other topics in numerical integration}

\subsubsection{Modified Gauss Quadratures (Section 10.6), won't be in final exam}

Sometimes some quadrature points are pre-determined while others can be chosen strategically, in which case we can choose them as roots of orthogonal polynomials. If the pre-determined point is one of the end point we call this strategy {\bf Radau quadrature}, if it is both end points we call it {\bf Lobatto quadrature}.\\

If there are $n+1$ quadrature points, $k$ of them are predetermined and the rest roots of Legendre polynomial of degree $n+1-k$, the error is bounded by $O((b-a)^{2n+3-k})$. When $n=k=2$ we get Simpson's method.\\

\newpage

\subsubsection{Richardson Extrapolation (Section 7.6, 7.7), won't be in final exam}

A common trick in numerical analysis is {\bf extrapolation}: if a sequence $a_n$ can be calculated whose limit as $n\rightarrow \infty$ is some $a$, and the speed of convergence is known, we can use linear combination of successive terms to speed up the convergence. For example, for composite trapezium rule, Euler-Maclaurin formula says that, when $f$ is ``good enough'', 
\[I_n-I=\sum_{k=1}^\infty c_kn^{-2k}\]
So ${4I_{2n}-I_n\over 3}$ (which is identical to composite Simpson's rule) has $O(n^{-4})$ convergence, and one can do it repeatedly to get higher convergence speed.

\subsection{Review}

\begin{itemize}
\item Quadrature rule
\item Newton-Cotes quadrature
\item Gauss quadrature
\item Composite method
\end{itemize}

\newpage

\section{Numerical ODE: IVP (Chapter 12)}

Initial value problem of ODE:
\[y'=f(t, y), y(0)=y_0\]
Here $y$ can be a real valued function or $\mathbb{R}^n$-valued function. For now we focus on the case where $y$ is real valued.\\

We always assume $f$ is continuous and Lipschitz on the second parameter ($|f(t, y)-f(t, z)|\leq L|y-z|$ for all pair $y, z$, $L$ is called the Lipschitz constant). So by Picard's Theorem, IVP has a unique solution.\\

Numerical integration can be seen as a special case of IVP of numerical ODE, with $f$ independent from $y$.\\

Strategy: Find some small positive number $h$ as the ``step size'', estimate the value of $y$ at $nh$ for all $n$.\\

\newpage

\subsection{Euler's Method (12.2, 12.3)}

$h$ is a small number, $z$ an approximation of $y$ evaluated using:
\[z(0)=y_0\]
\[z((n+1)h)=z(nh)+hf(nh, z(nh))\]

\begin{itemize}
\item The motivation is using PL function to approximate $y$.
\item {\bf Truncated error} $T_n$ is error introduced at step $n$, divided by step size. More precisely, suppose the method is $z((n+1)h)=G(z(nh))$, then
  \[T_n={G(y(nh))-y((n+1)h)\over h}\]. 
\item A method has {\bf order of accuracy} $p$ if when $f$ is sufficiently smooth, $T=O(h^p)$.
\item {\bf Global error at time} $t$ is the difference between the estimated $z(t)$ and the true value of $y(t)$.
\item A method is called {\bf consistent} if truncated error goes to $0$ as $h\rightarrow 0$. {\bf Convergent} if global error goes to $0$ as $h\rightarrow 0$.
\item Under certain assumptions, global error at given time $t$ is controlled by the bound on truncated error (Theorem 12.2 and 12.5 in textbook).
\end{itemize}

\newpage

\begin{exa} Euler's method applied to $y'=y$, $y(0)=1$\end{exa}

True solution: $y(t)=e^t$.\\

Approximated solution using Euler's method with step-size $h$:
\[z(0)=1, z(h)=1+hz(0)=1+h, z(2h)=z(h)+hz(h)=1+h+h(1+h)=(1+h)^2, \dots, z(nh)=(1+h)^n\]

So the global error at time $t=nh$ is $e^t-(1+h)^n=e^t-(1+h)^{t/h}$, which converges to $0$ as $h\rightarrow 0$.\\

To understand the error created at each step, consider the sequence $z_k(nh)$, such that
\[z_k(kh)=y(kh)=e^{kh}\]
\[z_k((n+1)h)=z_k(nh)+hz_k(nh)\]
So
\[z_k(nh)=e^{kh}(1+h)^{n-k}\]
\[T_n={e^{(n+1)h}-e^{nh}(1+h)\over h}=O(h)\]
So the method is consistent with order of accuracy $1$.\\

\newpage

\begin{tikzpicture}[yscale=2, xscale=8]
  \draw[->] (0, 0) -- (1, 0) node[right] {$x$};
  \draw[->] (0, 0) -- (0, 3) node[above] {$y$};
  \draw[scale=1, domain=0:1, smooth, variable=\x, black] plot ({\x}, {exp(\x)});
  \draw[-, blue] (0, 1)--(0.25, 1.25)--(0.5, 1.25*1.25)--(0.75, 1.25*1.25*1.25)--(1, 1.25*1.25*1.25*1.25);
  \draw[-, red] (0.25, 1.28402541669)--(0.5, 1.28402541669*1.25)--(0.75, 1.28402541669*1.25*1.25)--(1, 1.28402541669*1.25*1.25*1.25);
  \draw[-, red] (0.5, 1.6487212707)--(0.75, 1.6487212707*1.25)--(1, 1.6487212707*1.25*1.25);
  \draw[-, red] (0.75, 2.11700001661)--(1, 2.11700001661*1.25);
  \end{tikzpicture}

To study global error, we need to understand:

\begin{itemize}
\item What are the truncated errors?
\item How do error introduced in prior steps grow with time?
\end{itemize}

\newpage

\begin{itemize}
\item By definition of derivative, if $t=nh$,
\[y(t+h)=y(t)+hf(t, y(t))+o(h)\]
So $T_n={o(h)\over h}$, Euler's method is {\bf consistent}.
\item If $f$ is sufficiently smooth, so is $y$, so
\[y(t+h)=y(t)+hf(t, y(t))+{h^2\over 2}y''(t)+o(h^2)\]
So $T_n=O(h)$, Euler's method is of order of accuracy $1$.
\item Furthermore, if we write down the remainder of Taylor's series, we have
  \[y(t+h)=y(t)+hy'(t)+{h^2\over 2}y''(s)\]
  \[=y(t)+hf(t, y(t))+{h^2\over 2}(f_1(s, y(s))+f_2(s, y(s))f(s, y(s)))\]
for some $s\in [t, t+h]$, $f_1$ and $f_2$ are the partial derivative in first and second parameter. So,  if $f$ and its partial derivatives are all bounded, $T$ is uniformly bounded by some $Ch$.
\end{itemize}

\newpage

Let's now analyze how error grows in Euler's method:

\begin{lem}
  If $f$ is $L$-Lip. on second paramater, $w_1$, $w_2$ two functions defined on $mh, (m+1)h, \dots, nh$, such that
  \[w_i((k+1)h)=w_i(kh)+hf(kh, w_i(kh))\]
  Then 
\[|w_1(nh)-w_2(nh)|\leq e^{L(n-m)h}|w_1(mh)-w_2(mh)|\]
\end{lem}

\begin{proof}
  \[LHS\leq (1+hL)|w_1((n-1)h)-w_2((n-1)h)|\leq\]
    \[\dots\leq (1+hL)^{n-m}|w_1(mh)-w_2(mh)|\]
 And \[(1+hL)^{n-m}\leq e^{L(n-m)h}\]
\end{proof}

\newpage

Now let's estimate global error at time $t=nh$. Let $z_k(nh)$, $n\geq k$, be
\[z_k(kh)=y(kh)\]
\[z_k((n+1)h)=z_k(nh)+hf(nh, z_k(nh))\]
\[|y(nh)-z(nh)|=|z_{n}(nh)-z_{0}(nh)|\leq \sum_{k=0}^{n-1}|z_k(nh)-z_{k+1}(nh)|\]
(Triangle inequality)
\[\leq \sum_{k=0}^{n-1}e^{Lh(n-k-1)}|z_k((k+1)h)-z_{k+1}((k+1)h)|\]
(The Lemma from previous page)
\[\leq \sum_{k=0}^{n-1}Ch^2 e^{L(n-k-1)h}\leq {Ch^2(e^{Lnh}-1)\over e^h-1}\]
(Bound on truncated error)\\
So
\begin{thm}
If
\begin{itemize}
\item $f$ is smooth, $f$ and its partial derivatives are bounded.
\item $nh$ is fixed and $n\rightarrow\infty$
\end{itemize}
then the global error of Euler's method at time $nh$ converges to zero at $O(h)$. In other words, Euler's method is {\bf convergent}.
\end{thm}

\newpage

Review:

\begin{itemize}
\item Composite Gauss Quadrature
\item Euler's method.
\item Truncated error and global error.
\item Consistency and order of accuracy.
\item Convergence.
\end{itemize}


\newpage

\begin{exa}Midpoint rule\end{exa}
\begin{enumerate}
\item The weight $1$ orthogonal polynomial on $[a, b]$ of degree $1$. It is $x-{b+a\over 2}$. The root of it is ${b+a\over 2}$.
\item Hence, the Gauss-Legendre quadrature with $n=0$ is $x_0={b+a\over 2}$. Quadrature weight is ${b-a}$.
\item If $f$ is differentiable, the error bound is
  \[|\int_a^bfdx-{(b-a)}f({a+b\over 2})|\leq {\max|f''|\int_a^b(x-{a+b\over 2})^2dx\over 2!}={\max|f''|(b-a)^3\over 24}\]
\item Now decompose $f$ into $m$ subintervals, each applying the Gauss-Legendre quadrature, we get
  \[I={b-a\over m}\sum_{i=0}^{m-1} f(a+{(i+1/2)(b-a)\over m})\]
  And the error bound is $\max|f''|(b-a)^3\over 24m^2$.
\end{enumerate}

\newpage

\begin{exa} $y'=\sin(y)$, $y(0)=1$\end{exa}
\begin{enumerate}
\item True solution: $y(t)=2\tan^{-1}(\tan(1/2)e^x)$\\
\item Euler's method is:
\[z(0)=1, z(nh)=z((n-1)h)+h\sin(z((n-1)h))\]
\item Truncated error for Euler's method at time $t$ is
  \[|{y''(s)h\over 2!}|=|{y'\cos(y)h\over 2!}|\]
  \[=|{\sin(y)\cos(y)h\over 2!}|\leq Ch\]
  What is the number $C$?
\item $\sin(y)$ is $L$-Lipschitz. In other words, $|\sin(a)-\sin(b)|\leq L|a-b|$ for all $a, b$. What's a valid $L$?
\item Now the argument from last lecture shows that the global error bound at time $t=nh$ is ${Ch^2(e^{Lt}-1)\over e^h-1}$. 
\item Let $t=1$, $h=0.1$, Euler's method get $z(1)=1.95109$. True answer is $y(1)=1.95629$. Error bound as calculated from above, using $C=1/4, L=1$, is $0.04084$.
\end{enumerate}


\newpage

\begin{exa}Consider IVP $y'=f(y)$, $y(0)=y_0$. Suppose $f$ is {\bf real analytic}, i.e. the Taylor series at every point converges at a neighborhood of the point. Suppose $h$ is a small positive number. The theory of ODE tells us that $y$ is also real analytic.\end{exa}

Suppose $f(x)=\sum_ia_i(x-y_0)^i$.

\begin{enumerate}
\item Write down the Taylor expansion of $y$ at $t=0$, up to $t^2$ term.
\item Write down the result of $z(h)=z(0)+hf(z(0))$, as a power series of $h$.
\item Write down the result of $z(2h)=z(h)+hf(z(h))$, as a power series of $h$.
\item Find a linear combination of $y_0$, $z(h)$, $z(2h)$ which is close to $y(2h)$ up to the $h^2$ term. This is called the 2nd order Runge-Kutta method (rk2).
\end{enumerate}

Answer:

\begin{enumerate}
\item $y(t)=y_0+a_0t+{a_0a_1\over 2}t^2+O(t^3)$
\item $z(h)=y_0+a_0h$
\item $z(2h)=y_0+2a_0h+a_0a_1h^2+O(h^3)$
\item $y(2h)=y_0-2z(h)+2z(2h)+O(h^3)$
\end{enumerate}


\newpage

\subsection{Ways to get higher order methods}

\subsubsection{Linear Multistep Method (12.4, 12.6)}

Firstly some methods based on Lagrange interpolation:

\paragraph{Explicit Methods} Consider the function $g(t)=f(t, y(t))$. One can then use $g(t-h), \dots, g(t-kh)$ as interpolation point to estimate $\int_{t-dh}^{t}g(s)ds$, then use $g(t)=g(t-dh)+\int_{t-dh}^{t}g(s)ds$.

\paragraph{Implicit Methods} One can also use $g(t), g(t-h), \dots, g(t-kh)$ as interpolation point to estimate $\int_{t-dh}^{t}g(s)ds$, then use $g(t)=g(t-dh)+\int_{t-dh}^{t}g(s)ds$. This way $g(t)$ appears on the right hand side and the calculation of $g(t)$ requires solving an equation, hence is called {\bf implicit}.

\newpage

\begin{exa}\begin{itemize}
  \item Explicit method for $d=1$, $k=1$ is Euler's Method.
  \item Implicit method for $d=1$, $k=1$ is the {\bf trapezium method} as the numerical integration is via trapezium rule:
\[z(t)=z(t-h)+{h\over 2}(f(t, z(t))+f(t-h, z(t-h)))\] 
We can show that it is consistent with order of accuracy $2$.
\item Implicit method for $k=2$, $d=2$ will be
  \[z(t)=z(t-2h)+{h\over 3}(f(t, z(t))+4f(t-h, z(t-h))+f(t-2h, z(t-2h)))\]
  When $f(t, y)=f(t)$ this reduces to Simpson's rule.
\item When $d=1$, the explicit methods are called $k$-th step {\bf Adams-Bashforth} methods, while the implicit methods are called {\bf Adams-Moulton} methods. For example, let $k=4$, we get
  \[z(t)=z(t-h)+{h\over 24}(55f(t-h, z(t-h))-59f(t-2h, z(t-2h))+37f(t-3h, z(t-3h))-9f(t-4h, z(t-4h))\]
\end{itemize}
\end{exa}


\newpage

General Linear $k$-step Method:

\[\sum_{j=0}^k\alpha_jz((n+j)h)=h\sum_{j=0}^k\beta_jf((n+j)h, z((n+j)h))\]

If $\beta_k=0$ it is explicit, otherwise it is implicit.

\begin{itemize}  
\item We want a linear $k$-step method to be {\bf zero-stable}. In other words, if the equation is $y'=0$, then the $z(nh)$ does not go to infinity as $n\rightarrow\infty$. From the theory in linear difference equations in linear algebra, we know that this is equivalent to the {\bf first characteristic polynomial} $\rho(z)=\sum_{j=0}^k\alpha_jz^j$ having all roots inside the closed unit disc and at most only single roots on the unit circle.
\item Why do we need zero-stability? Suppose we let $h\rightarrow 0$ and $t=hn$ remain unchanged, then $n\rightarrow\infty$. If there is no zero stability, error at previous steps will grow indefinitely.
\end{itemize}

\newpage

\begin{itemize}
\item To get consistency, let $\sigma(z)=\sum_{j=0}^k\beta_jz^j$ be the {\bf second characteristic polynomial}. When $f$ is smooth, $h<<1$, $f(nh+s, y(nh+s))=f(nh, y(nh))+O(s)$, $y(nh+s)=y(nh)+sf(nh, y(nh))+O(s^2)$. To make method consistent, we want to make sure that if we put $y((n+j)h)$ in place of $z((n+j)h)$, the left hand side and right hand side are off by $o(h)$, hence
  \[\rho(1)+\rho'(0)f(nh, y(nh))h=h\sigma(1)f(nh, y(nh))+O(h^2)\]
  So $\rho(1)=0$, $\rho'(0)=\sigma(1)$.
\item To get order of consistency, carry out the same argument as above but do higher order power expansion for $y(nh+s)$ and $f(nh+s, y(nh+s))$.
\end{itemize}

For global convergence of linear multistep methods we have the Dahlquist's theorems:
\begin{itemize}
\item If a linear $k$-step method has zero stability, then it is consistent iff it is convergent, and the truncated error and global error has the same order as $h\rightarrow 0$.
\item (First Dahlquist barrier) If a linear $k$-step method is $k$ stable then the order of accuracy is no more than $k+1$ if $k$ is odd (e.g. AM), $k+2$ if $k$ is even (e.g. $d=k$ implicit), and $k$ if it has to be explicit (e.g. AB).
\end{itemize}

\newpage

\subsubsection{Runge-Kutta methods}



  
\end{document}